{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importación de Librerías"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este script esta pensado para correr en Google Cloud\n",
    "- 8 vCPU\n",
    "- 128 GB memoria RAM\n",
    "\n",
    "- se entrena con clase_binaria2  POS =  { BAJA+1, BAJA+2 }\n",
    "- Optimizacion Bayesiana de hiperparametros de  lightgbm,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semilla 1, base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# limpio la memoria\n",
    "rm(list = ls()) # remove all objects\n",
    "gc() # garbage collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "require(\"data.table\")\n",
    "require(\"rlist\")\n",
    "require(\"lightgbm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# paquetes necesarios para la Bayesian Optimization\n",
    "require(\"DiceKriging\")\n",
    "require(\"mlrMBO\")\n",
    "\n",
    "# para que se detenga ante el primer error\n",
    "# y muestre el stack de funciones invocadas\n",
    "options(error = function() {\n",
    "  traceback(20)\n",
    "  options(error = NULL)\n",
    "  stop(\"exiting after script error\")\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definición de Parametros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# defino los parametros de la corrida, en una lista, la variable global  PARAM\n",
    "#  muy pronto esto se leera desde un archivo formato .yaml\n",
    "PARAM <- list()\n",
    "\n",
    "PARAM$experimento <- \"HT8230 (Clase 12, Base Sem1)\"\n",
    "\n",
    "PARAM$input$dataset <- \"datasets/competencia_03_base_lags.csv.gz\"\n",
    "\n",
    "# los meses en los que vamos a entrenar\n",
    "#  mucha magia emerger de esta eleccion\n",
    "PARAM$input$testing <- c(202107) # Último mes, lo más cercano al 202109 de kaggle\n",
    "PARAM$input$validation <- c(202106)\n",
    "PARAM$input$training <- c(202105, 202104, 202103, 202102, 202101,202012) # 6 meses de entrenamiento\t\n",
    "\n",
    "# un undersampling de 0.1  toma solo el 10% de los CONTINUA\n",
    "PARAM$trainingstrategy$undersampling <- 1.0\n",
    "PARAM$trainingstrategy$semilla_azar <- c(279511, 279523, 279541, 279551, 279571)  # Aqui poner su  primer  semilla/ pongo todas mis semillas\n",
    "\n",
    "PARAM$hyperparametertuning$POS_ganancia <- 273000\n",
    "PARAM$hyperparametertuning$NEG_ganancia <- -7000\n",
    "\n",
    "# Aqui va semilla\n",
    "PARAM$lgb_semilla <- 279511"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Hiperparametros FIJOS de  lightgbm\n",
    "PARAM$lgb_basicos <- list(\n",
    "  boosting = \"gbdt\", # puede ir  dart  , ni pruebe random_forest\n",
    "  objective = \"binary\",\n",
    "  metric = \"custom\",\n",
    "  first_metric_only = TRUE,\n",
    "  boost_from_average = TRUE,\n",
    "  feature_pre_filter = FALSE,\n",
    "  force_row_wise = TRUE, # para reducir warnings\n",
    "  verbosity = -100,\n",
    "  max_depth = -1L, # -1 significa no limitar,  por ahora lo dejo fijo\n",
    "  min_gain_to_split = 0.0, # min_gain_to_split >= 0.0\n",
    "  min_sum_hessian_in_leaf = 0.001, #  min_sum_hessian_in_leaf >= 0.0\n",
    "  lambda_l1 = 0.0, # lambda_l1 >= 0.0\n",
    "  lambda_l2 = 0.0, # lambda_l2 >= 0.0\n",
    "  max_bin = 31L, # lo debo dejar fijo, no participa de la BO\n",
    "  num_iterations = 9999, # un numero muy grande, lo limita early_stopping_rounds\n",
    "\n",
    "  bagging_fraction = 1.0, # 0.0 < bagging_fraction <= 1.0\n",
    "  pos_bagging_fraction = 1.0, # 0.0 < pos_bagging_fraction <= 1.0\n",
    "  neg_bagging_fraction = 1.0, # 0.0 < neg_bagging_fraction <= 1.0\n",
    "  is_unbalance = FALSE, #\n",
    "  scale_pos_weight = 1.0, # scale_pos_weight > 0.0\n",
    "\n",
    "  drop_rate = 0.1, # 0.0 < neg_bagging_fraction <= 1.0\n",
    "  max_drop = 50, # <=0 means no limit\n",
    "  skip_drop = 0.5, # 0.0 <= skip_drop <= 1.0\n",
    "\n",
    "  extra_trees = TRUE, # Magic Sauce\n",
    "\n",
    "  seed = PARAM$lgb_semilla\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Aqui se cargan los hiperparametros que se optimizan\n",
    "#  en la Bayesian Optimization\n",
    "PARAM$bo_lgb <- makeParamSet(\n",
    "  makeNumericParam(\"learning_rate\", lower = 0.02, upper = 0.3),\n",
    "  makeNumericParam(\"feature_fraction\", lower = 0.01, upper = 1.0),\n",
    "  makeIntegerParam(\"num_leaves\", lower = 8L, upper = 1024L),\n",
    "  makeIntegerParam(\"min_data_in_leaf\", lower = 100L, upper = 50000L)\n",
    ")\n",
    "\n",
    "# si usted es ambicioso, y tiene paciencia, podria subir este valor a 100\n",
    "PARAM$bo_iteraciones <- 50 # iteraciones de la Optimizacion Bayesiana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# graba a un archivo los componentes de lista\n",
    "# para el primer registro, escribe antes los titulos\n",
    "\n",
    "loguear <- function(\n",
    "    reg, arch = NA, folder = \"./exp/\",\n",
    "    ext = \".txt\", verbose = TRUE) {\n",
    "  archivo <- arch\n",
    "  if (is.na(arch)) archivo <- paste0(folder, substitute(reg), ext)\n",
    "\n",
    "  if (!file.exists(archivo)) # Escribo los titulos\n",
    "    {\n",
    "      linea <- paste0(\n",
    "        \"fecha\\t\",\n",
    "        paste(list.names(reg), collapse = \"\\t\"), \"\\n\"\n",
    "      )\n",
    "\n",
    "      cat(linea, file = archivo)\n",
    "    }\n",
    "\n",
    "  linea <- paste0(\n",
    "    format(Sys.time(), \"%Y%m%d %H%M%S\"), \"\\t\", # la fecha y hora\n",
    "    gsub(\", \", \"\\t\", toString(reg)), \"\\n\"\n",
    "  )\n",
    "\n",
    "  cat(linea, file = archivo, append = TRUE) # grabo al archivo\n",
    "\n",
    "  if (verbose) cat(linea) # imprimo por pantalla\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------\n",
    "GLOBAL_arbol <- 0L\n",
    "GLOBAL_gan_max <- -Inf\n",
    "vcant_optima <- c()\n",
    "\n",
    "fganancia_lgbm_meseta <- function(probs, datos) {\n",
    "  vlabels <- get_field(datos, \"label\")\n",
    "  vpesos <- get_field(datos, \"weight\")\n",
    "\n",
    "\n",
    "  GLOBAL_arbol <<- GLOBAL_arbol + 1\n",
    "  tbl <- as.data.table(list(\n",
    "    \"prob\" = probs,\n",
    "    \"gan\" = ifelse(vlabels == 1 & vpesos > 1,\n",
    "      PARAM$hyperparametertuning$POS_ganancia,\n",
    "      PARAM$hyperparametertuning$NEG_ganancia  )\n",
    "  ))\n",
    "\n",
    "  setorder(tbl, -prob)\n",
    "  tbl[, posicion := .I]\n",
    "  tbl[, gan_acum := cumsum(gan)]\n",
    "\n",
    "  tbl[, gan_suavizada :=\n",
    "    frollmean(\n",
    "      x = gan_acum, n = 2001, align = \"center\",\n",
    "      na.rm = TRUE, hasNA = TRUE\n",
    "    )]\n",
    "\n",
    "  gan <- tbl[, max(gan_suavizada, na.rm = TRUE)]\n",
    "\n",
    "\n",
    "  pos <- which.max(tbl[, gan_suavizada])\n",
    "  vcant_optima <<- c(vcant_optima, pos)\n",
    "\n",
    "  if (GLOBAL_arbol %% 10 == 0) {\n",
    "    if (gan > GLOBAL_gan_max) GLOBAL_gan_max <<- gan\n",
    "\n",
    "    cat(\"\\r\")\n",
    "    cat(\n",
    "      \"Validate \", GLOBAL_iteracion, \" \", \" \",\n",
    "      GLOBAL_arbol, \"  \", gan, \"   \", GLOBAL_gan_max, \"   \"\n",
    "    )\n",
    "  }\n",
    "\n",
    "\n",
    "  return(list(\n",
    "    \"name\" = \"ganancia\",\n",
    "    \"value\" = gan,\n",
    "    \"higher_better\" = TRUE\n",
    "  ))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------\n",
    "\n",
    "EstimarGanancia_lightgbm <- function(x) {\n",
    "  gc()\n",
    "  GLOBAL_iteracion <<- GLOBAL_iteracion + 1L\n",
    "\n",
    "  # hago la union de los parametros basicos y los moviles que vienen en x\n",
    "  param_completo <- c(PARAM$lgb_basicos, x)\n",
    "\n",
    "  param_completo$early_stopping_rounds <-\n",
    "    as.integer(400 + 4 / param_completo$learning_rate)\n",
    "\n",
    "  GLOBAL_arbol <<- 0L\n",
    "  GLOBAL_gan_max <<- -Inf\n",
    "  vcant_optima <<- c()\n",
    "  set.seed(PARAM$lgb_semilla, kind = \"L'Ecuyer-CMRG\")\n",
    "  modelo_train <- lgb.train(\n",
    "    data = dtrain,\n",
    "    valids = list(valid = dvalidate),\n",
    "    eval = fganancia_lgbm_meseta,\n",
    "    param = param_completo,\n",
    "    verbose = -100\n",
    "  )\n",
    "\n",
    "  cat(\"\\n\")\n",
    "\n",
    "  cant_corte <- vcant_optima[modelo_train$best_iter]\n",
    "\n",
    "  # aplico el modelo a testing y calculo la ganancia\n",
    "  prediccion <- predict(\n",
    "    modelo_train,\n",
    "    data.matrix(dataset_test[, campos_buenos, with = FALSE])\n",
    "  )\n",
    "\n",
    "  tbl <- copy(dataset_test[, list(\"gan\" = ifelse(clase_ternaria == \"BAJA+2\",\n",
    "    PARAM$hyperparametertuning$POS_ganancia, \n",
    "    PARAM$hyperparametertuning$NEG_ganancia))])\n",
    "\n",
    "  tbl[, prob := prediccion]\n",
    "  setorder(tbl, -prob)\n",
    "  tbl[, gan_acum := cumsum(gan)]\n",
    "  tbl[, gan_suavizada := frollmean(\n",
    "    x = gan_acum, n = 2001,\n",
    "    align = \"center\", na.rm = TRUE, hasNA = TRUE\n",
    "  )]\n",
    "\n",
    "\n",
    "  ganancia_test <- tbl[, max(gan_suavizada, na.rm = TRUE)]\n",
    "\n",
    "  cantidad_test_normalizada <- which.max(tbl[, gan_suavizada])\n",
    "\n",
    "  rm(tbl)\n",
    "  gc()\n",
    "\n",
    "  ganancia_test_normalizada <- ganancia_test\n",
    "\n",
    "\n",
    "  # voy grabando las mejores column importance\n",
    "  if (ganancia_test_normalizada > GLOBAL_gananciamax) {\n",
    "    GLOBAL_gananciamax <<- ganancia_test_normalizada\n",
    "    tb_importancia <- as.data.table(lgb.importance(modelo_train))\n",
    "\n",
    "    fwrite(tb_importancia,\n",
    "      file = paste0(\"impo_\", sprintf(\"%03d\", GLOBAL_iteracion), \".txt\"),\n",
    "      sep = \"\\t\"\n",
    "    )\n",
    "\n",
    "    rm(tb_importancia)\n",
    "  }\n",
    "\n",
    "\n",
    "  # logueo final\n",
    "  ds <- list(\"cols\" = ncol(dtrain), \"rows\" = nrow(dtrain))\n",
    "  xx <- c(ds, copy(param_completo))\n",
    "\n",
    "  xx$early_stopping_rounds <- NULL\n",
    "  xx$num_iterations <- modelo_train$best_iter\n",
    "  xx$estimulos <- cantidad_test_normalizada\n",
    "  xx$ganancia <- ganancia_test_normalizada\n",
    "  xx$iteracion_bayesiana <- GLOBAL_iteracion\n",
    "\n",
    "  loguear(xx, arch = \"BO_log.txt\")\n",
    "\n",
    "  set.seed(PARAM$lgb_semilla, kind = \"L'Ecuyer-CMRG\")\n",
    "  return(ganancia_test_normalizada)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aquí empieza el programa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Aqui empieza el programa\n",
    "\n",
    "# Aqui se debe poner la carpeta de la computadora local\n",
    "setwd(\"~/buckets/b1/\") # Establezco el Working Directory\n",
    "\n",
    "# cargo el dataset donde voy a entrenar el modelo\n",
    "dataset <- fread(PARAM$input$dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# creo la carpeta donde va el experimento\n",
    "dir.create(\"./exp/\", showWarnings = FALSE)\n",
    "dir.create(paste0(\"./exp/\", PARAM$experimento, \"/\"), showWarnings = FALSE)\n",
    "\n",
    "# Establezco el Working Directory DEL EXPERIMENTO\n",
    "setwd(paste0(\"./exp/\", PARAM$experimento, \"/\"))\n",
    "\n",
    "# en estos archivos quedan los resultados\n",
    "kbayesiana <- paste0(PARAM$experimento, \".RDATA\")\n",
    "klog <- paste0(PARAM$experimento, \".txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Catastrophe Analysis  -------------------------------------------------------\n",
    "# deben ir cosas de este estilo\n",
    "#   dataset[foto_mes == 202006, active_quarter := NA]\n",
    "\n",
    "# Data Drifting\n",
    "# por ahora, no hago nada\n",
    "\n",
    "\n",
    "# Feature Engineering Historico  ----------------------------------------------\n",
    "#   aqui deben calcularse los  lags y  lag_delta\n",
    "#   Sin lags no hay paraiso !  corta la bocha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# ahora SI comienza la optimizacion Bayesiana\n",
    "\n",
    "GLOBAL_iteracion <- 0 # inicializo la variable global\n",
    "GLOBAL_gananciamax <- -1 # inicializo la variable global\n",
    "\n",
    "# si ya existe el archivo log, traigo hasta donde llegue\n",
    "if (file.exists(klog)) {\n",
    "  tabla_log <- fread(klog)\n",
    "  GLOBAL_iteracion <- nrow(tabla_log)\n",
    "  GLOBAL_gananciamax <- tabla_log[, max(ganancia)]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# paso la clase a binaria que tome valores {0,1}  enteros\n",
    "dataset[, clase01 := ifelse(clase_ternaria == \"CONTINUA\", 0L, 1L)]\n",
    "\n",
    "\n",
    "# los campos que se van a utilizar\n",
    "campos_buenos <- setdiff(\n",
    "  colnames(dataset),\n",
    "  c(\"clase_ternaria\", \"clase01\", \"azar\", \"training\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# defino los datos que forma parte del training\n",
    "# aqui se hace el undersampling de los CONTINUA\n",
    "set.seed(PARAM$trainingstrategy$semilla_azar)\n",
    "dataset[, azar := runif(nrow(dataset))]\n",
    "dataset[, training := 0L]\n",
    "dataset[\n",
    "  foto_mes %in% PARAM$input$training &\n",
    "    (azar <= PARAM$trainingstrategy$undersampling | clase_ternaria %in% c(\"BAJA+1\", \"BAJA+2\")),\n",
    "  training := 1L\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# dejo los datos en el formato que necesita LightGBM\n",
    "dtrain <- lgb.Dataset(\n",
    "  data = data.matrix(dataset[training == 1L, campos_buenos, with = FALSE]),\n",
    "  label = dataset[training == 1L, clase01],\n",
    "  weight = dataset[training == 1L, \n",
    "    ifelse(clase_ternaria == \"BAJA+2\", 1.0000001, \n",
    "      ifelse(clase_ternaria == \"BAJA+1\", 1.0, 1.0))],\n",
    "  free_raw_data = FALSE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# defino los datos que forman parte de validation\n",
    "#  no hay undersampling\n",
    "dataset[, validation := 0L]\n",
    "dataset[ foto_mes %in% PARAM$input$validation,  validation := 1L]\n",
    "\n",
    "dvalidate <- lgb.Dataset(\n",
    "  data = data.matrix(dataset[validation == 1L, campos_buenos, with = FALSE]),\n",
    "  label = dataset[validation == 1L, clase01],\n",
    "  weight = dataset[validation == 1L, \n",
    "    ifelse(clase_ternaria == \"BAJA+2\", 1.0000001, \n",
    "      ifelse(clase_ternaria == \"BAJA+1\", 1.0, 1.0))],\n",
    "  free_raw_data = FALSE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# defino los datos de testing\n",
    "dataset[, testing := 0L]\n",
    "dataset[ foto_mes %in% PARAM$input$testing,  testing := 1L]\n",
    "\n",
    "\n",
    "dataset_test <- dataset[testing == 1, ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# libero espacio\n",
    "rm(dataset)\n",
    "gc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Aqui comienza la configuracion de la Bayesian Optimization\n",
    "funcion_optimizar <- EstimarGanancia_lightgbm # la funcion que voy a maximizar\n",
    "\n",
    "configureMlr(show.learner.output = FALSE)\n",
    "\n",
    "# configuro la busqueda bayesiana,  los hiperparametros que se van a optimizar\n",
    "# por favor, no desesperarse por lo complejo\n",
    "obj.fun <- makeSingleObjectiveFunction(\n",
    "  fn = funcion_optimizar, # la funcion que voy a maximizar\n",
    "  minimize = FALSE, # estoy Maximizando la ganancia\n",
    "  noisy = TRUE,\n",
    "  par.set = PARAM$bo_lgb, # definido al comienzo del programa\n",
    "  has.simple.signature = FALSE # paso los parametros en una lista\n",
    ")\n",
    "\n",
    "# cada 600 segundos guardo el resultado intermedio\n",
    "ctrl <- makeMBOControl(\n",
    "  save.on.disk.at.time = 600, # se graba cada 600 segundos\n",
    "  save.file.path = kbayesiana\n",
    ") # se graba cada 600 segundos\n",
    "\n",
    "# indico la cantidad de iteraciones que va a tener la Bayesian Optimization\n",
    "ctrl <- setMBOControlTermination(\n",
    "  ctrl,\n",
    "  iters = PARAM$bo_iteraciones\n",
    ") # cantidad de iteraciones\n",
    "\n",
    "# defino el método estandar para la creacion de los puntos iniciales,\n",
    "# los \"No Inteligentes\"\n",
    "ctrl <- setMBOControlInfill(ctrl, crit = makeMBOInfillCritEI())\n",
    "\n",
    "\n",
    "# establezco la funcion que busca el maximo\n",
    "surr.km <- makeLearner(\n",
    "  \"regr.km\",\n",
    "  predict.type = \"se\",\n",
    "  covtype = \"matern3_2\",\n",
    "  control = list(trace = TRUE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# inicio la optimizacion bayesiana\n",
    "if (!file.exists(kbayesiana)) {\n",
    "  run <- mbo(obj.fun, learner = surr.km, control = ctrl)\n",
    "} else {\n",
    "  run <- mboContinue(kbayesiana) # retomo en caso que ya exista\n",
    "}\n",
    "\n",
    "\n",
    "cat(\"\\n\\nLa optimizacion Bayesiana ha terminado\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semilla 2, base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# limpio la memoria\n",
    "rm(list = ls()) # remove all objects\n",
    "gc() # garbage collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "require(\"data.table\")\n",
    "require(\"rlist\")\n",
    "require(\"lightgbm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# paquetes necesarios para la Bayesian Optimization\n",
    "require(\"DiceKriging\")\n",
    "require(\"mlrMBO\")\n",
    "\n",
    "# para que se detenga ante el primer error\n",
    "# y muestre el stack de funciones invocadas\n",
    "options(error = function() {\n",
    "  traceback(20)\n",
    "  options(error = NULL)\n",
    "  stop(\"exiting after script error\")\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definición de Parametros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# defino los parametros de la corrida, en una lista, la variable global  PARAM\n",
    "#  muy pronto esto se leera desde un archivo formato .yaml\n",
    "PARAM <- list()\n",
    "\n",
    "PARAM$experimento <- \"HT8230 (Clase 12, Base Sem2)\"\n",
    "\n",
    "PARAM$input$dataset <- \"datasets/competencia_03_base_lags.csv.gz\"\n",
    "\n",
    "# los meses en los que vamos a entrenar\n",
    "#  mucha magia emerger de esta eleccion\n",
    "PARAM$input$testing <- c(202107) # Último mes, lo más cercano al 202109 de kaggle\n",
    "PARAM$input$validation <- c(202106)\n",
    "PARAM$input$training <- c(202105, 202104, 202103, 202102, 202101,202012) # 6 meses de entrenamiento\t\n",
    "\n",
    "# un undersampling de 0.1  toma solo el 10% de los CONTINUA\n",
    "PARAM$trainingstrategy$undersampling <- 1.0\n",
    "PARAM$trainingstrategy$semilla_azar <- c(279511, 279523, 279541, 279551, 279571)  # Aqui poner su  primer  semilla/ pongo todas mis semillas\n",
    "\n",
    "PARAM$hyperparametertuning$POS_ganancia <- 273000\n",
    "PARAM$hyperparametertuning$NEG_ganancia <- -7000\n",
    "\n",
    "# Aqui va semilla\n",
    "PARAM$lgb_semilla <- 279523"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Hiperparametros FIJOS de  lightgbm\n",
    "PARAM$lgb_basicos <- list(\n",
    "  boosting = \"gbdt\", # puede ir  dart  , ni pruebe random_forest\n",
    "  objective = \"binary\",\n",
    "  metric = \"custom\",\n",
    "  first_metric_only = TRUE,\n",
    "  boost_from_average = TRUE,\n",
    "  feature_pre_filter = FALSE,\n",
    "  force_row_wise = TRUE, # para reducir warnings\n",
    "  verbosity = -100,\n",
    "  max_depth = -1L, # -1 significa no limitar,  por ahora lo dejo fijo\n",
    "  min_gain_to_split = 0.0, # min_gain_to_split >= 0.0\n",
    "  min_sum_hessian_in_leaf = 0.001, #  min_sum_hessian_in_leaf >= 0.0\n",
    "  lambda_l1 = 0.0, # lambda_l1 >= 0.0\n",
    "  lambda_l2 = 0.0, # lambda_l2 >= 0.0\n",
    "  max_bin = 31L, # lo debo dejar fijo, no participa de la BO\n",
    "  num_iterations = 9999, # un numero muy grande, lo limita early_stopping_rounds\n",
    "\n",
    "  bagging_fraction = 1.0, # 0.0 < bagging_fraction <= 1.0\n",
    "  pos_bagging_fraction = 1.0, # 0.0 < pos_bagging_fraction <= 1.0\n",
    "  neg_bagging_fraction = 1.0, # 0.0 < neg_bagging_fraction <= 1.0\n",
    "  is_unbalance = FALSE, #\n",
    "  scale_pos_weight = 1.0, # scale_pos_weight > 0.0\n",
    "\n",
    "  drop_rate = 0.1, # 0.0 < neg_bagging_fraction <= 1.0\n",
    "  max_drop = 50, # <=0 means no limit\n",
    "  skip_drop = 0.5, # 0.0 <= skip_drop <= 1.0\n",
    "\n",
    "  extra_trees = TRUE, # Magic Sauce\n",
    "\n",
    "  seed = PARAM$lgb_semilla\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Aqui se cargan los hiperparametros que se optimizan\n",
    "#  en la Bayesian Optimization\n",
    "PARAM$bo_lgb <- makeParamSet(\n",
    "  makeNumericParam(\"learning_rate\", lower = 0.02, upper = 0.3),\n",
    "  makeNumericParam(\"feature_fraction\", lower = 0.01, upper = 1.0),\n",
    "  makeIntegerParam(\"num_leaves\", lower = 8L, upper = 1024L),\n",
    "  makeIntegerParam(\"min_data_in_leaf\", lower = 100L, upper = 50000L)\n",
    ")\n",
    "\n",
    "# si usted es ambicioso, y tiene paciencia, podria subir este valor a 100\n",
    "PARAM$bo_iteraciones <- 50 # iteraciones de la Optimizacion Bayesiana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# graba a un archivo los componentes de lista\n",
    "# para el primer registro, escribe antes los titulos\n",
    "\n",
    "loguear <- function(\n",
    "    reg, arch = NA, folder = \"./exp/\",\n",
    "    ext = \".txt\", verbose = TRUE) {\n",
    "  archivo <- arch\n",
    "  if (is.na(arch)) archivo <- paste0(folder, substitute(reg), ext)\n",
    "\n",
    "  if (!file.exists(archivo)) # Escribo los titulos\n",
    "    {\n",
    "      linea <- paste0(\n",
    "        \"fecha\\t\",\n",
    "        paste(list.names(reg), collapse = \"\\t\"), \"\\n\"\n",
    "      )\n",
    "\n",
    "      cat(linea, file = archivo)\n",
    "    }\n",
    "\n",
    "  linea <- paste0(\n",
    "    format(Sys.time(), \"%Y%m%d %H%M%S\"), \"\\t\", # la fecha y hora\n",
    "    gsub(\", \", \"\\t\", toString(reg)), \"\\n\"\n",
    "  )\n",
    "\n",
    "  cat(linea, file = archivo, append = TRUE) # grabo al archivo\n",
    "\n",
    "  if (verbose) cat(linea) # imprimo por pantalla\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------\n",
    "GLOBAL_arbol <- 0L\n",
    "GLOBAL_gan_max <- -Inf\n",
    "vcant_optima <- c()\n",
    "\n",
    "fganancia_lgbm_meseta <- function(probs, datos) {\n",
    "  vlabels <- get_field(datos, \"label\")\n",
    "  vpesos <- get_field(datos, \"weight\")\n",
    "\n",
    "\n",
    "  GLOBAL_arbol <<- GLOBAL_arbol + 1\n",
    "  tbl <- as.data.table(list(\n",
    "    \"prob\" = probs,\n",
    "    \"gan\" = ifelse(vlabels == 1 & vpesos > 1,\n",
    "      PARAM$hyperparametertuning$POS_ganancia,\n",
    "      PARAM$hyperparametertuning$NEG_ganancia  )\n",
    "  ))\n",
    "\n",
    "  setorder(tbl, -prob)\n",
    "  tbl[, posicion := .I]\n",
    "  tbl[, gan_acum := cumsum(gan)]\n",
    "\n",
    "  tbl[, gan_suavizada :=\n",
    "    frollmean(\n",
    "      x = gan_acum, n = 2001, align = \"center\",\n",
    "      na.rm = TRUE, hasNA = TRUE\n",
    "    )]\n",
    "\n",
    "  gan <- tbl[, max(gan_suavizada, na.rm = TRUE)]\n",
    "\n",
    "\n",
    "  pos <- which.max(tbl[, gan_suavizada])\n",
    "  vcant_optima <<- c(vcant_optima, pos)\n",
    "\n",
    "  if (GLOBAL_arbol %% 10 == 0) {\n",
    "    if (gan > GLOBAL_gan_max) GLOBAL_gan_max <<- gan\n",
    "\n",
    "    cat(\"\\r\")\n",
    "    cat(\n",
    "      \"Validate \", GLOBAL_iteracion, \" \", \" \",\n",
    "      GLOBAL_arbol, \"  \", gan, \"   \", GLOBAL_gan_max, \"   \"\n",
    "    )\n",
    "  }\n",
    "\n",
    "\n",
    "  return(list(\n",
    "    \"name\" = \"ganancia\",\n",
    "    \"value\" = gan,\n",
    "    \"higher_better\" = TRUE\n",
    "  ))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------\n",
    "\n",
    "EstimarGanancia_lightgbm <- function(x) {\n",
    "  gc()\n",
    "  GLOBAL_iteracion <<- GLOBAL_iteracion + 1L\n",
    "\n",
    "  # hago la union de los parametros basicos y los moviles que vienen en x\n",
    "  param_completo <- c(PARAM$lgb_basicos, x)\n",
    "\n",
    "  param_completo$early_stopping_rounds <-\n",
    "    as.integer(400 + 4 / param_completo$learning_rate)\n",
    "\n",
    "  GLOBAL_arbol <<- 0L\n",
    "  GLOBAL_gan_max <<- -Inf\n",
    "  vcant_optima <<- c()\n",
    "  set.seed(PARAM$lgb_semilla, kind = \"L'Ecuyer-CMRG\")\n",
    "  modelo_train <- lgb.train(\n",
    "    data = dtrain,\n",
    "    valids = list(valid = dvalidate),\n",
    "    eval = fganancia_lgbm_meseta,\n",
    "    param = param_completo,\n",
    "    verbose = -100\n",
    "  )\n",
    "\n",
    "  cat(\"\\n\")\n",
    "\n",
    "  cant_corte <- vcant_optima[modelo_train$best_iter]\n",
    "\n",
    "  # aplico el modelo a testing y calculo la ganancia\n",
    "  prediccion <- predict(\n",
    "    modelo_train,\n",
    "    data.matrix(dataset_test[, campos_buenos, with = FALSE])\n",
    "  )\n",
    "\n",
    "  tbl <- copy(dataset_test[, list(\"gan\" = ifelse(clase_ternaria == \"BAJA+2\",\n",
    "    PARAM$hyperparametertuning$POS_ganancia, \n",
    "    PARAM$hyperparametertuning$NEG_ganancia))])\n",
    "\n",
    "  tbl[, prob := prediccion]\n",
    "  setorder(tbl, -prob)\n",
    "  tbl[, gan_acum := cumsum(gan)]\n",
    "  tbl[, gan_suavizada := frollmean(\n",
    "    x = gan_acum, n = 2001,\n",
    "    align = \"center\", na.rm = TRUE, hasNA = TRUE\n",
    "  )]\n",
    "\n",
    "\n",
    "  ganancia_test <- tbl[, max(gan_suavizada, na.rm = TRUE)]\n",
    "\n",
    "  cantidad_test_normalizada <- which.max(tbl[, gan_suavizada])\n",
    "\n",
    "  rm(tbl)\n",
    "  gc()\n",
    "\n",
    "  ganancia_test_normalizada <- ganancia_test\n",
    "\n",
    "\n",
    "  # voy grabando las mejores column importance\n",
    "  if (ganancia_test_normalizada > GLOBAL_gananciamax) {\n",
    "    GLOBAL_gananciamax <<- ganancia_test_normalizada\n",
    "    tb_importancia <- as.data.table(lgb.importance(modelo_train))\n",
    "\n",
    "    fwrite(tb_importancia,\n",
    "      file = paste0(\"impo_\", sprintf(\"%03d\", GLOBAL_iteracion), \".txt\"),\n",
    "      sep = \"\\t\"\n",
    "    )\n",
    "\n",
    "    rm(tb_importancia)\n",
    "  }\n",
    "\n",
    "\n",
    "  # logueo final\n",
    "  ds <- list(\"cols\" = ncol(dtrain), \"rows\" = nrow(dtrain))\n",
    "  xx <- c(ds, copy(param_completo))\n",
    "\n",
    "  xx$early_stopping_rounds <- NULL\n",
    "  xx$num_iterations <- modelo_train$best_iter\n",
    "  xx$estimulos <- cantidad_test_normalizada\n",
    "  xx$ganancia <- ganancia_test_normalizada\n",
    "  xx$iteracion_bayesiana <- GLOBAL_iteracion\n",
    "\n",
    "  loguear(xx, arch = \"BO_log.txt\")\n",
    "\n",
    "  set.seed(PARAM$lgb_semilla, kind = \"L'Ecuyer-CMRG\")\n",
    "  return(ganancia_test_normalizada)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aquí empieza el programa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Aqui empieza el programa\n",
    "\n",
    "# Aqui se debe poner la carpeta de la computadora local\n",
    "setwd(\"~/buckets/b1/\") # Establezco el Working Directory\n",
    "\n",
    "# cargo el dataset donde voy a entrenar el modelo\n",
    "dataset <- fread(PARAM$input$dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# creo la carpeta donde va el experimento\n",
    "dir.create(\"./exp/\", showWarnings = FALSE)\n",
    "dir.create(paste0(\"./exp/\", PARAM$experimento, \"/\"), showWarnings = FALSE)\n",
    "\n",
    "# Establezco el Working Directory DEL EXPERIMENTO\n",
    "setwd(paste0(\"./exp/\", PARAM$experimento, \"/\"))\n",
    "\n",
    "# en estos archivos quedan los resultados\n",
    "kbayesiana <- paste0(PARAM$experimento, \".RDATA\")\n",
    "klog <- paste0(PARAM$experimento, \".txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Catastrophe Analysis  -------------------------------------------------------\n",
    "# deben ir cosas de este estilo\n",
    "#   dataset[foto_mes == 202006, active_quarter := NA]\n",
    "\n",
    "# Data Drifting\n",
    "# por ahora, no hago nada\n",
    "\n",
    "\n",
    "# Feature Engineering Historico  ----------------------------------------------\n",
    "#   aqui deben calcularse los  lags y  lag_delta\n",
    "#   Sin lags no hay paraiso !  corta la bocha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# ahora SI comienza la optimizacion Bayesiana\n",
    "\n",
    "GLOBAL_iteracion <- 0 # inicializo la variable global\n",
    "GLOBAL_gananciamax <- -1 # inicializo la variable global\n",
    "\n",
    "# si ya existe el archivo log, traigo hasta donde llegue\n",
    "if (file.exists(klog)) {\n",
    "  tabla_log <- fread(klog)\n",
    "  GLOBAL_iteracion <- nrow(tabla_log)\n",
    "  GLOBAL_gananciamax <- tabla_log[, max(ganancia)]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# paso la clase a binaria que tome valores {0,1}  enteros\n",
    "dataset[, clase01 := ifelse(clase_ternaria == \"CONTINUA\", 0L, 1L)]\n",
    "\n",
    "\n",
    "# los campos que se van a utilizar\n",
    "campos_buenos <- setdiff(\n",
    "  colnames(dataset),\n",
    "  c(\"clase_ternaria\", \"clase01\", \"azar\", \"training\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# defino los datos que forma parte del training\n",
    "# aqui se hace el undersampling de los CONTINUA\n",
    "set.seed(PARAM$trainingstrategy$semilla_azar)\n",
    "dataset[, azar := runif(nrow(dataset))]\n",
    "dataset[, training := 0L]\n",
    "dataset[\n",
    "  foto_mes %in% PARAM$input$training &\n",
    "    (azar <= PARAM$trainingstrategy$undersampling | clase_ternaria %in% c(\"BAJA+1\", \"BAJA+2\")),\n",
    "  training := 1L\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# dejo los datos en el formato que necesita LightGBM\n",
    "dtrain <- lgb.Dataset(\n",
    "  data = data.matrix(dataset[training == 1L, campos_buenos, with = FALSE]),\n",
    "  label = dataset[training == 1L, clase01],\n",
    "  weight = dataset[training == 1L, \n",
    "    ifelse(clase_ternaria == \"BAJA+2\", 1.0000001, \n",
    "      ifelse(clase_ternaria == \"BAJA+1\", 1.0, 1.0))],\n",
    "  free_raw_data = FALSE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# defino los datos que forman parte de validation\n",
    "#  no hay undersampling\n",
    "dataset[, validation := 0L]\n",
    "dataset[ foto_mes %in% PARAM$input$validation,  validation := 1L]\n",
    "\n",
    "dvalidate <- lgb.Dataset(\n",
    "  data = data.matrix(dataset[validation == 1L, campos_buenos, with = FALSE]),\n",
    "  label = dataset[validation == 1L, clase01],\n",
    "  weight = dataset[validation == 1L, \n",
    "    ifelse(clase_ternaria == \"BAJA+2\", 1.0000001, \n",
    "      ifelse(clase_ternaria == \"BAJA+1\", 1.0, 1.0))],\n",
    "  free_raw_data = FALSE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# defino los datos de testing\n",
    "dataset[, testing := 0L]\n",
    "dataset[ foto_mes %in% PARAM$input$testing,  testing := 1L]\n",
    "\n",
    "\n",
    "dataset_test <- dataset[testing == 1, ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# libero espacio\n",
    "rm(dataset)\n",
    "gc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Aqui comienza la configuracion de la Bayesian Optimization\n",
    "funcion_optimizar <- EstimarGanancia_lightgbm # la funcion que voy a maximizar\n",
    "\n",
    "configureMlr(show.learner.output = FALSE)\n",
    "\n",
    "# configuro la busqueda bayesiana,  los hiperparametros que se van a optimizar\n",
    "# por favor, no desesperarse por lo complejo\n",
    "obj.fun <- makeSingleObjectiveFunction(\n",
    "  fn = funcion_optimizar, # la funcion que voy a maximizar\n",
    "  minimize = FALSE, # estoy Maximizando la ganancia\n",
    "  noisy = TRUE,\n",
    "  par.set = PARAM$bo_lgb, # definido al comienzo del programa\n",
    "  has.simple.signature = FALSE # paso los parametros en una lista\n",
    ")\n",
    "\n",
    "# cada 600 segundos guardo el resultado intermedio\n",
    "ctrl <- makeMBOControl(\n",
    "  save.on.disk.at.time = 600, # se graba cada 600 segundos\n",
    "  save.file.path = kbayesiana\n",
    ") # se graba cada 600 segundos\n",
    "\n",
    "# indico la cantidad de iteraciones que va a tener la Bayesian Optimization\n",
    "ctrl <- setMBOControlTermination(\n",
    "  ctrl,\n",
    "  iters = PARAM$bo_iteraciones\n",
    ") # cantidad de iteraciones\n",
    "\n",
    "# defino el método estandar para la creacion de los puntos iniciales,\n",
    "# los \"No Inteligentes\"\n",
    "ctrl <- setMBOControlInfill(ctrl, crit = makeMBOInfillCritEI())\n",
    "\n",
    "\n",
    "# establezco la funcion que busca el maximo\n",
    "surr.km <- makeLearner(\n",
    "  \"regr.km\",\n",
    "  predict.type = \"se\",\n",
    "  covtype = \"matern3_2\",\n",
    "  control = list(trace = TRUE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# inicio la optimizacion bayesiana\n",
    "if (!file.exists(kbayesiana)) {\n",
    "  run <- mbo(obj.fun, learner = surr.km, control = ctrl)\n",
    "} else {\n",
    "  run <- mboContinue(kbayesiana) # retomo en caso que ya exista\n",
    "}\n",
    "\n",
    "\n",
    "cat(\"\\n\\nLa optimizacion Bayesiana ha terminado\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semilla 3, base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# limpio la memoria\n",
    "rm(list = ls()) # remove all objects\n",
    "gc() # garbage collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "require(\"data.table\")\n",
    "require(\"rlist\")\n",
    "require(\"lightgbm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# paquetes necesarios para la Bayesian Optimization\n",
    "require(\"DiceKriging\")\n",
    "require(\"mlrMBO\")\n",
    "\n",
    "# para que se detenga ante el primer error\n",
    "# y muestre el stack de funciones invocadas\n",
    "options(error = function() {\n",
    "  traceback(20)\n",
    "  options(error = NULL)\n",
    "  stop(\"exiting after script error\")\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definición de Parametros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# defino los parametros de la corrida, en una lista, la variable global  PARAM\n",
    "#  muy pronto esto se leera desde un archivo formato .yaml\n",
    "PARAM <- list()\n",
    "\n",
    "PARAM$experimento <- \"HT8230 (Clase 12, Base Sem3)\"\n",
    "\n",
    "PARAM$input$dataset <- \"datasets/competencia_03_base_lags.csv.gz\"\n",
    "\n",
    "# los meses en los que vamos a entrenar\n",
    "#  mucha magia emerger de esta eleccion\n",
    "PARAM$input$testing <- c(202107) # Último mes, lo más cercano al 202109 de kaggle\n",
    "PARAM$input$validation <- c(202106)\n",
    "PARAM$input$training <- c(202105, 202104, 202103, 202102, 202101,202012) # 6 meses de entrenamiento\t\n",
    "\n",
    "# un undersampling de 0.1  toma solo el 10% de los CONTINUA\n",
    "PARAM$trainingstrategy$undersampling <- 1.0\n",
    "PARAM$trainingstrategy$semilla_azar <- c(279511, 279523, 279541, 279551, 279571)  # Aqui poner su  primer  semilla/ pongo todas mis semillas\n",
    "\n",
    "PARAM$hyperparametertuning$POS_ganancia <- 273000\n",
    "PARAM$hyperparametertuning$NEG_ganancia <- -7000\n",
    "\n",
    "# Aqui va semilla\n",
    "PARAM$lgb_semilla <- 279541"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Hiperparametros FIJOS de  lightgbm\n",
    "PARAM$lgb_basicos <- list(\n",
    "  boosting = \"gbdt\", # puede ir  dart  , ni pruebe random_forest\n",
    "  objective = \"binary\",\n",
    "  metric = \"custom\",\n",
    "  first_metric_only = TRUE,\n",
    "  boost_from_average = TRUE,\n",
    "  feature_pre_filter = FALSE,\n",
    "  force_row_wise = TRUE, # para reducir warnings\n",
    "  verbosity = -100,\n",
    "  max_depth = -1L, # -1 significa no limitar,  por ahora lo dejo fijo\n",
    "  min_gain_to_split = 0.0, # min_gain_to_split >= 0.0\n",
    "  min_sum_hessian_in_leaf = 0.001, #  min_sum_hessian_in_leaf >= 0.0\n",
    "  lambda_l1 = 0.0, # lambda_l1 >= 0.0\n",
    "  lambda_l2 = 0.0, # lambda_l2 >= 0.0\n",
    "  max_bin = 31L, # lo debo dejar fijo, no participa de la BO\n",
    "  num_iterations = 9999, # un numero muy grande, lo limita early_stopping_rounds\n",
    "\n",
    "  bagging_fraction = 1.0, # 0.0 < bagging_fraction <= 1.0\n",
    "  pos_bagging_fraction = 1.0, # 0.0 < pos_bagging_fraction <= 1.0\n",
    "  neg_bagging_fraction = 1.0, # 0.0 < neg_bagging_fraction <= 1.0\n",
    "  is_unbalance = FALSE, #\n",
    "  scale_pos_weight = 1.0, # scale_pos_weight > 0.0\n",
    "\n",
    "  drop_rate = 0.1, # 0.0 < neg_bagging_fraction <= 1.0\n",
    "  max_drop = 50, # <=0 means no limit\n",
    "  skip_drop = 0.5, # 0.0 <= skip_drop <= 1.0\n",
    "\n",
    "  extra_trees = TRUE, # Magic Sauce\n",
    "\n",
    "  seed = PARAM$lgb_semilla\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Aqui se cargan los hiperparametros que se optimizan\n",
    "#  en la Bayesian Optimization\n",
    "PARAM$bo_lgb <- makeParamSet(\n",
    "  makeNumericParam(\"learning_rate\", lower = 0.02, upper = 0.3),\n",
    "  makeNumericParam(\"feature_fraction\", lower = 0.01, upper = 1.0),\n",
    "  makeIntegerParam(\"num_leaves\", lower = 8L, upper = 1024L),\n",
    "  makeIntegerParam(\"min_data_in_leaf\", lower = 100L, upper = 50000L)\n",
    ")\n",
    "\n",
    "# si usted es ambicioso, y tiene paciencia, podria subir este valor a 100\n",
    "PARAM$bo_iteraciones <- 50 # iteraciones de la Optimizacion Bayesiana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# graba a un archivo los componentes de lista\n",
    "# para el primer registro, escribe antes los titulos\n",
    "\n",
    "loguear <- function(\n",
    "    reg, arch = NA, folder = \"./exp/\",\n",
    "    ext = \".txt\", verbose = TRUE) {\n",
    "  archivo <- arch\n",
    "  if (is.na(arch)) archivo <- paste0(folder, substitute(reg), ext)\n",
    "\n",
    "  if (!file.exists(archivo)) # Escribo los titulos\n",
    "    {\n",
    "      linea <- paste0(\n",
    "        \"fecha\\t\",\n",
    "        paste(list.names(reg), collapse = \"\\t\"), \"\\n\"\n",
    "      )\n",
    "\n",
    "      cat(linea, file = archivo)\n",
    "    }\n",
    "\n",
    "  linea <- paste0(\n",
    "    format(Sys.time(), \"%Y%m%d %H%M%S\"), \"\\t\", # la fecha y hora\n",
    "    gsub(\", \", \"\\t\", toString(reg)), \"\\n\"\n",
    "  )\n",
    "\n",
    "  cat(linea, file = archivo, append = TRUE) # grabo al archivo\n",
    "\n",
    "  if (verbose) cat(linea) # imprimo por pantalla\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------\n",
    "GLOBAL_arbol <- 0L\n",
    "GLOBAL_gan_max <- -Inf\n",
    "vcant_optima <- c()\n",
    "\n",
    "fganancia_lgbm_meseta <- function(probs, datos) {\n",
    "  vlabels <- get_field(datos, \"label\")\n",
    "  vpesos <- get_field(datos, \"weight\")\n",
    "\n",
    "\n",
    "  GLOBAL_arbol <<- GLOBAL_arbol + 1\n",
    "  tbl <- as.data.table(list(\n",
    "    \"prob\" = probs,\n",
    "    \"gan\" = ifelse(vlabels == 1 & vpesos > 1,\n",
    "      PARAM$hyperparametertuning$POS_ganancia,\n",
    "      PARAM$hyperparametertuning$NEG_ganancia  )\n",
    "  ))\n",
    "\n",
    "  setorder(tbl, -prob)\n",
    "  tbl[, posicion := .I]\n",
    "  tbl[, gan_acum := cumsum(gan)]\n",
    "\n",
    "  tbl[, gan_suavizada :=\n",
    "    frollmean(\n",
    "      x = gan_acum, n = 2001, align = \"center\",\n",
    "      na.rm = TRUE, hasNA = TRUE\n",
    "    )]\n",
    "\n",
    "  gan <- tbl[, max(gan_suavizada, na.rm = TRUE)]\n",
    "\n",
    "\n",
    "  pos <- which.max(tbl[, gan_suavizada])\n",
    "  vcant_optima <<- c(vcant_optima, pos)\n",
    "\n",
    "  if (GLOBAL_arbol %% 10 == 0) {\n",
    "    if (gan > GLOBAL_gan_max) GLOBAL_gan_max <<- gan\n",
    "\n",
    "    cat(\"\\r\")\n",
    "    cat(\n",
    "      \"Validate \", GLOBAL_iteracion, \" \", \" \",\n",
    "      GLOBAL_arbol, \"  \", gan, \"   \", GLOBAL_gan_max, \"   \"\n",
    "    )\n",
    "  }\n",
    "\n",
    "\n",
    "  return(list(\n",
    "    \"name\" = \"ganancia\",\n",
    "    \"value\" = gan,\n",
    "    \"higher_better\" = TRUE\n",
    "  ))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------\n",
    "\n",
    "EstimarGanancia_lightgbm <- function(x) {\n",
    "  gc()\n",
    "  GLOBAL_iteracion <<- GLOBAL_iteracion + 1L\n",
    "\n",
    "  # hago la union de los parametros basicos y los moviles que vienen en x\n",
    "  param_completo <- c(PARAM$lgb_basicos, x)\n",
    "\n",
    "  param_completo$early_stopping_rounds <-\n",
    "    as.integer(400 + 4 / param_completo$learning_rate)\n",
    "\n",
    "  GLOBAL_arbol <<- 0L\n",
    "  GLOBAL_gan_max <<- -Inf\n",
    "  vcant_optima <<- c()\n",
    "  set.seed(PARAM$lgb_semilla, kind = \"L'Ecuyer-CMRG\")\n",
    "  modelo_train <- lgb.train(\n",
    "    data = dtrain,\n",
    "    valids = list(valid = dvalidate),\n",
    "    eval = fganancia_lgbm_meseta,\n",
    "    param = param_completo,\n",
    "    verbose = -100\n",
    "  )\n",
    "\n",
    "  cat(\"\\n\")\n",
    "\n",
    "  cant_corte <- vcant_optima[modelo_train$best_iter]\n",
    "\n",
    "  # aplico el modelo a testing y calculo la ganancia\n",
    "  prediccion <- predict(\n",
    "    modelo_train,\n",
    "    data.matrix(dataset_test[, campos_buenos, with = FALSE])\n",
    "  )\n",
    "\n",
    "  tbl <- copy(dataset_test[, list(\"gan\" = ifelse(clase_ternaria == \"BAJA+2\",\n",
    "    PARAM$hyperparametertuning$POS_ganancia, \n",
    "    PARAM$hyperparametertuning$NEG_ganancia))])\n",
    "\n",
    "  tbl[, prob := prediccion]\n",
    "  setorder(tbl, -prob)\n",
    "  tbl[, gan_acum := cumsum(gan)]\n",
    "  tbl[, gan_suavizada := frollmean(\n",
    "    x = gan_acum, n = 2001,\n",
    "    align = \"center\", na.rm = TRUE, hasNA = TRUE\n",
    "  )]\n",
    "\n",
    "\n",
    "  ganancia_test <- tbl[, max(gan_suavizada, na.rm = TRUE)]\n",
    "\n",
    "  cantidad_test_normalizada <- which.max(tbl[, gan_suavizada])\n",
    "\n",
    "  rm(tbl)\n",
    "  gc()\n",
    "\n",
    "  ganancia_test_normalizada <- ganancia_test\n",
    "\n",
    "\n",
    "  # voy grabando las mejores column importance\n",
    "  if (ganancia_test_normalizada > GLOBAL_gananciamax) {\n",
    "    GLOBAL_gananciamax <<- ganancia_test_normalizada\n",
    "    tb_importancia <- as.data.table(lgb.importance(modelo_train))\n",
    "\n",
    "    fwrite(tb_importancia,\n",
    "      file = paste0(\"impo_\", sprintf(\"%03d\", GLOBAL_iteracion), \".txt\"),\n",
    "      sep = \"\\t\"\n",
    "    )\n",
    "\n",
    "    rm(tb_importancia)\n",
    "  }\n",
    "\n",
    "\n",
    "  # logueo final\n",
    "  ds <- list(\"cols\" = ncol(dtrain), \"rows\" = nrow(dtrain))\n",
    "  xx <- c(ds, copy(param_completo))\n",
    "\n",
    "  xx$early_stopping_rounds <- NULL\n",
    "  xx$num_iterations <- modelo_train$best_iter\n",
    "  xx$estimulos <- cantidad_test_normalizada\n",
    "  xx$ganancia <- ganancia_test_normalizada\n",
    "  xx$iteracion_bayesiana <- GLOBAL_iteracion\n",
    "\n",
    "  loguear(xx, arch = \"BO_log.txt\")\n",
    "\n",
    "  set.seed(PARAM$lgb_semilla, kind = \"L'Ecuyer-CMRG\")\n",
    "  return(ganancia_test_normalizada)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aquí empieza el programa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Aqui empieza el programa\n",
    "\n",
    "# Aqui se debe poner la carpeta de la computadora local\n",
    "setwd(\"~/buckets/b1/\") # Establezco el Working Directory\n",
    "\n",
    "# cargo el dataset donde voy a entrenar el modelo\n",
    "dataset <- fread(PARAM$input$dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# creo la carpeta donde va el experimento\n",
    "dir.create(\"./exp/\", showWarnings = FALSE)\n",
    "dir.create(paste0(\"./exp/\", PARAM$experimento, \"/\"), showWarnings = FALSE)\n",
    "\n",
    "# Establezco el Working Directory DEL EXPERIMENTO\n",
    "setwd(paste0(\"./exp/\", PARAM$experimento, \"/\"))\n",
    "\n",
    "# en estos archivos quedan los resultados\n",
    "kbayesiana <- paste0(PARAM$experimento, \".RDATA\")\n",
    "klog <- paste0(PARAM$experimento, \".txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Catastrophe Analysis  -------------------------------------------------------\n",
    "# deben ir cosas de este estilo\n",
    "#   dataset[foto_mes == 202006, active_quarter := NA]\n",
    "\n",
    "# Data Drifting\n",
    "# por ahora, no hago nada\n",
    "\n",
    "\n",
    "# Feature Engineering Historico  ----------------------------------------------\n",
    "#   aqui deben calcularse los  lags y  lag_delta\n",
    "#   Sin lags no hay paraiso !  corta la bocha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# ahora SI comienza la optimizacion Bayesiana\n",
    "\n",
    "GLOBAL_iteracion <- 0 # inicializo la variable global\n",
    "GLOBAL_gananciamax <- -1 # inicializo la variable global\n",
    "\n",
    "# si ya existe el archivo log, traigo hasta donde llegue\n",
    "if (file.exists(klog)) {\n",
    "  tabla_log <- fread(klog)\n",
    "  GLOBAL_iteracion <- nrow(tabla_log)\n",
    "  GLOBAL_gananciamax <- tabla_log[, max(ganancia)]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# paso la clase a binaria que tome valores {0,1}  enteros\n",
    "dataset[, clase01 := ifelse(clase_ternaria == \"CONTINUA\", 0L, 1L)]\n",
    "\n",
    "\n",
    "# los campos que se van a utilizar\n",
    "campos_buenos <- setdiff(\n",
    "  colnames(dataset),\n",
    "  c(\"clase_ternaria\", \"clase01\", \"azar\", \"training\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# defino los datos que forma parte del training\n",
    "# aqui se hace el undersampling de los CONTINUA\n",
    "set.seed(PARAM$trainingstrategy$semilla_azar)\n",
    "dataset[, azar := runif(nrow(dataset))]\n",
    "dataset[, training := 0L]\n",
    "dataset[\n",
    "  foto_mes %in% PARAM$input$training &\n",
    "    (azar <= PARAM$trainingstrategy$undersampling | clase_ternaria %in% c(\"BAJA+1\", \"BAJA+2\")),\n",
    "  training := 1L\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# dejo los datos en el formato que necesita LightGBM\n",
    "dtrain <- lgb.Dataset(\n",
    "  data = data.matrix(dataset[training == 1L, campos_buenos, with = FALSE]),\n",
    "  label = dataset[training == 1L, clase01],\n",
    "  weight = dataset[training == 1L, \n",
    "    ifelse(clase_ternaria == \"BAJA+2\", 1.0000001, \n",
    "      ifelse(clase_ternaria == \"BAJA+1\", 1.0, 1.0))],\n",
    "  free_raw_data = FALSE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# defino los datos que forman parte de validation\n",
    "#  no hay undersampling\n",
    "dataset[, validation := 0L]\n",
    "dataset[ foto_mes %in% PARAM$input$validation,  validation := 1L]\n",
    "\n",
    "dvalidate <- lgb.Dataset(\n",
    "  data = data.matrix(dataset[validation == 1L, campos_buenos, with = FALSE]),\n",
    "  label = dataset[validation == 1L, clase01],\n",
    "  weight = dataset[validation == 1L, \n",
    "    ifelse(clase_ternaria == \"BAJA+2\", 1.0000001, \n",
    "      ifelse(clase_ternaria == \"BAJA+1\", 1.0, 1.0))],\n",
    "  free_raw_data = FALSE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# defino los datos de testing\n",
    "dataset[, testing := 0L]\n",
    "dataset[ foto_mes %in% PARAM$input$testing,  testing := 1L]\n",
    "\n",
    "\n",
    "dataset_test <- dataset[testing == 1, ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# libero espacio\n",
    "rm(dataset)\n",
    "gc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Aqui comienza la configuracion de la Bayesian Optimization\n",
    "funcion_optimizar <- EstimarGanancia_lightgbm # la funcion que voy a maximizar\n",
    "\n",
    "configureMlr(show.learner.output = FALSE)\n",
    "\n",
    "# configuro la busqueda bayesiana,  los hiperparametros que se van a optimizar\n",
    "# por favor, no desesperarse por lo complejo\n",
    "obj.fun <- makeSingleObjectiveFunction(\n",
    "  fn = funcion_optimizar, # la funcion que voy a maximizar\n",
    "  minimize = FALSE, # estoy Maximizando la ganancia\n",
    "  noisy = TRUE,\n",
    "  par.set = PARAM$bo_lgb, # definido al comienzo del programa\n",
    "  has.simple.signature = FALSE # paso los parametros en una lista\n",
    ")\n",
    "\n",
    "# cada 600 segundos guardo el resultado intermedio\n",
    "ctrl <- makeMBOControl(\n",
    "  save.on.disk.at.time = 600, # se graba cada 600 segundos\n",
    "  save.file.path = kbayesiana\n",
    ") # se graba cada 600 segundos\n",
    "\n",
    "# indico la cantidad de iteraciones que va a tener la Bayesian Optimization\n",
    "ctrl <- setMBOControlTermination(\n",
    "  ctrl,\n",
    "  iters = PARAM$bo_iteraciones\n",
    ") # cantidad de iteraciones\n",
    "\n",
    "# defino el método estandar para la creacion de los puntos iniciales,\n",
    "# los \"No Inteligentes\"\n",
    "ctrl <- setMBOControlInfill(ctrl, crit = makeMBOInfillCritEI())\n",
    "\n",
    "\n",
    "# establezco la funcion que busca el maximo\n",
    "surr.km <- makeLearner(\n",
    "  \"regr.km\",\n",
    "  predict.type = \"se\",\n",
    "  covtype = \"matern3_2\",\n",
    "  control = list(trace = TRUE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# inicio la optimizacion bayesiana\n",
    "if (!file.exists(kbayesiana)) {\n",
    "  run <- mbo(obj.fun, learner = surr.km, control = ctrl)\n",
    "} else {\n",
    "  run <- mboContinue(kbayesiana) # retomo en caso que ya exista\n",
    "}\n",
    "\n",
    "\n",
    "cat(\"\\n\\nLa optimizacion Bayesiana ha terminado\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semilla 4, base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# limpio la memoria\n",
    "rm(list = ls()) # remove all objects\n",
    "gc() # garbage collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "require(\"data.table\")\n",
    "require(\"rlist\")\n",
    "require(\"lightgbm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# paquetes necesarios para la Bayesian Optimization\n",
    "require(\"DiceKriging\")\n",
    "require(\"mlrMBO\")\n",
    "\n",
    "# para que se detenga ante el primer error\n",
    "# y muestre el stack de funciones invocadas\n",
    "options(error = function() {\n",
    "  traceback(20)\n",
    "  options(error = NULL)\n",
    "  stop(\"exiting after script error\")\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definición de Parametros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# defino los parametros de la corrida, en una lista, la variable global  PARAM\n",
    "#  muy pronto esto se leera desde un archivo formato .yaml\n",
    "PARAM <- list()\n",
    "\n",
    "PARAM$experimento <- \"HT8230 (Clase 12, Base Sem4)\"\n",
    "\n",
    "PARAM$input$dataset <- \"datasets/competencia_03_base_lags.csv.gz\"\n",
    "\n",
    "# los meses en los que vamos a entrenar\n",
    "#  mucha magia emerger de esta eleccion\n",
    "PARAM$input$testing <- c(202107) # Último mes, lo más cercano al 202109 de kaggle\n",
    "PARAM$input$validation <- c(202106)\n",
    "PARAM$input$training <- c(202105, 202104, 202103, 202102, 202101,202012) # 6 meses de entrenamiento\t\n",
    "\n",
    "# un undersampling de 0.1  toma solo el 10% de los CONTINUA\n",
    "PARAM$trainingstrategy$undersampling <- 1.0\n",
    "PARAM$trainingstrategy$semilla_azar <- c(279511, 279523, 279541, 279551, 279571)  # Aqui poner su  primer  semilla/ pongo todas mis semillas\n",
    "\n",
    "PARAM$hyperparametertuning$POS_ganancia <- 273000\n",
    "PARAM$hyperparametertuning$NEG_ganancia <- -7000\n",
    "\n",
    "# Aqui va semilla\n",
    "PARAM$lgb_semilla <- 279551"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Hiperparametros FIJOS de  lightgbm\n",
    "PARAM$lgb_basicos <- list(\n",
    "  boosting = \"gbdt\", # puede ir  dart  , ni pruebe random_forest\n",
    "  objective = \"binary\",\n",
    "  metric = \"custom\",\n",
    "  first_metric_only = TRUE,\n",
    "  boost_from_average = TRUE,\n",
    "  feature_pre_filter = FALSE,\n",
    "  force_row_wise = TRUE, # para reducir warnings\n",
    "  verbosity = -100,\n",
    "  max_depth = -1L, # -1 significa no limitar,  por ahora lo dejo fijo\n",
    "  min_gain_to_split = 0.0, # min_gain_to_split >= 0.0\n",
    "  min_sum_hessian_in_leaf = 0.001, #  min_sum_hessian_in_leaf >= 0.0\n",
    "  lambda_l1 = 0.0, # lambda_l1 >= 0.0\n",
    "  lambda_l2 = 0.0, # lambda_l2 >= 0.0\n",
    "  max_bin = 31L, # lo debo dejar fijo, no participa de la BO\n",
    "  num_iterations = 9999, # un numero muy grande, lo limita early_stopping_rounds\n",
    "\n",
    "  bagging_fraction = 1.0, # 0.0 < bagging_fraction <= 1.0\n",
    "  pos_bagging_fraction = 1.0, # 0.0 < pos_bagging_fraction <= 1.0\n",
    "  neg_bagging_fraction = 1.0, # 0.0 < neg_bagging_fraction <= 1.0\n",
    "  is_unbalance = FALSE, #\n",
    "  scale_pos_weight = 1.0, # scale_pos_weight > 0.0\n",
    "\n",
    "  drop_rate = 0.1, # 0.0 < neg_bagging_fraction <= 1.0\n",
    "  max_drop = 50, # <=0 means no limit\n",
    "  skip_drop = 0.5, # 0.0 <= skip_drop <= 1.0\n",
    "\n",
    "  extra_trees = TRUE, # Magic Sauce\n",
    "\n",
    "  seed = PARAM$lgb_semilla\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Aqui se cargan los hiperparametros que se optimizan\n",
    "#  en la Bayesian Optimization\n",
    "PARAM$bo_lgb <- makeParamSet(\n",
    "  makeNumericParam(\"learning_rate\", lower = 0.02, upper = 0.3),\n",
    "  makeNumericParam(\"feature_fraction\", lower = 0.01, upper = 1.0),\n",
    "  makeIntegerParam(\"num_leaves\", lower = 8L, upper = 1024L),\n",
    "  makeIntegerParam(\"min_data_in_leaf\", lower = 100L, upper = 50000L)\n",
    ")\n",
    "\n",
    "# si usted es ambicioso, y tiene paciencia, podria subir este valor a 100\n",
    "PARAM$bo_iteraciones <- 50 # iteraciones de la Optimizacion Bayesiana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# graba a un archivo los componentes de lista\n",
    "# para el primer registro, escribe antes los titulos\n",
    "\n",
    "loguear <- function(\n",
    "    reg, arch = NA, folder = \"./exp/\",\n",
    "    ext = \".txt\", verbose = TRUE) {\n",
    "  archivo <- arch\n",
    "  if (is.na(arch)) archivo <- paste0(folder, substitute(reg), ext)\n",
    "\n",
    "  if (!file.exists(archivo)) # Escribo los titulos\n",
    "    {\n",
    "      linea <- paste0(\n",
    "        \"fecha\\t\",\n",
    "        paste(list.names(reg), collapse = \"\\t\"), \"\\n\"\n",
    "      )\n",
    "\n",
    "      cat(linea, file = archivo)\n",
    "    }\n",
    "\n",
    "  linea <- paste0(\n",
    "    format(Sys.time(), \"%Y%m%d %H%M%S\"), \"\\t\", # la fecha y hora\n",
    "    gsub(\", \", \"\\t\", toString(reg)), \"\\n\"\n",
    "  )\n",
    "\n",
    "  cat(linea, file = archivo, append = TRUE) # grabo al archivo\n",
    "\n",
    "  if (verbose) cat(linea) # imprimo por pantalla\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------\n",
    "GLOBAL_arbol <- 0L\n",
    "GLOBAL_gan_max <- -Inf\n",
    "vcant_optima <- c()\n",
    "\n",
    "fganancia_lgbm_meseta <- function(probs, datos) {\n",
    "  vlabels <- get_field(datos, \"label\")\n",
    "  vpesos <- get_field(datos, \"weight\")\n",
    "\n",
    "\n",
    "  GLOBAL_arbol <<- GLOBAL_arbol + 1\n",
    "  tbl <- as.data.table(list(\n",
    "    \"prob\" = probs,\n",
    "    \"gan\" = ifelse(vlabels == 1 & vpesos > 1,\n",
    "      PARAM$hyperparametertuning$POS_ganancia,\n",
    "      PARAM$hyperparametertuning$NEG_ganancia  )\n",
    "  ))\n",
    "\n",
    "  setorder(tbl, -prob)\n",
    "  tbl[, posicion := .I]\n",
    "  tbl[, gan_acum := cumsum(gan)]\n",
    "\n",
    "  tbl[, gan_suavizada :=\n",
    "    frollmean(\n",
    "      x = gan_acum, n = 2001, align = \"center\",\n",
    "      na.rm = TRUE, hasNA = TRUE\n",
    "    )]\n",
    "\n",
    "  gan <- tbl[, max(gan_suavizada, na.rm = TRUE)]\n",
    "\n",
    "\n",
    "  pos <- which.max(tbl[, gan_suavizada])\n",
    "  vcant_optima <<- c(vcant_optima, pos)\n",
    "\n",
    "  if (GLOBAL_arbol %% 10 == 0) {\n",
    "    if (gan > GLOBAL_gan_max) GLOBAL_gan_max <<- gan\n",
    "\n",
    "    cat(\"\\r\")\n",
    "    cat(\n",
    "      \"Validate \", GLOBAL_iteracion, \" \", \" \",\n",
    "      GLOBAL_arbol, \"  \", gan, \"   \", GLOBAL_gan_max, \"   \"\n",
    "    )\n",
    "  }\n",
    "\n",
    "\n",
    "  return(list(\n",
    "    \"name\" = \"ganancia\",\n",
    "    \"value\" = gan,\n",
    "    \"higher_better\" = TRUE\n",
    "  ))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------\n",
    "\n",
    "EstimarGanancia_lightgbm <- function(x) {\n",
    "  gc()\n",
    "  GLOBAL_iteracion <<- GLOBAL_iteracion + 1L\n",
    "\n",
    "  # hago la union de los parametros basicos y los moviles que vienen en x\n",
    "  param_completo <- c(PARAM$lgb_basicos, x)\n",
    "\n",
    "  param_completo$early_stopping_rounds <-\n",
    "    as.integer(400 + 4 / param_completo$learning_rate)\n",
    "\n",
    "  GLOBAL_arbol <<- 0L\n",
    "  GLOBAL_gan_max <<- -Inf\n",
    "  vcant_optima <<- c()\n",
    "  set.seed(PARAM$lgb_semilla, kind = \"L'Ecuyer-CMRG\")\n",
    "  modelo_train <- lgb.train(\n",
    "    data = dtrain,\n",
    "    valids = list(valid = dvalidate),\n",
    "    eval = fganancia_lgbm_meseta,\n",
    "    param = param_completo,\n",
    "    verbose = -100\n",
    "  )\n",
    "\n",
    "  cat(\"\\n\")\n",
    "\n",
    "  cant_corte <- vcant_optima[modelo_train$best_iter]\n",
    "\n",
    "  # aplico el modelo a testing y calculo la ganancia\n",
    "  prediccion <- predict(\n",
    "    modelo_train,\n",
    "    data.matrix(dataset_test[, campos_buenos, with = FALSE])\n",
    "  )\n",
    "\n",
    "  tbl <- copy(dataset_test[, list(\"gan\" = ifelse(clase_ternaria == \"BAJA+2\",\n",
    "    PARAM$hyperparametertuning$POS_ganancia, \n",
    "    PARAM$hyperparametertuning$NEG_ganancia))])\n",
    "\n",
    "  tbl[, prob := prediccion]\n",
    "  setorder(tbl, -prob)\n",
    "  tbl[, gan_acum := cumsum(gan)]\n",
    "  tbl[, gan_suavizada := frollmean(\n",
    "    x = gan_acum, n = 2001,\n",
    "    align = \"center\", na.rm = TRUE, hasNA = TRUE\n",
    "  )]\n",
    "\n",
    "\n",
    "  ganancia_test <- tbl[, max(gan_suavizada, na.rm = TRUE)]\n",
    "\n",
    "  cantidad_test_normalizada <- which.max(tbl[, gan_suavizada])\n",
    "\n",
    "  rm(tbl)\n",
    "  gc()\n",
    "\n",
    "  ganancia_test_normalizada <- ganancia_test\n",
    "\n",
    "\n",
    "  # voy grabando las mejores column importance\n",
    "  if (ganancia_test_normalizada > GLOBAL_gananciamax) {\n",
    "    GLOBAL_gananciamax <<- ganancia_test_normalizada\n",
    "    tb_importancia <- as.data.table(lgb.importance(modelo_train))\n",
    "\n",
    "    fwrite(tb_importancia,\n",
    "      file = paste0(\"impo_\", sprintf(\"%03d\", GLOBAL_iteracion), \".txt\"),\n",
    "      sep = \"\\t\"\n",
    "    )\n",
    "\n",
    "    rm(tb_importancia)\n",
    "  }\n",
    "\n",
    "\n",
    "  # logueo final\n",
    "  ds <- list(\"cols\" = ncol(dtrain), \"rows\" = nrow(dtrain))\n",
    "  xx <- c(ds, copy(param_completo))\n",
    "\n",
    "  xx$early_stopping_rounds <- NULL\n",
    "  xx$num_iterations <- modelo_train$best_iter\n",
    "  xx$estimulos <- cantidad_test_normalizada\n",
    "  xx$ganancia <- ganancia_test_normalizada\n",
    "  xx$iteracion_bayesiana <- GLOBAL_iteracion\n",
    "\n",
    "  loguear(xx, arch = \"BO_log.txt\")\n",
    "\n",
    "  set.seed(PARAM$lgb_semilla, kind = \"L'Ecuyer-CMRG\")\n",
    "  return(ganancia_test_normalizada)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aquí empieza el programa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Aqui empieza el programa\n",
    "\n",
    "# Aqui se debe poner la carpeta de la computadora local\n",
    "setwd(\"~/buckets/b1/\") # Establezco el Working Directory\n",
    "\n",
    "# cargo el dataset donde voy a entrenar el modelo\n",
    "dataset <- fread(PARAM$input$dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# creo la carpeta donde va el experimento\n",
    "dir.create(\"./exp/\", showWarnings = FALSE)\n",
    "dir.create(paste0(\"./exp/\", PARAM$experimento, \"/\"), showWarnings = FALSE)\n",
    "\n",
    "# Establezco el Working Directory DEL EXPERIMENTO\n",
    "setwd(paste0(\"./exp/\", PARAM$experimento, \"/\"))\n",
    "\n",
    "# en estos archivos quedan los resultados\n",
    "kbayesiana <- paste0(PARAM$experimento, \".RDATA\")\n",
    "klog <- paste0(PARAM$experimento, \".txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Catastrophe Analysis  -------------------------------------------------------\n",
    "# deben ir cosas de este estilo\n",
    "#   dataset[foto_mes == 202006, active_quarter := NA]\n",
    "\n",
    "# Data Drifting\n",
    "# por ahora, no hago nada\n",
    "\n",
    "\n",
    "# Feature Engineering Historico  ----------------------------------------------\n",
    "#   aqui deben calcularse los  lags y  lag_delta\n",
    "#   Sin lags no hay paraiso !  corta la bocha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# ahora SI comienza la optimizacion Bayesiana\n",
    "\n",
    "GLOBAL_iteracion <- 0 # inicializo la variable global\n",
    "GLOBAL_gananciamax <- -1 # inicializo la variable global\n",
    "\n",
    "# si ya existe el archivo log, traigo hasta donde llegue\n",
    "if (file.exists(klog)) {\n",
    "  tabla_log <- fread(klog)\n",
    "  GLOBAL_iteracion <- nrow(tabla_log)\n",
    "  GLOBAL_gananciamax <- tabla_log[, max(ganancia)]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# paso la clase a binaria que tome valores {0,1}  enteros\n",
    "dataset[, clase01 := ifelse(clase_ternaria == \"CONTINUA\", 0L, 1L)]\n",
    "\n",
    "\n",
    "# los campos que se van a utilizar\n",
    "campos_buenos <- setdiff(\n",
    "  colnames(dataset),\n",
    "  c(\"clase_ternaria\", \"clase01\", \"azar\", \"training\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# defino los datos que forma parte del training\n",
    "# aqui se hace el undersampling de los CONTINUA\n",
    "set.seed(PARAM$trainingstrategy$semilla_azar)\n",
    "dataset[, azar := runif(nrow(dataset))]\n",
    "dataset[, training := 0L]\n",
    "dataset[\n",
    "  foto_mes %in% PARAM$input$training &\n",
    "    (azar <= PARAM$trainingstrategy$undersampling | clase_ternaria %in% c(\"BAJA+1\", \"BAJA+2\")),\n",
    "  training := 1L\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# dejo los datos en el formato que necesita LightGBM\n",
    "dtrain <- lgb.Dataset(\n",
    "  data = data.matrix(dataset[training == 1L, campos_buenos, with = FALSE]),\n",
    "  label = dataset[training == 1L, clase01],\n",
    "  weight = dataset[training == 1L, \n",
    "    ifelse(clase_ternaria == \"BAJA+2\", 1.0000001, \n",
    "      ifelse(clase_ternaria == \"BAJA+1\", 1.0, 1.0))],\n",
    "  free_raw_data = FALSE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# defino los datos que forman parte de validation\n",
    "#  no hay undersampling\n",
    "dataset[, validation := 0L]\n",
    "dataset[ foto_mes %in% PARAM$input$validation,  validation := 1L]\n",
    "\n",
    "dvalidate <- lgb.Dataset(\n",
    "  data = data.matrix(dataset[validation == 1L, campos_buenos, with = FALSE]),\n",
    "  label = dataset[validation == 1L, clase01],\n",
    "  weight = dataset[validation == 1L, \n",
    "    ifelse(clase_ternaria == \"BAJA+2\", 1.0000001, \n",
    "      ifelse(clase_ternaria == \"BAJA+1\", 1.0, 1.0))],\n",
    "  free_raw_data = FALSE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# defino los datos de testing\n",
    "dataset[, testing := 0L]\n",
    "dataset[ foto_mes %in% PARAM$input$testing,  testing := 1L]\n",
    "\n",
    "\n",
    "dataset_test <- dataset[testing == 1, ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# libero espacio\n",
    "rm(dataset)\n",
    "gc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Aqui comienza la configuracion de la Bayesian Optimization\n",
    "funcion_optimizar <- EstimarGanancia_lightgbm # la funcion que voy a maximizar\n",
    "\n",
    "configureMlr(show.learner.output = FALSE)\n",
    "\n",
    "# configuro la busqueda bayesiana,  los hiperparametros que se van a optimizar\n",
    "# por favor, no desesperarse por lo complejo\n",
    "obj.fun <- makeSingleObjectiveFunction(\n",
    "  fn = funcion_optimizar, # la funcion que voy a maximizar\n",
    "  minimize = FALSE, # estoy Maximizando la ganancia\n",
    "  noisy = TRUE,\n",
    "  par.set = PARAM$bo_lgb, # definido al comienzo del programa\n",
    "  has.simple.signature = FALSE # paso los parametros en una lista\n",
    ")\n",
    "\n",
    "# cada 600 segundos guardo el resultado intermedio\n",
    "ctrl <- makeMBOControl(\n",
    "  save.on.disk.at.time = 600, # se graba cada 600 segundos\n",
    "  save.file.path = kbayesiana\n",
    ") # se graba cada 600 segundos\n",
    "\n",
    "# indico la cantidad de iteraciones que va a tener la Bayesian Optimization\n",
    "ctrl <- setMBOControlTermination(\n",
    "  ctrl,\n",
    "  iters = PARAM$bo_iteraciones\n",
    ") # cantidad de iteraciones\n",
    "\n",
    "# defino el método estandar para la creacion de los puntos iniciales,\n",
    "# los \"No Inteligentes\"\n",
    "ctrl <- setMBOControlInfill(ctrl, crit = makeMBOInfillCritEI())\n",
    "\n",
    "\n",
    "# establezco la funcion que busca el maximo\n",
    "surr.km <- makeLearner(\n",
    "  \"regr.km\",\n",
    "  predict.type = \"se\",\n",
    "  covtype = \"matern3_2\",\n",
    "  control = list(trace = TRUE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# inicio la optimizacion bayesiana\n",
    "if (!file.exists(kbayesiana)) {\n",
    "  run <- mbo(obj.fun, learner = surr.km, control = ctrl)\n",
    "} else {\n",
    "  run <- mboContinue(kbayesiana) # retomo en caso que ya exista\n",
    "}\n",
    "\n",
    "\n",
    "cat(\"\\n\\nLa optimizacion Bayesiana ha terminado\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semilla 5, base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# limpio la memoria\n",
    "rm(list = ls()) # remove all objects\n",
    "gc() # garbage collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "require(\"data.table\")\n",
    "require(\"rlist\")\n",
    "require(\"lightgbm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# paquetes necesarios para la Bayesian Optimization\n",
    "require(\"DiceKriging\")\n",
    "require(\"mlrMBO\")\n",
    "\n",
    "# para que se detenga ante el primer error\n",
    "# y muestre el stack de funciones invocadas\n",
    "options(error = function() {\n",
    "  traceback(20)\n",
    "  options(error = NULL)\n",
    "  stop(\"exiting after script error\")\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definición de Parametros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# defino los parametros de la corrida, en una lista, la variable global  PARAM\n",
    "#  muy pronto esto se leera desde un archivo formato .yaml\n",
    "PARAM <- list()\n",
    "\n",
    "PARAM$experimento <- \"HT8230 (Clase 12, Base Sem5)\"\n",
    "\n",
    "PARAM$input$dataset <- \"datasets/competencia_03_base_lags.csv.gz\"\n",
    "\n",
    "# los meses en los que vamos a entrenar\n",
    "#  mucha magia emerger de esta eleccion\n",
    "PARAM$input$testing <- c(202107) # Último mes, lo más cercano al 202109 de kaggle\n",
    "PARAM$input$validation <- c(202106)\n",
    "PARAM$input$training <- c(202105, 202104, 202103, 202102, 202101,202012) # 6 meses de entrenamiento\t\n",
    "\n",
    "# un undersampling de 0.1  toma solo el 10% de los CONTINUA\n",
    "PARAM$trainingstrategy$undersampling <- 1.0\n",
    "PARAM$trainingstrategy$semilla_azar <- c(279511, 279523, 279541, 279551, 279571)  # Aqui poner su  primer  semilla/ pongo todas mis semillas\n",
    "\n",
    "PARAM$hyperparametertuning$POS_ganancia <- 273000\n",
    "PARAM$hyperparametertuning$NEG_ganancia <- -7000\n",
    "\n",
    "# Aqui va semilla\n",
    "PARAM$lgb_semilla <- 279571"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Hiperparametros FIJOS de  lightgbm\n",
    "PARAM$lgb_basicos <- list(\n",
    "  boosting = \"gbdt\", # puede ir  dart  , ni pruebe random_forest\n",
    "  objective = \"binary\",\n",
    "  metric = \"custom\",\n",
    "  first_metric_only = TRUE,\n",
    "  boost_from_average = TRUE,\n",
    "  feature_pre_filter = FALSE,\n",
    "  force_row_wise = TRUE, # para reducir warnings\n",
    "  verbosity = -100,\n",
    "  max_depth = -1L, # -1 significa no limitar,  por ahora lo dejo fijo\n",
    "  min_gain_to_split = 0.0, # min_gain_to_split >= 0.0\n",
    "  min_sum_hessian_in_leaf = 0.001, #  min_sum_hessian_in_leaf >= 0.0\n",
    "  lambda_l1 = 0.0, # lambda_l1 >= 0.0\n",
    "  lambda_l2 = 0.0, # lambda_l2 >= 0.0\n",
    "  max_bin = 31L, # lo debo dejar fijo, no participa de la BO\n",
    "  num_iterations = 9999, # un numero muy grande, lo limita early_stopping_rounds\n",
    "\n",
    "  bagging_fraction = 1.0, # 0.0 < bagging_fraction <= 1.0\n",
    "  pos_bagging_fraction = 1.0, # 0.0 < pos_bagging_fraction <= 1.0\n",
    "  neg_bagging_fraction = 1.0, # 0.0 < neg_bagging_fraction <= 1.0\n",
    "  is_unbalance = FALSE, #\n",
    "  scale_pos_weight = 1.0, # scale_pos_weight > 0.0\n",
    "\n",
    "  drop_rate = 0.1, # 0.0 < neg_bagging_fraction <= 1.0\n",
    "  max_drop = 50, # <=0 means no limit\n",
    "  skip_drop = 0.5, # 0.0 <= skip_drop <= 1.0\n",
    "\n",
    "  extra_trees = TRUE, # Magic Sauce\n",
    "\n",
    "  seed = PARAM$lgb_semilla\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Aqui se cargan los hiperparametros que se optimizan\n",
    "#  en la Bayesian Optimization\n",
    "PARAM$bo_lgb <- makeParamSet(\n",
    "  makeNumericParam(\"learning_rate\", lower = 0.02, upper = 0.3),\n",
    "  makeNumericParam(\"feature_fraction\", lower = 0.01, upper = 1.0),\n",
    "  makeIntegerParam(\"num_leaves\", lower = 8L, upper = 1024L),\n",
    "  makeIntegerParam(\"min_data_in_leaf\", lower = 100L, upper = 50000L)\n",
    ")\n",
    "\n",
    "# si usted es ambicioso, y tiene paciencia, podria subir este valor a 100\n",
    "PARAM$bo_iteraciones <- 50 # iteraciones de la Optimizacion Bayesiana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# graba a un archivo los componentes de lista\n",
    "# para el primer registro, escribe antes los titulos\n",
    "\n",
    "loguear <- function(\n",
    "    reg, arch = NA, folder = \"./exp/\",\n",
    "    ext = \".txt\", verbose = TRUE) {\n",
    "  archivo <- arch\n",
    "  if (is.na(arch)) archivo <- paste0(folder, substitute(reg), ext)\n",
    "\n",
    "  if (!file.exists(archivo)) # Escribo los titulos\n",
    "    {\n",
    "      linea <- paste0(\n",
    "        \"fecha\\t\",\n",
    "        paste(list.names(reg), collapse = \"\\t\"), \"\\n\"\n",
    "      )\n",
    "\n",
    "      cat(linea, file = archivo)\n",
    "    }\n",
    "\n",
    "  linea <- paste0(\n",
    "    format(Sys.time(), \"%Y%m%d %H%M%S\"), \"\\t\", # la fecha y hora\n",
    "    gsub(\", \", \"\\t\", toString(reg)), \"\\n\"\n",
    "  )\n",
    "\n",
    "  cat(linea, file = archivo, append = TRUE) # grabo al archivo\n",
    "\n",
    "  if (verbose) cat(linea) # imprimo por pantalla\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------\n",
    "GLOBAL_arbol <- 0L\n",
    "GLOBAL_gan_max <- -Inf\n",
    "vcant_optima <- c()\n",
    "\n",
    "fganancia_lgbm_meseta <- function(probs, datos) {\n",
    "  vlabels <- get_field(datos, \"label\")\n",
    "  vpesos <- get_field(datos, \"weight\")\n",
    "\n",
    "\n",
    "  GLOBAL_arbol <<- GLOBAL_arbol + 1\n",
    "  tbl <- as.data.table(list(\n",
    "    \"prob\" = probs,\n",
    "    \"gan\" = ifelse(vlabels == 1 & vpesos > 1,\n",
    "      PARAM$hyperparametertuning$POS_ganancia,\n",
    "      PARAM$hyperparametertuning$NEG_ganancia  )\n",
    "  ))\n",
    "\n",
    "  setorder(tbl, -prob)\n",
    "  tbl[, posicion := .I]\n",
    "  tbl[, gan_acum := cumsum(gan)]\n",
    "\n",
    "  tbl[, gan_suavizada :=\n",
    "    frollmean(\n",
    "      x = gan_acum, n = 2001, align = \"center\",\n",
    "      na.rm = TRUE, hasNA = TRUE\n",
    "    )]\n",
    "\n",
    "  gan <- tbl[, max(gan_suavizada, na.rm = TRUE)]\n",
    "\n",
    "\n",
    "  pos <- which.max(tbl[, gan_suavizada])\n",
    "  vcant_optima <<- c(vcant_optima, pos)\n",
    "\n",
    "  if (GLOBAL_arbol %% 10 == 0) {\n",
    "    if (gan > GLOBAL_gan_max) GLOBAL_gan_max <<- gan\n",
    "\n",
    "    cat(\"\\r\")\n",
    "    cat(\n",
    "      \"Validate \", GLOBAL_iteracion, \" \", \" \",\n",
    "      GLOBAL_arbol, \"  \", gan, \"   \", GLOBAL_gan_max, \"   \"\n",
    "    )\n",
    "  }\n",
    "\n",
    "\n",
    "  return(list(\n",
    "    \"name\" = \"ganancia\",\n",
    "    \"value\" = gan,\n",
    "    \"higher_better\" = TRUE\n",
    "  ))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------\n",
    "\n",
    "EstimarGanancia_lightgbm <- function(x) {\n",
    "  gc()\n",
    "  GLOBAL_iteracion <<- GLOBAL_iteracion + 1L\n",
    "\n",
    "  # hago la union de los parametros basicos y los moviles que vienen en x\n",
    "  param_completo <- c(PARAM$lgb_basicos, x)\n",
    "\n",
    "  param_completo$early_stopping_rounds <-\n",
    "    as.integer(400 + 4 / param_completo$learning_rate)\n",
    "\n",
    "  GLOBAL_arbol <<- 0L\n",
    "  GLOBAL_gan_max <<- -Inf\n",
    "  vcant_optima <<- c()\n",
    "  set.seed(PARAM$lgb_semilla, kind = \"L'Ecuyer-CMRG\")\n",
    "  modelo_train <- lgb.train(\n",
    "    data = dtrain,\n",
    "    valids = list(valid = dvalidate),\n",
    "    eval = fganancia_lgbm_meseta,\n",
    "    param = param_completo,\n",
    "    verbose = -100\n",
    "  )\n",
    "\n",
    "  cat(\"\\n\")\n",
    "\n",
    "  cant_corte <- vcant_optima[modelo_train$best_iter]\n",
    "\n",
    "  # aplico el modelo a testing y calculo la ganancia\n",
    "  prediccion <- predict(\n",
    "    modelo_train,\n",
    "    data.matrix(dataset_test[, campos_buenos, with = FALSE])\n",
    "  )\n",
    "\n",
    "  tbl <- copy(dataset_test[, list(\"gan\" = ifelse(clase_ternaria == \"BAJA+2\",\n",
    "    PARAM$hyperparametertuning$POS_ganancia, \n",
    "    PARAM$hyperparametertuning$NEG_ganancia))])\n",
    "\n",
    "  tbl[, prob := prediccion]\n",
    "  setorder(tbl, -prob)\n",
    "  tbl[, gan_acum := cumsum(gan)]\n",
    "  tbl[, gan_suavizada := frollmean(\n",
    "    x = gan_acum, n = 2001,\n",
    "    align = \"center\", na.rm = TRUE, hasNA = TRUE\n",
    "  )]\n",
    "\n",
    "\n",
    "  ganancia_test <- tbl[, max(gan_suavizada, na.rm = TRUE)]\n",
    "\n",
    "  cantidad_test_normalizada <- which.max(tbl[, gan_suavizada])\n",
    "\n",
    "  rm(tbl)\n",
    "  gc()\n",
    "\n",
    "  ganancia_test_normalizada <- ganancia_test\n",
    "\n",
    "\n",
    "  # voy grabando las mejores column importance\n",
    "  if (ganancia_test_normalizada > GLOBAL_gananciamax) {\n",
    "    GLOBAL_gananciamax <<- ganancia_test_normalizada\n",
    "    tb_importancia <- as.data.table(lgb.importance(modelo_train))\n",
    "\n",
    "    fwrite(tb_importancia,\n",
    "      file = paste0(\"impo_\", sprintf(\"%03d\", GLOBAL_iteracion), \".txt\"),\n",
    "      sep = \"\\t\"\n",
    "    )\n",
    "\n",
    "    rm(tb_importancia)\n",
    "  }\n",
    "\n",
    "\n",
    "  # logueo final\n",
    "  ds <- list(\"cols\" = ncol(dtrain), \"rows\" = nrow(dtrain))\n",
    "  xx <- c(ds, copy(param_completo))\n",
    "\n",
    "  xx$early_stopping_rounds <- NULL\n",
    "  xx$num_iterations <- modelo_train$best_iter\n",
    "  xx$estimulos <- cantidad_test_normalizada\n",
    "  xx$ganancia <- ganancia_test_normalizada\n",
    "  xx$iteracion_bayesiana <- GLOBAL_iteracion\n",
    "\n",
    "  loguear(xx, arch = \"BO_log.txt\")\n",
    "\n",
    "  set.seed(PARAM$lgb_semilla, kind = \"L'Ecuyer-CMRG\")\n",
    "  return(ganancia_test_normalizada)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aquí empieza el programa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Aqui empieza el programa\n",
    "\n",
    "# Aqui se debe poner la carpeta de la computadora local\n",
    "setwd(\"~/buckets/b1/\") # Establezco el Working Directory\n",
    "\n",
    "# cargo el dataset donde voy a entrenar el modelo\n",
    "dataset <- fread(PARAM$input$dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# creo la carpeta donde va el experimento\n",
    "dir.create(\"./exp/\", showWarnings = FALSE)\n",
    "dir.create(paste0(\"./exp/\", PARAM$experimento, \"/\"), showWarnings = FALSE)\n",
    "\n",
    "# Establezco el Working Directory DEL EXPERIMENTO\n",
    "setwd(paste0(\"./exp/\", PARAM$experimento, \"/\"))\n",
    "\n",
    "# en estos archivos quedan los resultados\n",
    "kbayesiana <- paste0(PARAM$experimento, \".RDATA\")\n",
    "klog <- paste0(PARAM$experimento, \".txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Catastrophe Analysis  -------------------------------------------------------\n",
    "# deben ir cosas de este estilo\n",
    "#   dataset[foto_mes == 202006, active_quarter := NA]\n",
    "\n",
    "# Data Drifting\n",
    "# por ahora, no hago nada\n",
    "\n",
    "\n",
    "# Feature Engineering Historico  ----------------------------------------------\n",
    "#   aqui deben calcularse los  lags y  lag_delta\n",
    "#   Sin lags no hay paraiso !  corta la bocha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# ahora SI comienza la optimizacion Bayesiana\n",
    "\n",
    "GLOBAL_iteracion <- 0 # inicializo la variable global\n",
    "GLOBAL_gananciamax <- -1 # inicializo la variable global\n",
    "\n",
    "# si ya existe el archivo log, traigo hasta donde llegue\n",
    "if (file.exists(klog)) {\n",
    "  tabla_log <- fread(klog)\n",
    "  GLOBAL_iteracion <- nrow(tabla_log)\n",
    "  GLOBAL_gananciamax <- tabla_log[, max(ganancia)]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# paso la clase a binaria que tome valores {0,1}  enteros\n",
    "dataset[, clase01 := ifelse(clase_ternaria == \"CONTINUA\", 0L, 1L)]\n",
    "\n",
    "\n",
    "# los campos que se van a utilizar\n",
    "campos_buenos <- setdiff(\n",
    "  colnames(dataset),\n",
    "  c(\"clase_ternaria\", \"clase01\", \"azar\", \"training\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# defino los datos que forma parte del training\n",
    "# aqui se hace el undersampling de los CONTINUA\n",
    "set.seed(PARAM$trainingstrategy$semilla_azar)\n",
    "dataset[, azar := runif(nrow(dataset))]\n",
    "dataset[, training := 0L]\n",
    "dataset[\n",
    "  foto_mes %in% PARAM$input$training &\n",
    "    (azar <= PARAM$trainingstrategy$undersampling | clase_ternaria %in% c(\"BAJA+1\", \"BAJA+2\")),\n",
    "  training := 1L\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# dejo los datos en el formato que necesita LightGBM\n",
    "dtrain <- lgb.Dataset(\n",
    "  data = data.matrix(dataset[training == 1L, campos_buenos, with = FALSE]),\n",
    "  label = dataset[training == 1L, clase01],\n",
    "  weight = dataset[training == 1L, \n",
    "    ifelse(clase_ternaria == \"BAJA+2\", 1.0000001, \n",
    "      ifelse(clase_ternaria == \"BAJA+1\", 1.0, 1.0))],\n",
    "  free_raw_data = FALSE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# defino los datos que forman parte de validation\n",
    "#  no hay undersampling\n",
    "dataset[, validation := 0L]\n",
    "dataset[ foto_mes %in% PARAM$input$validation,  validation := 1L]\n",
    "\n",
    "dvalidate <- lgb.Dataset(\n",
    "  data = data.matrix(dataset[validation == 1L, campos_buenos, with = FALSE]),\n",
    "  label = dataset[validation == 1L, clase01],\n",
    "  weight = dataset[validation == 1L, \n",
    "    ifelse(clase_ternaria == \"BAJA+2\", 1.0000001, \n",
    "      ifelse(clase_ternaria == \"BAJA+1\", 1.0, 1.0))],\n",
    "  free_raw_data = FALSE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# defino los datos de testing\n",
    "dataset[, testing := 0L]\n",
    "dataset[ foto_mes %in% PARAM$input$testing,  testing := 1L]\n",
    "\n",
    "\n",
    "dataset_test <- dataset[testing == 1, ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# libero espacio\n",
    "rm(dataset)\n",
    "gc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Aqui comienza la configuracion de la Bayesian Optimization\n",
    "funcion_optimizar <- EstimarGanancia_lightgbm # la funcion que voy a maximizar\n",
    "\n",
    "configureMlr(show.learner.output = FALSE)\n",
    "\n",
    "# configuro la busqueda bayesiana,  los hiperparametros que se van a optimizar\n",
    "# por favor, no desesperarse por lo complejo\n",
    "obj.fun <- makeSingleObjectiveFunction(\n",
    "  fn = funcion_optimizar, # la funcion que voy a maximizar\n",
    "  minimize = FALSE, # estoy Maximizando la ganancia\n",
    "  noisy = TRUE,\n",
    "  par.set = PARAM$bo_lgb, # definido al comienzo del programa\n",
    "  has.simple.signature = FALSE # paso los parametros en una lista\n",
    ")\n",
    "\n",
    "# cada 600 segundos guardo el resultado intermedio\n",
    "ctrl <- makeMBOControl(\n",
    "  save.on.disk.at.time = 600, # se graba cada 600 segundos\n",
    "  save.file.path = kbayesiana\n",
    ") # se graba cada 600 segundos\n",
    "\n",
    "# indico la cantidad de iteraciones que va a tener la Bayesian Optimization\n",
    "ctrl <- setMBOControlTermination(\n",
    "  ctrl,\n",
    "  iters = PARAM$bo_iteraciones\n",
    ") # cantidad de iteraciones\n",
    "\n",
    "# defino el método estandar para la creacion de los puntos iniciales,\n",
    "# los \"No Inteligentes\"\n",
    "ctrl <- setMBOControlInfill(ctrl, crit = makeMBOInfillCritEI())\n",
    "\n",
    "\n",
    "# establezco la funcion que busca el maximo\n",
    "surr.km <- makeLearner(\n",
    "  \"regr.km\",\n",
    "  predict.type = \"se\",\n",
    "  covtype = \"matern3_2\",\n",
    "  control = list(trace = TRUE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# inicio la optimizacion bayesiana\n",
    "if (!file.exists(kbayesiana)) {\n",
    "  run <- mbo(obj.fun, learner = surr.km, control = ctrl)\n",
    "} else {\n",
    "  run <- mboContinue(kbayesiana) # retomo en caso que ya exista\n",
    "}\n",
    "\n",
    "\n",
    "cat(\"\\n\\nLa optimizacion Bayesiana ha terminado\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semilla 1, ajustado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# limpio la memoria\n",
    "rm(list = ls()) # remove all objects\n",
    "gc() # garbage collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "require(\"data.table\")\n",
    "require(\"rlist\")\n",
    "require(\"lightgbm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# paquetes necesarios para la Bayesian Optimization\n",
    "require(\"DiceKriging\")\n",
    "require(\"mlrMBO\")\n",
    "\n",
    "# para que se detenga ante el primer error\n",
    "# y muestre el stack de funciones invocadas\n",
    "options(error = function() {\n",
    "  traceback(20)\n",
    "  options(error = NULL)\n",
    "  stop(\"exiting after script error\")\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definición de Parametros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# defino los parametros de la corrida, en una lista, la variable global  PARAM\n",
    "#  muy pronto esto se leera desde un archivo formato .yaml\n",
    "PARAM <- list()\n",
    "\n",
    "PARAM$experimento <- \"HT8230 (Clase 12, Ajustado Sem1)\"\n",
    "\n",
    "PARAM$input$dataset <- \"datasets/competencia_03_infla_ajustado_lags.csv.gz\"\n",
    "\n",
    "# los meses en los que vamos a entrenar\n",
    "#  mucha magia emerger de esta eleccion\n",
    "PARAM$input$testing <- c(202107) # Último mes, lo más cercano al 202109 de kaggle\n",
    "PARAM$input$validation <- c(202106)\n",
    "PARAM$input$training <- c(202105, 202104, 202103, 202102, 202101,202012) # 6 meses de entrenamiento\t\n",
    "\n",
    "# un undersampling de 0.1  toma solo el 10% de los CONTINUA\n",
    "PARAM$trainingstrategy$undersampling <- 1.0\n",
    "PARAM$trainingstrategy$semilla_azar <- c(279511, 279523, 279541, 279551, 279571)  # Aqui poner su  primer  semilla/ pongo todas mis semillas\n",
    "\n",
    "PARAM$hyperparametertuning$POS_ganancia <- 273000\n",
    "PARAM$hyperparametertuning$NEG_ganancia <- -7000\n",
    "\n",
    "# Aqui va semilla\n",
    "PARAM$lgb_semilla <- 279511"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Hiperparametros FIJOS de  lightgbm\n",
    "PARAM$lgb_basicos <- list(\n",
    "  boosting = \"gbdt\", # puede ir  dart  , ni pruebe random_forest\n",
    "  objective = \"binary\",\n",
    "  metric = \"custom\",\n",
    "  first_metric_only = TRUE,\n",
    "  boost_from_average = TRUE,\n",
    "  feature_pre_filter = FALSE,\n",
    "  force_row_wise = TRUE, # para reducir warnings\n",
    "  verbosity = -100,\n",
    "  max_depth = -1L, # -1 significa no limitar,  por ahora lo dejo fijo\n",
    "  min_gain_to_split = 0.0, # min_gain_to_split >= 0.0\n",
    "  min_sum_hessian_in_leaf = 0.001, #  min_sum_hessian_in_leaf >= 0.0\n",
    "  lambda_l1 = 0.0, # lambda_l1 >= 0.0\n",
    "  lambda_l2 = 0.0, # lambda_l2 >= 0.0\n",
    "  max_bin = 31L, # lo debo dejar fijo, no participa de la BO\n",
    "  num_iterations = 9999, # un numero muy grande, lo limita early_stopping_rounds\n",
    "\n",
    "  bagging_fraction = 1.0, # 0.0 < bagging_fraction <= 1.0\n",
    "  pos_bagging_fraction = 1.0, # 0.0 < pos_bagging_fraction <= 1.0\n",
    "  neg_bagging_fraction = 1.0, # 0.0 < neg_bagging_fraction <= 1.0\n",
    "  is_unbalance = FALSE, #\n",
    "  scale_pos_weight = 1.0, # scale_pos_weight > 0.0\n",
    "\n",
    "  drop_rate = 0.1, # 0.0 < neg_bagging_fraction <= 1.0\n",
    "  max_drop = 50, # <=0 means no limit\n",
    "  skip_drop = 0.5, # 0.0 <= skip_drop <= 1.0\n",
    "\n",
    "  extra_trees = TRUE, # Magic Sauce\n",
    "\n",
    "  seed = PARAM$lgb_semilla\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Aqui se cargan los hiperparametros que se optimizan\n",
    "#  en la Bayesian Optimization\n",
    "PARAM$bo_lgb <- makeParamSet(\n",
    "  makeNumericParam(\"learning_rate\", lower = 0.02, upper = 0.3),\n",
    "  makeNumericParam(\"feature_fraction\", lower = 0.01, upper = 1.0),\n",
    "  makeIntegerParam(\"num_leaves\", lower = 8L, upper = 1024L),\n",
    "  makeIntegerParam(\"min_data_in_leaf\", lower = 100L, upper = 50000L)\n",
    ")\n",
    "\n",
    "# si usted es ambicioso, y tiene paciencia, podria subir este valor a 100\n",
    "PARAM$bo_iteraciones <- 50 # iteraciones de la Optimizacion Bayesiana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# graba a un archivo los componentes de lista\n",
    "# para el primer registro, escribe antes los titulos\n",
    "\n",
    "loguear <- function(\n",
    "    reg, arch = NA, folder = \"./exp/\",\n",
    "    ext = \".txt\", verbose = TRUE) {\n",
    "  archivo <- arch\n",
    "  if (is.na(arch)) archivo <- paste0(folder, substitute(reg), ext)\n",
    "\n",
    "  if (!file.exists(archivo)) # Escribo los titulos\n",
    "    {\n",
    "      linea <- paste0(\n",
    "        \"fecha\\t\",\n",
    "        paste(list.names(reg), collapse = \"\\t\"), \"\\n\"\n",
    "      )\n",
    "\n",
    "      cat(linea, file = archivo)\n",
    "    }\n",
    "\n",
    "  linea <- paste0(\n",
    "    format(Sys.time(), \"%Y%m%d %H%M%S\"), \"\\t\", # la fecha y hora\n",
    "    gsub(\", \", \"\\t\", toString(reg)), \"\\n\"\n",
    "  )\n",
    "\n",
    "  cat(linea, file = archivo, append = TRUE) # grabo al archivo\n",
    "\n",
    "  if (verbose) cat(linea) # imprimo por pantalla\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------\n",
    "GLOBAL_arbol <- 0L\n",
    "GLOBAL_gan_max <- -Inf\n",
    "vcant_optima <- c()\n",
    "\n",
    "fganancia_lgbm_meseta <- function(probs, datos) {\n",
    "  vlabels <- get_field(datos, \"label\")\n",
    "  vpesos <- get_field(datos, \"weight\")\n",
    "\n",
    "\n",
    "  GLOBAL_arbol <<- GLOBAL_arbol + 1\n",
    "  tbl <- as.data.table(list(\n",
    "    \"prob\" = probs,\n",
    "    \"gan\" = ifelse(vlabels == 1 & vpesos > 1,\n",
    "      PARAM$hyperparametertuning$POS_ganancia,\n",
    "      PARAM$hyperparametertuning$NEG_ganancia  )\n",
    "  ))\n",
    "\n",
    "  setorder(tbl, -prob)\n",
    "  tbl[, posicion := .I]\n",
    "  tbl[, gan_acum := cumsum(gan)]\n",
    "\n",
    "  tbl[, gan_suavizada :=\n",
    "    frollmean(\n",
    "      x = gan_acum, n = 2001, align = \"center\",\n",
    "      na.rm = TRUE, hasNA = TRUE\n",
    "    )]\n",
    "\n",
    "  gan <- tbl[, max(gan_suavizada, na.rm = TRUE)]\n",
    "\n",
    "\n",
    "  pos <- which.max(tbl[, gan_suavizada])\n",
    "  vcant_optima <<- c(vcant_optima, pos)\n",
    "\n",
    "  if (GLOBAL_arbol %% 10 == 0) {\n",
    "    if (gan > GLOBAL_gan_max) GLOBAL_gan_max <<- gan\n",
    "\n",
    "    cat(\"\\r\")\n",
    "    cat(\n",
    "      \"Validate \", GLOBAL_iteracion, \" \", \" \",\n",
    "      GLOBAL_arbol, \"  \", gan, \"   \", GLOBAL_gan_max, \"   \"\n",
    "    )\n",
    "  }\n",
    "\n",
    "\n",
    "  return(list(\n",
    "    \"name\" = \"ganancia\",\n",
    "    \"value\" = gan,\n",
    "    \"higher_better\" = TRUE\n",
    "  ))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------\n",
    "\n",
    "EstimarGanancia_lightgbm <- function(x) {\n",
    "  gc()\n",
    "  GLOBAL_iteracion <<- GLOBAL_iteracion + 1L\n",
    "\n",
    "  # hago la union de los parametros basicos y los moviles que vienen en x\n",
    "  param_completo <- c(PARAM$lgb_basicos, x)\n",
    "\n",
    "  param_completo$early_stopping_rounds <-\n",
    "    as.integer(400 + 4 / param_completo$learning_rate)\n",
    "\n",
    "  GLOBAL_arbol <<- 0L\n",
    "  GLOBAL_gan_max <<- -Inf\n",
    "  vcant_optima <<- c()\n",
    "  set.seed(PARAM$lgb_semilla, kind = \"L'Ecuyer-CMRG\")\n",
    "  modelo_train <- lgb.train(\n",
    "    data = dtrain,\n",
    "    valids = list(valid = dvalidate),\n",
    "    eval = fganancia_lgbm_meseta,\n",
    "    param = param_completo,\n",
    "    verbose = -100\n",
    "  )\n",
    "\n",
    "  cat(\"\\n\")\n",
    "\n",
    "  cant_corte <- vcant_optima[modelo_train$best_iter]\n",
    "\n",
    "  # aplico el modelo a testing y calculo la ganancia\n",
    "  prediccion <- predict(\n",
    "    modelo_train,\n",
    "    data.matrix(dataset_test[, campos_buenos, with = FALSE])\n",
    "  )\n",
    "\n",
    "  tbl <- copy(dataset_test[, list(\"gan\" = ifelse(clase_ternaria == \"BAJA+2\",\n",
    "    PARAM$hyperparametertuning$POS_ganancia, \n",
    "    PARAM$hyperparametertuning$NEG_ganancia))])\n",
    "\n",
    "  tbl[, prob := prediccion]\n",
    "  setorder(tbl, -prob)\n",
    "  tbl[, gan_acum := cumsum(gan)]\n",
    "  tbl[, gan_suavizada := frollmean(\n",
    "    x = gan_acum, n = 2001,\n",
    "    align = \"center\", na.rm = TRUE, hasNA = TRUE\n",
    "  )]\n",
    "\n",
    "\n",
    "  ganancia_test <- tbl[, max(gan_suavizada, na.rm = TRUE)]\n",
    "\n",
    "  cantidad_test_normalizada <- which.max(tbl[, gan_suavizada])\n",
    "\n",
    "  rm(tbl)\n",
    "  gc()\n",
    "\n",
    "  ganancia_test_normalizada <- ganancia_test\n",
    "\n",
    "\n",
    "  # voy grabando las mejores column importance\n",
    "  if (ganancia_test_normalizada > GLOBAL_gananciamax) {\n",
    "    GLOBAL_gananciamax <<- ganancia_test_normalizada\n",
    "    tb_importancia <- as.data.table(lgb.importance(modelo_train))\n",
    "\n",
    "    fwrite(tb_importancia,\n",
    "      file = paste0(\"impo_\", sprintf(\"%03d\", GLOBAL_iteracion), \".txt\"),\n",
    "      sep = \"\\t\"\n",
    "    )\n",
    "\n",
    "    rm(tb_importancia)\n",
    "  }\n",
    "\n",
    "\n",
    "  # logueo final\n",
    "  ds <- list(\"cols\" = ncol(dtrain), \"rows\" = nrow(dtrain))\n",
    "  xx <- c(ds, copy(param_completo))\n",
    "\n",
    "  xx$early_stopping_rounds <- NULL\n",
    "  xx$num_iterations <- modelo_train$best_iter\n",
    "  xx$estimulos <- cantidad_test_normalizada\n",
    "  xx$ganancia <- ganancia_test_normalizada\n",
    "  xx$iteracion_bayesiana <- GLOBAL_iteracion\n",
    "\n",
    "  loguear(xx, arch = \"BO_log.txt\")\n",
    "\n",
    "  set.seed(PARAM$lgb_semilla, kind = \"L'Ecuyer-CMRG\")\n",
    "  return(ganancia_test_normalizada)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aquí empieza el programa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Aqui empieza el programa\n",
    "\n",
    "# Aqui se debe poner la carpeta de la computadora local\n",
    "setwd(\"~/buckets/b1/\") # Establezco el Working Directory\n",
    "\n",
    "# cargo el dataset donde voy a entrenar el modelo\n",
    "dataset <- fread(PARAM$input$dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# creo la carpeta donde va el experimento\n",
    "dir.create(\"./exp/\", showWarnings = FALSE)\n",
    "dir.create(paste0(\"./exp/\", PARAM$experimento, \"/\"), showWarnings = FALSE)\n",
    "\n",
    "# Establezco el Working Directory DEL EXPERIMENTO\n",
    "setwd(paste0(\"./exp/\", PARAM$experimento, \"/\"))\n",
    "\n",
    "# en estos archivos quedan los resultados\n",
    "kbayesiana <- paste0(PARAM$experimento, \".RDATA\")\n",
    "klog <- paste0(PARAM$experimento, \".txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Catastrophe Analysis  -------------------------------------------------------\n",
    "# deben ir cosas de este estilo\n",
    "#   dataset[foto_mes == 202006, active_quarter := NA]\n",
    "\n",
    "# Data Drifting\n",
    "# por ahora, no hago nada\n",
    "\n",
    "\n",
    "# Feature Engineering Historico  ----------------------------------------------\n",
    "#   aqui deben calcularse los  lags y  lag_delta\n",
    "#   Sin lags no hay paraiso !  corta la bocha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# ahora SI comienza la optimizacion Bayesiana\n",
    "\n",
    "GLOBAL_iteracion <- 0 # inicializo la variable global\n",
    "GLOBAL_gananciamax <- -1 # inicializo la variable global\n",
    "\n",
    "# si ya existe el archivo log, traigo hasta donde llegue\n",
    "if (file.exists(klog)) {\n",
    "  tabla_log <- fread(klog)\n",
    "  GLOBAL_iteracion <- nrow(tabla_log)\n",
    "  GLOBAL_gananciamax <- tabla_log[, max(ganancia)]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# paso la clase a binaria que tome valores {0,1}  enteros\n",
    "dataset[, clase01 := ifelse(clase_ternaria == \"CONTINUA\", 0L, 1L)]\n",
    "\n",
    "\n",
    "# los campos que se van a utilizar\n",
    "campos_buenos <- setdiff(\n",
    "  colnames(dataset),\n",
    "  c(\"clase_ternaria\", \"clase01\", \"azar\", \"training\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# defino los datos que forma parte del training\n",
    "# aqui se hace el undersampling de los CONTINUA\n",
    "set.seed(PARAM$trainingstrategy$semilla_azar)\n",
    "dataset[, azar := runif(nrow(dataset))]\n",
    "dataset[, training := 0L]\n",
    "dataset[\n",
    "  foto_mes %in% PARAM$input$training &\n",
    "    (azar <= PARAM$trainingstrategy$undersampling | clase_ternaria %in% c(\"BAJA+1\", \"BAJA+2\")),\n",
    "  training := 1L\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# dejo los datos en el formato que necesita LightGBM\n",
    "dtrain <- lgb.Dataset(\n",
    "  data = data.matrix(dataset[training == 1L, campos_buenos, with = FALSE]),\n",
    "  label = dataset[training == 1L, clase01],\n",
    "  weight = dataset[training == 1L, \n",
    "    ifelse(clase_ternaria == \"BAJA+2\", 1.0000001, \n",
    "      ifelse(clase_ternaria == \"BAJA+1\", 1.0, 1.0))],\n",
    "  free_raw_data = FALSE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# defino los datos que forman parte de validation\n",
    "#  no hay undersampling\n",
    "dataset[, validation := 0L]\n",
    "dataset[ foto_mes %in% PARAM$input$validation,  validation := 1L]\n",
    "\n",
    "dvalidate <- lgb.Dataset(\n",
    "  data = data.matrix(dataset[validation == 1L, campos_buenos, with = FALSE]),\n",
    "  label = dataset[validation == 1L, clase01],\n",
    "  weight = dataset[validation == 1L, \n",
    "    ifelse(clase_ternaria == \"BAJA+2\", 1.0000001, \n",
    "      ifelse(clase_ternaria == \"BAJA+1\", 1.0, 1.0))],\n",
    "  free_raw_data = FALSE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# defino los datos de testing\n",
    "dataset[, testing := 0L]\n",
    "dataset[ foto_mes %in% PARAM$input$testing,  testing := 1L]\n",
    "\n",
    "\n",
    "dataset_test <- dataset[testing == 1, ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# libero espacio\n",
    "rm(dataset)\n",
    "gc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Aqui comienza la configuracion de la Bayesian Optimization\n",
    "funcion_optimizar <- EstimarGanancia_lightgbm # la funcion que voy a maximizar\n",
    "\n",
    "configureMlr(show.learner.output = FALSE)\n",
    "\n",
    "# configuro la busqueda bayesiana,  los hiperparametros que se van a optimizar\n",
    "# por favor, no desesperarse por lo complejo\n",
    "obj.fun <- makeSingleObjectiveFunction(\n",
    "  fn = funcion_optimizar, # la funcion que voy a maximizar\n",
    "  minimize = FALSE, # estoy Maximizando la ganancia\n",
    "  noisy = TRUE,\n",
    "  par.set = PARAM$bo_lgb, # definido al comienzo del programa\n",
    "  has.simple.signature = FALSE # paso los parametros en una lista\n",
    ")\n",
    "\n",
    "# cada 600 segundos guardo el resultado intermedio\n",
    "ctrl <- makeMBOControl(\n",
    "  save.on.disk.at.time = 600, # se graba cada 600 segundos\n",
    "  save.file.path = kbayesiana\n",
    ") # se graba cada 600 segundos\n",
    "\n",
    "# indico la cantidad de iteraciones que va a tener la Bayesian Optimization\n",
    "ctrl <- setMBOControlTermination(\n",
    "  ctrl,\n",
    "  iters = PARAM$bo_iteraciones\n",
    ") # cantidad de iteraciones\n",
    "\n",
    "# defino el método estandar para la creacion de los puntos iniciales,\n",
    "# los \"No Inteligentes\"\n",
    "ctrl <- setMBOControlInfill(ctrl, crit = makeMBOInfillCritEI())\n",
    "\n",
    "\n",
    "# establezco la funcion que busca el maximo\n",
    "surr.km <- makeLearner(\n",
    "  \"regr.km\",\n",
    "  predict.type = \"se\",\n",
    "  covtype = \"matern3_2\",\n",
    "  control = list(trace = TRUE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# inicio la optimizacion bayesiana\n",
    "if (!file.exists(kbayesiana)) {\n",
    "  run <- mbo(obj.fun, learner = surr.km, control = ctrl)\n",
    "} else {\n",
    "  run <- mboContinue(kbayesiana) # retomo en caso que ya exista\n",
    "}\n",
    "\n",
    "\n",
    "cat(\"\\n\\nLa optimizacion Bayesiana ha terminado\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semilla 2, ajustado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# limpio la memoria\n",
    "rm(list = ls()) # remove all objects\n",
    "gc() # garbage collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "require(\"data.table\")\n",
    "require(\"rlist\")\n",
    "require(\"lightgbm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# paquetes necesarios para la Bayesian Optimization\n",
    "require(\"DiceKriging\")\n",
    "require(\"mlrMBO\")\n",
    "\n",
    "# para que se detenga ante el primer error\n",
    "# y muestre el stack de funciones invocadas\n",
    "options(error = function() {\n",
    "  traceback(20)\n",
    "  options(error = NULL)\n",
    "  stop(\"exiting after script error\")\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definición de Parametros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# defino los parametros de la corrida, en una lista, la variable global  PARAM\n",
    "#  muy pronto esto se leera desde un archivo formato .yaml\n",
    "PARAM <- list()\n",
    "\n",
    "PARAM$experimento <- \"HT8230 (Clase 12, Ajustado Sem2)\"\n",
    "\n",
    "PARAM$input$dataset <- \"datasets/competencia_03_infla_ajustado_lags.csv.gz\"\n",
    "\n",
    "# los meses en los que vamos a entrenar\n",
    "#  mucha magia emerger de esta eleccion\n",
    "PARAM$input$testing <- c(202107) # Último mes, lo más cercano al 202109 de kaggle\n",
    "PARAM$input$validation <- c(202106)\n",
    "PARAM$input$training <- c(202105, 202104, 202103, 202102, 202101,202012) # 6 meses de entrenamiento\t\n",
    "\n",
    "# un undersampling de 0.1  toma solo el 10% de los CONTINUA\n",
    "PARAM$trainingstrategy$undersampling <- 1.0\n",
    "PARAM$trainingstrategy$semilla_azar <- c(279511, 279523, 279541, 279551, 279571)  # Aqui poner su  primer  semilla/ pongo todas mis semillas\n",
    "\n",
    "PARAM$hyperparametertuning$POS_ganancia <- 273000\n",
    "PARAM$hyperparametertuning$NEG_ganancia <- -7000\n",
    "\n",
    "# Aqui va semilla\n",
    "PARAM$lgb_semilla <- 279523"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Hiperparametros FIJOS de  lightgbm\n",
    "PARAM$lgb_basicos <- list(\n",
    "  boosting = \"gbdt\", # puede ir  dart  , ni pruebe random_forest\n",
    "  objective = \"binary\",\n",
    "  metric = \"custom\",\n",
    "  first_metric_only = TRUE,\n",
    "  boost_from_average = TRUE,\n",
    "  feature_pre_filter = FALSE,\n",
    "  force_row_wise = TRUE, # para reducir warnings\n",
    "  verbosity = -100,\n",
    "  max_depth = -1L, # -1 significa no limitar,  por ahora lo dejo fijo\n",
    "  min_gain_to_split = 0.0, # min_gain_to_split >= 0.0\n",
    "  min_sum_hessian_in_leaf = 0.001, #  min_sum_hessian_in_leaf >= 0.0\n",
    "  lambda_l1 = 0.0, # lambda_l1 >= 0.0\n",
    "  lambda_l2 = 0.0, # lambda_l2 >= 0.0\n",
    "  max_bin = 31L, # lo debo dejar fijo, no participa de la BO\n",
    "  num_iterations = 9999, # un numero muy grande, lo limita early_stopping_rounds\n",
    "\n",
    "  bagging_fraction = 1.0, # 0.0 < bagging_fraction <= 1.0\n",
    "  pos_bagging_fraction = 1.0, # 0.0 < pos_bagging_fraction <= 1.0\n",
    "  neg_bagging_fraction = 1.0, # 0.0 < neg_bagging_fraction <= 1.0\n",
    "  is_unbalance = FALSE, #\n",
    "  scale_pos_weight = 1.0, # scale_pos_weight > 0.0\n",
    "\n",
    "  drop_rate = 0.1, # 0.0 < neg_bagging_fraction <= 1.0\n",
    "  max_drop = 50, # <=0 means no limit\n",
    "  skip_drop = 0.5, # 0.0 <= skip_drop <= 1.0\n",
    "\n",
    "  extra_trees = TRUE, # Magic Sauce\n",
    "\n",
    "  seed = PARAM$lgb_semilla\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Aqui se cargan los hiperparametros que se optimizan\n",
    "#  en la Bayesian Optimization\n",
    "PARAM$bo_lgb <- makeParamSet(\n",
    "  makeNumericParam(\"learning_rate\", lower = 0.02, upper = 0.3),\n",
    "  makeNumericParam(\"feature_fraction\", lower = 0.01, upper = 1.0),\n",
    "  makeIntegerParam(\"num_leaves\", lower = 8L, upper = 1024L),\n",
    "  makeIntegerParam(\"min_data_in_leaf\", lower = 100L, upper = 50000L)\n",
    ")\n",
    "\n",
    "# si usted es ambicioso, y tiene paciencia, podria subir este valor a 100\n",
    "PARAM$bo_iteraciones <- 50 # iteraciones de la Optimizacion Bayesiana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# graba a un archivo los componentes de lista\n",
    "# para el primer registro, escribe antes los titulos\n",
    "\n",
    "loguear <- function(\n",
    "    reg, arch = NA, folder = \"./exp/\",\n",
    "    ext = \".txt\", verbose = TRUE) {\n",
    "  archivo <- arch\n",
    "  if (is.na(arch)) archivo <- paste0(folder, substitute(reg), ext)\n",
    "\n",
    "  if (!file.exists(archivo)) # Escribo los titulos\n",
    "    {\n",
    "      linea <- paste0(\n",
    "        \"fecha\\t\",\n",
    "        paste(list.names(reg), collapse = \"\\t\"), \"\\n\"\n",
    "      )\n",
    "\n",
    "      cat(linea, file = archivo)\n",
    "    }\n",
    "\n",
    "  linea <- paste0(\n",
    "    format(Sys.time(), \"%Y%m%d %H%M%S\"), \"\\t\", # la fecha y hora\n",
    "    gsub(\", \", \"\\t\", toString(reg)), \"\\n\"\n",
    "  )\n",
    "\n",
    "  cat(linea, file = archivo, append = TRUE) # grabo al archivo\n",
    "\n",
    "  if (verbose) cat(linea) # imprimo por pantalla\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------\n",
    "GLOBAL_arbol <- 0L\n",
    "GLOBAL_gan_max <- -Inf\n",
    "vcant_optima <- c()\n",
    "\n",
    "fganancia_lgbm_meseta <- function(probs, datos) {\n",
    "  vlabels <- get_field(datos, \"label\")\n",
    "  vpesos <- get_field(datos, \"weight\")\n",
    "\n",
    "\n",
    "  GLOBAL_arbol <<- GLOBAL_arbol + 1\n",
    "  tbl <- as.data.table(list(\n",
    "    \"prob\" = probs,\n",
    "    \"gan\" = ifelse(vlabels == 1 & vpesos > 1,\n",
    "      PARAM$hyperparametertuning$POS_ganancia,\n",
    "      PARAM$hyperparametertuning$NEG_ganancia  )\n",
    "  ))\n",
    "\n",
    "  setorder(tbl, -prob)\n",
    "  tbl[, posicion := .I]\n",
    "  tbl[, gan_acum := cumsum(gan)]\n",
    "\n",
    "  tbl[, gan_suavizada :=\n",
    "    frollmean(\n",
    "      x = gan_acum, n = 2001, align = \"center\",\n",
    "      na.rm = TRUE, hasNA = TRUE\n",
    "    )]\n",
    "\n",
    "  gan <- tbl[, max(gan_suavizada, na.rm = TRUE)]\n",
    "\n",
    "\n",
    "  pos <- which.max(tbl[, gan_suavizada])\n",
    "  vcant_optima <<- c(vcant_optima, pos)\n",
    "\n",
    "  if (GLOBAL_arbol %% 10 == 0) {\n",
    "    if (gan > GLOBAL_gan_max) GLOBAL_gan_max <<- gan\n",
    "\n",
    "    cat(\"\\r\")\n",
    "    cat(\n",
    "      \"Validate \", GLOBAL_iteracion, \" \", \" \",\n",
    "      GLOBAL_arbol, \"  \", gan, \"   \", GLOBAL_gan_max, \"   \"\n",
    "    )\n",
    "  }\n",
    "\n",
    "\n",
    "  return(list(\n",
    "    \"name\" = \"ganancia\",\n",
    "    \"value\" = gan,\n",
    "    \"higher_better\" = TRUE\n",
    "  ))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------\n",
    "\n",
    "EstimarGanancia_lightgbm <- function(x) {\n",
    "  gc()\n",
    "  GLOBAL_iteracion <<- GLOBAL_iteracion + 1L\n",
    "\n",
    "  # hago la union de los parametros basicos y los moviles que vienen en x\n",
    "  param_completo <- c(PARAM$lgb_basicos, x)\n",
    "\n",
    "  param_completo$early_stopping_rounds <-\n",
    "    as.integer(400 + 4 / param_completo$learning_rate)\n",
    "\n",
    "  GLOBAL_arbol <<- 0L\n",
    "  GLOBAL_gan_max <<- -Inf\n",
    "  vcant_optima <<- c()\n",
    "  set.seed(PARAM$lgb_semilla, kind = \"L'Ecuyer-CMRG\")\n",
    "  modelo_train <- lgb.train(\n",
    "    data = dtrain,\n",
    "    valids = list(valid = dvalidate),\n",
    "    eval = fganancia_lgbm_meseta,\n",
    "    param = param_completo,\n",
    "    verbose = -100\n",
    "  )\n",
    "\n",
    "  cat(\"\\n\")\n",
    "\n",
    "  cant_corte <- vcant_optima[modelo_train$best_iter]\n",
    "\n",
    "  # aplico el modelo a testing y calculo la ganancia\n",
    "  prediccion <- predict(\n",
    "    modelo_train,\n",
    "    data.matrix(dataset_test[, campos_buenos, with = FALSE])\n",
    "  )\n",
    "\n",
    "  tbl <- copy(dataset_test[, list(\"gan\" = ifelse(clase_ternaria == \"BAJA+2\",\n",
    "    PARAM$hyperparametertuning$POS_ganancia, \n",
    "    PARAM$hyperparametertuning$NEG_ganancia))])\n",
    "\n",
    "  tbl[, prob := prediccion]\n",
    "  setorder(tbl, -prob)\n",
    "  tbl[, gan_acum := cumsum(gan)]\n",
    "  tbl[, gan_suavizada := frollmean(\n",
    "    x = gan_acum, n = 2001,\n",
    "    align = \"center\", na.rm = TRUE, hasNA = TRUE\n",
    "  )]\n",
    "\n",
    "\n",
    "  ganancia_test <- tbl[, max(gan_suavizada, na.rm = TRUE)]\n",
    "\n",
    "  cantidad_test_normalizada <- which.max(tbl[, gan_suavizada])\n",
    "\n",
    "  rm(tbl)\n",
    "  gc()\n",
    "\n",
    "  ganancia_test_normalizada <- ganancia_test\n",
    "\n",
    "\n",
    "  # voy grabando las mejores column importance\n",
    "  if (ganancia_test_normalizada > GLOBAL_gananciamax) {\n",
    "    GLOBAL_gananciamax <<- ganancia_test_normalizada\n",
    "    tb_importancia <- as.data.table(lgb.importance(modelo_train))\n",
    "\n",
    "    fwrite(tb_importancia,\n",
    "      file = paste0(\"impo_\", sprintf(\"%03d\", GLOBAL_iteracion), \".txt\"),\n",
    "      sep = \"\\t\"\n",
    "    )\n",
    "\n",
    "    rm(tb_importancia)\n",
    "  }\n",
    "\n",
    "\n",
    "  # logueo final\n",
    "  ds <- list(\"cols\" = ncol(dtrain), \"rows\" = nrow(dtrain))\n",
    "  xx <- c(ds, copy(param_completo))\n",
    "\n",
    "  xx$early_stopping_rounds <- NULL\n",
    "  xx$num_iterations <- modelo_train$best_iter\n",
    "  xx$estimulos <- cantidad_test_normalizada\n",
    "  xx$ganancia <- ganancia_test_normalizada\n",
    "  xx$iteracion_bayesiana <- GLOBAL_iteracion\n",
    "\n",
    "  loguear(xx, arch = \"BO_log.txt\")\n",
    "\n",
    "  set.seed(PARAM$lgb_semilla, kind = \"L'Ecuyer-CMRG\")\n",
    "  return(ganancia_test_normalizada)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aquí empieza el programa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Aqui empieza el programa\n",
    "\n",
    "# Aqui se debe poner la carpeta de la computadora local\n",
    "setwd(\"~/buckets/b1/\") # Establezco el Working Directory\n",
    "\n",
    "# cargo el dataset donde voy a entrenar el modelo\n",
    "dataset <- fread(PARAM$input$dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# creo la carpeta donde va el experimento\n",
    "dir.create(\"./exp/\", showWarnings = FALSE)\n",
    "dir.create(paste0(\"./exp/\", PARAM$experimento, \"/\"), showWarnings = FALSE)\n",
    "\n",
    "# Establezco el Working Directory DEL EXPERIMENTO\n",
    "setwd(paste0(\"./exp/\", PARAM$experimento, \"/\"))\n",
    "\n",
    "# en estos archivos quedan los resultados\n",
    "kbayesiana <- paste0(PARAM$experimento, \".RDATA\")\n",
    "klog <- paste0(PARAM$experimento, \".txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Catastrophe Analysis  -------------------------------------------------------\n",
    "# deben ir cosas de este estilo\n",
    "#   dataset[foto_mes == 202006, active_quarter := NA]\n",
    "\n",
    "# Data Drifting\n",
    "# por ahora, no hago nada\n",
    "\n",
    "\n",
    "# Feature Engineering Historico  ----------------------------------------------\n",
    "#   aqui deben calcularse los  lags y  lag_delta\n",
    "#   Sin lags no hay paraiso !  corta la bocha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# ahora SI comienza la optimizacion Bayesiana\n",
    "\n",
    "GLOBAL_iteracion <- 0 # inicializo la variable global\n",
    "GLOBAL_gananciamax <- -1 # inicializo la variable global\n",
    "\n",
    "# si ya existe el archivo log, traigo hasta donde llegue\n",
    "if (file.exists(klog)) {\n",
    "  tabla_log <- fread(klog)\n",
    "  GLOBAL_iteracion <- nrow(tabla_log)\n",
    "  GLOBAL_gananciamax <- tabla_log[, max(ganancia)]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# paso la clase a binaria que tome valores {0,1}  enteros\n",
    "dataset[, clase01 := ifelse(clase_ternaria == \"CONTINUA\", 0L, 1L)]\n",
    "\n",
    "\n",
    "# los campos que se van a utilizar\n",
    "campos_buenos <- setdiff(\n",
    "  colnames(dataset),\n",
    "  c(\"clase_ternaria\", \"clase01\", \"azar\", \"training\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# defino los datos que forma parte del training\n",
    "# aqui se hace el undersampling de los CONTINUA\n",
    "set.seed(PARAM$trainingstrategy$semilla_azar)\n",
    "dataset[, azar := runif(nrow(dataset))]\n",
    "dataset[, training := 0L]\n",
    "dataset[\n",
    "  foto_mes %in% PARAM$input$training &\n",
    "    (azar <= PARAM$trainingstrategy$undersampling | clase_ternaria %in% c(\"BAJA+1\", \"BAJA+2\")),\n",
    "  training := 1L\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# dejo los datos en el formato que necesita LightGBM\n",
    "dtrain <- lgb.Dataset(\n",
    "  data = data.matrix(dataset[training == 1L, campos_buenos, with = FALSE]),\n",
    "  label = dataset[training == 1L, clase01],\n",
    "  weight = dataset[training == 1L, \n",
    "    ifelse(clase_ternaria == \"BAJA+2\", 1.0000001, \n",
    "      ifelse(clase_ternaria == \"BAJA+1\", 1.0, 1.0))],\n",
    "  free_raw_data = FALSE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# defino los datos que forman parte de validation\n",
    "#  no hay undersampling\n",
    "dataset[, validation := 0L]\n",
    "dataset[ foto_mes %in% PARAM$input$validation,  validation := 1L]\n",
    "\n",
    "dvalidate <- lgb.Dataset(\n",
    "  data = data.matrix(dataset[validation == 1L, campos_buenos, with = FALSE]),\n",
    "  label = dataset[validation == 1L, clase01],\n",
    "  weight = dataset[validation == 1L, \n",
    "    ifelse(clase_ternaria == \"BAJA+2\", 1.0000001, \n",
    "      ifelse(clase_ternaria == \"BAJA+1\", 1.0, 1.0))],\n",
    "  free_raw_data = FALSE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# defino los datos de testing\n",
    "dataset[, testing := 0L]\n",
    "dataset[ foto_mes %in% PARAM$input$testing,  testing := 1L]\n",
    "\n",
    "\n",
    "dataset_test <- dataset[testing == 1, ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# libero espacio\n",
    "rm(dataset)\n",
    "gc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Aqui comienza la configuracion de la Bayesian Optimization\n",
    "funcion_optimizar <- EstimarGanancia_lightgbm # la funcion que voy a maximizar\n",
    "\n",
    "configureMlr(show.learner.output = FALSE)\n",
    "\n",
    "# configuro la busqueda bayesiana,  los hiperparametros que se van a optimizar\n",
    "# por favor, no desesperarse por lo complejo\n",
    "obj.fun <- makeSingleObjectiveFunction(\n",
    "  fn = funcion_optimizar, # la funcion que voy a maximizar\n",
    "  minimize = FALSE, # estoy Maximizando la ganancia\n",
    "  noisy = TRUE,\n",
    "  par.set = PARAM$bo_lgb, # definido al comienzo del programa\n",
    "  has.simple.signature = FALSE # paso los parametros en una lista\n",
    ")\n",
    "\n",
    "# cada 600 segundos guardo el resultado intermedio\n",
    "ctrl <- makeMBOControl(\n",
    "  save.on.disk.at.time = 600, # se graba cada 600 segundos\n",
    "  save.file.path = kbayesiana\n",
    ") # se graba cada 600 segundos\n",
    "\n",
    "# indico la cantidad de iteraciones que va a tener la Bayesian Optimization\n",
    "ctrl <- setMBOControlTermination(\n",
    "  ctrl,\n",
    "  iters = PARAM$bo_iteraciones\n",
    ") # cantidad de iteraciones\n",
    "\n",
    "# defino el método estandar para la creacion de los puntos iniciales,\n",
    "# los \"No Inteligentes\"\n",
    "ctrl <- setMBOControlInfill(ctrl, crit = makeMBOInfillCritEI())\n",
    "\n",
    "\n",
    "# establezco la funcion que busca el maximo\n",
    "surr.km <- makeLearner(\n",
    "  \"regr.km\",\n",
    "  predict.type = \"se\",\n",
    "  covtype = \"matern3_2\",\n",
    "  control = list(trace = TRUE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# inicio la optimizacion bayesiana\n",
    "if (!file.exists(kbayesiana)) {\n",
    "  run <- mbo(obj.fun, learner = surr.km, control = ctrl)\n",
    "} else {\n",
    "  run <- mboContinue(kbayesiana) # retomo en caso que ya exista\n",
    "}\n",
    "\n",
    "\n",
    "cat(\"\\n\\nLa optimizacion Bayesiana ha terminado\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semilla 3, ajustado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# limpio la memoria\n",
    "rm(list = ls()) # remove all objects\n",
    "gc() # garbage collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "require(\"data.table\")\n",
    "require(\"rlist\")\n",
    "require(\"lightgbm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# paquetes necesarios para la Bayesian Optimization\n",
    "require(\"DiceKriging\")\n",
    "require(\"mlrMBO\")\n",
    "\n",
    "# para que se detenga ante el primer error\n",
    "# y muestre el stack de funciones invocadas\n",
    "options(error = function() {\n",
    "  traceback(20)\n",
    "  options(error = NULL)\n",
    "  stop(\"exiting after script error\")\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definición de Parametros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# defino los parametros de la corrida, en una lista, la variable global  PARAM\n",
    "#  muy pronto esto se leera desde un archivo formato .yaml\n",
    "PARAM <- list()\n",
    "\n",
    "PARAM$experimento <- \"HT8230 (Clase 12, Ajustado Sem3)\"\n",
    "\n",
    "PARAM$input$dataset <- \"datasets/competencia_03_infla_ajustado_lags.csv.gz\"\n",
    "\n",
    "# los meses en los que vamos a entrenar\n",
    "#  mucha magia emerger de esta eleccion\n",
    "PARAM$input$testing <- c(202107) # Último mes, lo más cercano al 202109 de kaggle\n",
    "PARAM$input$validation <- c(202106)\n",
    "PARAM$input$training <- c(202105, 202104, 202103, 202102, 202101,202012) # 6 meses de entrenamiento\t\n",
    "\n",
    "# un undersampling de 0.1  toma solo el 10% de los CONTINUA\n",
    "PARAM$trainingstrategy$undersampling <- 1.0\n",
    "PARAM$trainingstrategy$semilla_azar <- c(279511, 279523, 279541, 279551, 279571)  # Aqui poner su  primer  semilla/ pongo todas mis semillas\n",
    "\n",
    "PARAM$hyperparametertuning$POS_ganancia <- 273000\n",
    "PARAM$hyperparametertuning$NEG_ganancia <- -7000\n",
    "\n",
    "# Aqui va semilla\n",
    "PARAM$lgb_semilla <- 279541"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Hiperparametros FIJOS de  lightgbm\n",
    "PARAM$lgb_basicos <- list(\n",
    "  boosting = \"gbdt\", # puede ir  dart  , ni pruebe random_forest\n",
    "  objective = \"binary\",\n",
    "  metric = \"custom\",\n",
    "  first_metric_only = TRUE,\n",
    "  boost_from_average = TRUE,\n",
    "  feature_pre_filter = FALSE,\n",
    "  force_row_wise = TRUE, # para reducir warnings\n",
    "  verbosity = -100,\n",
    "  max_depth = -1L, # -1 significa no limitar,  por ahora lo dejo fijo\n",
    "  min_gain_to_split = 0.0, # min_gain_to_split >= 0.0\n",
    "  min_sum_hessian_in_leaf = 0.001, #  min_sum_hessian_in_leaf >= 0.0\n",
    "  lambda_l1 = 0.0, # lambda_l1 >= 0.0\n",
    "  lambda_l2 = 0.0, # lambda_l2 >= 0.0\n",
    "  max_bin = 31L, # lo debo dejar fijo, no participa de la BO\n",
    "  num_iterations = 9999, # un numero muy grande, lo limita early_stopping_rounds\n",
    "\n",
    "  bagging_fraction = 1.0, # 0.0 < bagging_fraction <= 1.0\n",
    "  pos_bagging_fraction = 1.0, # 0.0 < pos_bagging_fraction <= 1.0\n",
    "  neg_bagging_fraction = 1.0, # 0.0 < neg_bagging_fraction <= 1.0\n",
    "  is_unbalance = FALSE, #\n",
    "  scale_pos_weight = 1.0, # scale_pos_weight > 0.0\n",
    "\n",
    "  drop_rate = 0.1, # 0.0 < neg_bagging_fraction <= 1.0\n",
    "  max_drop = 50, # <=0 means no limit\n",
    "  skip_drop = 0.5, # 0.0 <= skip_drop <= 1.0\n",
    "\n",
    "  extra_trees = TRUE, # Magic Sauce\n",
    "\n",
    "  seed = PARAM$lgb_semilla\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Aqui se cargan los hiperparametros que se optimizan\n",
    "#  en la Bayesian Optimization\n",
    "PARAM$bo_lgb <- makeParamSet(\n",
    "  makeNumericParam(\"learning_rate\", lower = 0.02, upper = 0.3),\n",
    "  makeNumericParam(\"feature_fraction\", lower = 0.01, upper = 1.0),\n",
    "  makeIntegerParam(\"num_leaves\", lower = 8L, upper = 1024L),\n",
    "  makeIntegerParam(\"min_data_in_leaf\", lower = 100L, upper = 50000L)\n",
    ")\n",
    "\n",
    "# si usted es ambicioso, y tiene paciencia, podria subir este valor a 100\n",
    "PARAM$bo_iteraciones <- 50 # iteraciones de la Optimizacion Bayesiana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# graba a un archivo los componentes de lista\n",
    "# para el primer registro, escribe antes los titulos\n",
    "\n",
    "loguear <- function(\n",
    "    reg, arch = NA, folder = \"./exp/\",\n",
    "    ext = \".txt\", verbose = TRUE) {\n",
    "  archivo <- arch\n",
    "  if (is.na(arch)) archivo <- paste0(folder, substitute(reg), ext)\n",
    "\n",
    "  if (!file.exists(archivo)) # Escribo los titulos\n",
    "    {\n",
    "      linea <- paste0(\n",
    "        \"fecha\\t\",\n",
    "        paste(list.names(reg), collapse = \"\\t\"), \"\\n\"\n",
    "      )\n",
    "\n",
    "      cat(linea, file = archivo)\n",
    "    }\n",
    "\n",
    "  linea <- paste0(\n",
    "    format(Sys.time(), \"%Y%m%d %H%M%S\"), \"\\t\", # la fecha y hora\n",
    "    gsub(\", \", \"\\t\", toString(reg)), \"\\n\"\n",
    "  )\n",
    "\n",
    "  cat(linea, file = archivo, append = TRUE) # grabo al archivo\n",
    "\n",
    "  if (verbose) cat(linea) # imprimo por pantalla\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------\n",
    "GLOBAL_arbol <- 0L\n",
    "GLOBAL_gan_max <- -Inf\n",
    "vcant_optima <- c()\n",
    "\n",
    "fganancia_lgbm_meseta <- function(probs, datos) {\n",
    "  vlabels <- get_field(datos, \"label\")\n",
    "  vpesos <- get_field(datos, \"weight\")\n",
    "\n",
    "\n",
    "  GLOBAL_arbol <<- GLOBAL_arbol + 1\n",
    "  tbl <- as.data.table(list(\n",
    "    \"prob\" = probs,\n",
    "    \"gan\" = ifelse(vlabels == 1 & vpesos > 1,\n",
    "      PARAM$hyperparametertuning$POS_ganancia,\n",
    "      PARAM$hyperparametertuning$NEG_ganancia  )\n",
    "  ))\n",
    "\n",
    "  setorder(tbl, -prob)\n",
    "  tbl[, posicion := .I]\n",
    "  tbl[, gan_acum := cumsum(gan)]\n",
    "\n",
    "  tbl[, gan_suavizada :=\n",
    "    frollmean(\n",
    "      x = gan_acum, n = 2001, align = \"center\",\n",
    "      na.rm = TRUE, hasNA = TRUE\n",
    "    )]\n",
    "\n",
    "  gan <- tbl[, max(gan_suavizada, na.rm = TRUE)]\n",
    "\n",
    "\n",
    "  pos <- which.max(tbl[, gan_suavizada])\n",
    "  vcant_optima <<- c(vcant_optima, pos)\n",
    "\n",
    "  if (GLOBAL_arbol %% 10 == 0) {\n",
    "    if (gan > GLOBAL_gan_max) GLOBAL_gan_max <<- gan\n",
    "\n",
    "    cat(\"\\r\")\n",
    "    cat(\n",
    "      \"Validate \", GLOBAL_iteracion, \" \", \" \",\n",
    "      GLOBAL_arbol, \"  \", gan, \"   \", GLOBAL_gan_max, \"   \"\n",
    "    )\n",
    "  }\n",
    "\n",
    "\n",
    "  return(list(\n",
    "    \"name\" = \"ganancia\",\n",
    "    \"value\" = gan,\n",
    "    \"higher_better\" = TRUE\n",
    "  ))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------\n",
    "\n",
    "EstimarGanancia_lightgbm <- function(x) {\n",
    "  gc()\n",
    "  GLOBAL_iteracion <<- GLOBAL_iteracion + 1L\n",
    "\n",
    "  # hago la union de los parametros basicos y los moviles que vienen en x\n",
    "  param_completo <- c(PARAM$lgb_basicos, x)\n",
    "\n",
    "  param_completo$early_stopping_rounds <-\n",
    "    as.integer(400 + 4 / param_completo$learning_rate)\n",
    "\n",
    "  GLOBAL_arbol <<- 0L\n",
    "  GLOBAL_gan_max <<- -Inf\n",
    "  vcant_optima <<- c()\n",
    "  set.seed(PARAM$lgb_semilla, kind = \"L'Ecuyer-CMRG\")\n",
    "  modelo_train <- lgb.train(\n",
    "    data = dtrain,\n",
    "    valids = list(valid = dvalidate),\n",
    "    eval = fganancia_lgbm_meseta,\n",
    "    param = param_completo,\n",
    "    verbose = -100\n",
    "  )\n",
    "\n",
    "  cat(\"\\n\")\n",
    "\n",
    "  cant_corte <- vcant_optima[modelo_train$best_iter]\n",
    "\n",
    "  # aplico el modelo a testing y calculo la ganancia\n",
    "  prediccion <- predict(\n",
    "    modelo_train,\n",
    "    data.matrix(dataset_test[, campos_buenos, with = FALSE])\n",
    "  )\n",
    "\n",
    "  tbl <- copy(dataset_test[, list(\"gan\" = ifelse(clase_ternaria == \"BAJA+2\",\n",
    "    PARAM$hyperparametertuning$POS_ganancia, \n",
    "    PARAM$hyperparametertuning$NEG_ganancia))])\n",
    "\n",
    "  tbl[, prob := prediccion]\n",
    "  setorder(tbl, -prob)\n",
    "  tbl[, gan_acum := cumsum(gan)]\n",
    "  tbl[, gan_suavizada := frollmean(\n",
    "    x = gan_acum, n = 2001,\n",
    "    align = \"center\", na.rm = TRUE, hasNA = TRUE\n",
    "  )]\n",
    "\n",
    "\n",
    "  ganancia_test <- tbl[, max(gan_suavizada, na.rm = TRUE)]\n",
    "\n",
    "  cantidad_test_normalizada <- which.max(tbl[, gan_suavizada])\n",
    "\n",
    "  rm(tbl)\n",
    "  gc()\n",
    "\n",
    "  ganancia_test_normalizada <- ganancia_test\n",
    "\n",
    "\n",
    "  # voy grabando las mejores column importance\n",
    "  if (ganancia_test_normalizada > GLOBAL_gananciamax) {\n",
    "    GLOBAL_gananciamax <<- ganancia_test_normalizada\n",
    "    tb_importancia <- as.data.table(lgb.importance(modelo_train))\n",
    "\n",
    "    fwrite(tb_importancia,\n",
    "      file = paste0(\"impo_\", sprintf(\"%03d\", GLOBAL_iteracion), \".txt\"),\n",
    "      sep = \"\\t\"\n",
    "    )\n",
    "\n",
    "    rm(tb_importancia)\n",
    "  }\n",
    "\n",
    "\n",
    "  # logueo final\n",
    "  ds <- list(\"cols\" = ncol(dtrain), \"rows\" = nrow(dtrain))\n",
    "  xx <- c(ds, copy(param_completo))\n",
    "\n",
    "  xx$early_stopping_rounds <- NULL\n",
    "  xx$num_iterations <- modelo_train$best_iter\n",
    "  xx$estimulos <- cantidad_test_normalizada\n",
    "  xx$ganancia <- ganancia_test_normalizada\n",
    "  xx$iteracion_bayesiana <- GLOBAL_iteracion\n",
    "\n",
    "  loguear(xx, arch = \"BO_log.txt\")\n",
    "\n",
    "  set.seed(PARAM$lgb_semilla, kind = \"L'Ecuyer-CMRG\")\n",
    "  return(ganancia_test_normalizada)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aquí empieza el programa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Aqui empieza el programa\n",
    "\n",
    "# Aqui se debe poner la carpeta de la computadora local\n",
    "setwd(\"~/buckets/b1/\") # Establezco el Working Directory\n",
    "\n",
    "# cargo el dataset donde voy a entrenar el modelo\n",
    "dataset <- fread(PARAM$input$dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# creo la carpeta donde va el experimento\n",
    "dir.create(\"./exp/\", showWarnings = FALSE)\n",
    "dir.create(paste0(\"./exp/\", PARAM$experimento, \"/\"), showWarnings = FALSE)\n",
    "\n",
    "# Establezco el Working Directory DEL EXPERIMENTO\n",
    "setwd(paste0(\"./exp/\", PARAM$experimento, \"/\"))\n",
    "\n",
    "# en estos archivos quedan los resultados\n",
    "kbayesiana <- paste0(PARAM$experimento, \".RDATA\")\n",
    "klog <- paste0(PARAM$experimento, \".txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Catastrophe Analysis  -------------------------------------------------------\n",
    "# deben ir cosas de este estilo\n",
    "#   dataset[foto_mes == 202006, active_quarter := NA]\n",
    "\n",
    "# Data Drifting\n",
    "# por ahora, no hago nada\n",
    "\n",
    "\n",
    "# Feature Engineering Historico  ----------------------------------------------\n",
    "#   aqui deben calcularse los  lags y  lag_delta\n",
    "#   Sin lags no hay paraiso !  corta la bocha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# ahora SI comienza la optimizacion Bayesiana\n",
    "\n",
    "GLOBAL_iteracion <- 0 # inicializo la variable global\n",
    "GLOBAL_gananciamax <- -1 # inicializo la variable global\n",
    "\n",
    "# si ya existe el archivo log, traigo hasta donde llegue\n",
    "if (file.exists(klog)) {\n",
    "  tabla_log <- fread(klog)\n",
    "  GLOBAL_iteracion <- nrow(tabla_log)\n",
    "  GLOBAL_gananciamax <- tabla_log[, max(ganancia)]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# paso la clase a binaria que tome valores {0,1}  enteros\n",
    "dataset[, clase01 := ifelse(clase_ternaria == \"CONTINUA\", 0L, 1L)]\n",
    "\n",
    "\n",
    "# los campos que se van a utilizar\n",
    "campos_buenos <- setdiff(\n",
    "  colnames(dataset),\n",
    "  c(\"clase_ternaria\", \"clase01\", \"azar\", \"training\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# defino los datos que forma parte del training\n",
    "# aqui se hace el undersampling de los CONTINUA\n",
    "set.seed(PARAM$trainingstrategy$semilla_azar)\n",
    "dataset[, azar := runif(nrow(dataset))]\n",
    "dataset[, training := 0L]\n",
    "dataset[\n",
    "  foto_mes %in% PARAM$input$training &\n",
    "    (azar <= PARAM$trainingstrategy$undersampling | clase_ternaria %in% c(\"BAJA+1\", \"BAJA+2\")),\n",
    "  training := 1L\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# dejo los datos en el formato que necesita LightGBM\n",
    "dtrain <- lgb.Dataset(\n",
    "  data = data.matrix(dataset[training == 1L, campos_buenos, with = FALSE]),\n",
    "  label = dataset[training == 1L, clase01],\n",
    "  weight = dataset[training == 1L, \n",
    "    ifelse(clase_ternaria == \"BAJA+2\", 1.0000001, \n",
    "      ifelse(clase_ternaria == \"BAJA+1\", 1.0, 1.0))],\n",
    "  free_raw_data = FALSE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# defino los datos que forman parte de validation\n",
    "#  no hay undersampling\n",
    "dataset[, validation := 0L]\n",
    "dataset[ foto_mes %in% PARAM$input$validation,  validation := 1L]\n",
    "\n",
    "dvalidate <- lgb.Dataset(\n",
    "  data = data.matrix(dataset[validation == 1L, campos_buenos, with = FALSE]),\n",
    "  label = dataset[validation == 1L, clase01],\n",
    "  weight = dataset[validation == 1L, \n",
    "    ifelse(clase_ternaria == \"BAJA+2\", 1.0000001, \n",
    "      ifelse(clase_ternaria == \"BAJA+1\", 1.0, 1.0))],\n",
    "  free_raw_data = FALSE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# defino los datos de testing\n",
    "dataset[, testing := 0L]\n",
    "dataset[ foto_mes %in% PARAM$input$testing,  testing := 1L]\n",
    "\n",
    "\n",
    "dataset_test <- dataset[testing == 1, ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# libero espacio\n",
    "rm(dataset)\n",
    "gc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Aqui comienza la configuracion de la Bayesian Optimization\n",
    "funcion_optimizar <- EstimarGanancia_lightgbm # la funcion que voy a maximizar\n",
    "\n",
    "configureMlr(show.learner.output = FALSE)\n",
    "\n",
    "# configuro la busqueda bayesiana,  los hiperparametros que se van a optimizar\n",
    "# por favor, no desesperarse por lo complejo\n",
    "obj.fun <- makeSingleObjectiveFunction(\n",
    "  fn = funcion_optimizar, # la funcion que voy a maximizar\n",
    "  minimize = FALSE, # estoy Maximizando la ganancia\n",
    "  noisy = TRUE,\n",
    "  par.set = PARAM$bo_lgb, # definido al comienzo del programa\n",
    "  has.simple.signature = FALSE # paso los parametros en una lista\n",
    ")\n",
    "\n",
    "# cada 600 segundos guardo el resultado intermedio\n",
    "ctrl <- makeMBOControl(\n",
    "  save.on.disk.at.time = 600, # se graba cada 600 segundos\n",
    "  save.file.path = kbayesiana\n",
    ") # se graba cada 600 segundos\n",
    "\n",
    "# indico la cantidad de iteraciones que va a tener la Bayesian Optimization\n",
    "ctrl <- setMBOControlTermination(\n",
    "  ctrl,\n",
    "  iters = PARAM$bo_iteraciones\n",
    ") # cantidad de iteraciones\n",
    "\n",
    "# defino el método estandar para la creacion de los puntos iniciales,\n",
    "# los \"No Inteligentes\"\n",
    "ctrl <- setMBOControlInfill(ctrl, crit = makeMBOInfillCritEI())\n",
    "\n",
    "\n",
    "# establezco la funcion que busca el maximo\n",
    "surr.km <- makeLearner(\n",
    "  \"regr.km\",\n",
    "  predict.type = \"se\",\n",
    "  covtype = \"matern3_2\",\n",
    "  control = list(trace = TRUE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# inicio la optimizacion bayesiana\n",
    "if (!file.exists(kbayesiana)) {\n",
    "  run <- mbo(obj.fun, learner = surr.km, control = ctrl)\n",
    "} else {\n",
    "  run <- mboContinue(kbayesiana) # retomo en caso que ya exista\n",
    "}\n",
    "\n",
    "\n",
    "cat(\"\\n\\nLa optimizacion Bayesiana ha terminado\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semilla 4, ajustado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# limpio la memoria\n",
    "rm(list = ls()) # remove all objects\n",
    "gc() # garbage collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "require(\"data.table\")\n",
    "require(\"rlist\")\n",
    "require(\"lightgbm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# paquetes necesarios para la Bayesian Optimization\n",
    "require(\"DiceKriging\")\n",
    "require(\"mlrMBO\")\n",
    "\n",
    "# para que se detenga ante el primer error\n",
    "# y muestre el stack de funciones invocadas\n",
    "options(error = function() {\n",
    "  traceback(20)\n",
    "  options(error = NULL)\n",
    "  stop(\"exiting after script error\")\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definición de Parametros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# defino los parametros de la corrida, en una lista, la variable global  PARAM\n",
    "#  muy pronto esto se leera desde un archivo formato .yaml\n",
    "PARAM <- list()\n",
    "\n",
    "PARAM$experimento <- \"HT8230 (Clase 12, Ajustado Sem4)\"\n",
    "\n",
    "PARAM$input$dataset <- \"datasets/competencia_03_infla_ajustado_lags.csv.gz\"\n",
    "\n",
    "# los meses en los que vamos a entrenar\n",
    "#  mucha magia emerger de esta eleccion\n",
    "PARAM$input$testing <- c(202107) # Último mes, lo más cercano al 202109 de kaggle\n",
    "PARAM$input$validation <- c(202106)\n",
    "PARAM$input$training <- c(202105, 202104, 202103, 202102, 202101,202012) # 6 meses de entrenamiento\t\n",
    "\n",
    "# un undersampling de 0.1  toma solo el 10% de los CONTINUA\n",
    "PARAM$trainingstrategy$undersampling <- 1.0\n",
    "PARAM$trainingstrategy$semilla_azar <- c(279511, 279523, 279541, 279551, 279571)  # Aqui poner su  primer  semilla/ pongo todas mis semillas\n",
    "\n",
    "PARAM$hyperparametertuning$POS_ganancia <- 273000\n",
    "PARAM$hyperparametertuning$NEG_ganancia <- -7000\n",
    "\n",
    "# Aqui va semilla\n",
    "PARAM$lgb_semilla <- 279551"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Hiperparametros FIJOS de  lightgbm\n",
    "PARAM$lgb_basicos <- list(\n",
    "  boosting = \"gbdt\", # puede ir  dart  , ni pruebe random_forest\n",
    "  objective = \"binary\",\n",
    "  metric = \"custom\",\n",
    "  first_metric_only = TRUE,\n",
    "  boost_from_average = TRUE,\n",
    "  feature_pre_filter = FALSE,\n",
    "  force_row_wise = TRUE, # para reducir warnings\n",
    "  verbosity = -100,\n",
    "  max_depth = -1L, # -1 significa no limitar,  por ahora lo dejo fijo\n",
    "  min_gain_to_split = 0.0, # min_gain_to_split >= 0.0\n",
    "  min_sum_hessian_in_leaf = 0.001, #  min_sum_hessian_in_leaf >= 0.0\n",
    "  lambda_l1 = 0.0, # lambda_l1 >= 0.0\n",
    "  lambda_l2 = 0.0, # lambda_l2 >= 0.0\n",
    "  max_bin = 31L, # lo debo dejar fijo, no participa de la BO\n",
    "  num_iterations = 9999, # un numero muy grande, lo limita early_stopping_rounds\n",
    "\n",
    "  bagging_fraction = 1.0, # 0.0 < bagging_fraction <= 1.0\n",
    "  pos_bagging_fraction = 1.0, # 0.0 < pos_bagging_fraction <= 1.0\n",
    "  neg_bagging_fraction = 1.0, # 0.0 < neg_bagging_fraction <= 1.0\n",
    "  is_unbalance = FALSE, #\n",
    "  scale_pos_weight = 1.0, # scale_pos_weight > 0.0\n",
    "\n",
    "  drop_rate = 0.1, # 0.0 < neg_bagging_fraction <= 1.0\n",
    "  max_drop = 50, # <=0 means no limit\n",
    "  skip_drop = 0.5, # 0.0 <= skip_drop <= 1.0\n",
    "\n",
    "  extra_trees = TRUE, # Magic Sauce\n",
    "\n",
    "  seed = PARAM$lgb_semilla\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Aqui se cargan los hiperparametros que se optimizan\n",
    "#  en la Bayesian Optimization\n",
    "PARAM$bo_lgb <- makeParamSet(\n",
    "  makeNumericParam(\"learning_rate\", lower = 0.02, upper = 0.3),\n",
    "  makeNumericParam(\"feature_fraction\", lower = 0.01, upper = 1.0),\n",
    "  makeIntegerParam(\"num_leaves\", lower = 8L, upper = 1024L),\n",
    "  makeIntegerParam(\"min_data_in_leaf\", lower = 100L, upper = 50000L)\n",
    ")\n",
    "\n",
    "# si usted es ambicioso, y tiene paciencia, podria subir este valor a 100\n",
    "PARAM$bo_iteraciones <- 50 # iteraciones de la Optimizacion Bayesiana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# graba a un archivo los componentes de lista\n",
    "# para el primer registro, escribe antes los titulos\n",
    "\n",
    "loguear <- function(\n",
    "    reg, arch = NA, folder = \"./exp/\",\n",
    "    ext = \".txt\", verbose = TRUE) {\n",
    "  archivo <- arch\n",
    "  if (is.na(arch)) archivo <- paste0(folder, substitute(reg), ext)\n",
    "\n",
    "  if (!file.exists(archivo)) # Escribo los titulos\n",
    "    {\n",
    "      linea <- paste0(\n",
    "        \"fecha\\t\",\n",
    "        paste(list.names(reg), collapse = \"\\t\"), \"\\n\"\n",
    "      )\n",
    "\n",
    "      cat(linea, file = archivo)\n",
    "    }\n",
    "\n",
    "  linea <- paste0(\n",
    "    format(Sys.time(), \"%Y%m%d %H%M%S\"), \"\\t\", # la fecha y hora\n",
    "    gsub(\", \", \"\\t\", toString(reg)), \"\\n\"\n",
    "  )\n",
    "\n",
    "  cat(linea, file = archivo, append = TRUE) # grabo al archivo\n",
    "\n",
    "  if (verbose) cat(linea) # imprimo por pantalla\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------\n",
    "GLOBAL_arbol <- 0L\n",
    "GLOBAL_gan_max <- -Inf\n",
    "vcant_optima <- c()\n",
    "\n",
    "fganancia_lgbm_meseta <- function(probs, datos) {\n",
    "  vlabels <- get_field(datos, \"label\")\n",
    "  vpesos <- get_field(datos, \"weight\")\n",
    "\n",
    "\n",
    "  GLOBAL_arbol <<- GLOBAL_arbol + 1\n",
    "  tbl <- as.data.table(list(\n",
    "    \"prob\" = probs,\n",
    "    \"gan\" = ifelse(vlabels == 1 & vpesos > 1,\n",
    "      PARAM$hyperparametertuning$POS_ganancia,\n",
    "      PARAM$hyperparametertuning$NEG_ganancia  )\n",
    "  ))\n",
    "\n",
    "  setorder(tbl, -prob)\n",
    "  tbl[, posicion := .I]\n",
    "  tbl[, gan_acum := cumsum(gan)]\n",
    "\n",
    "  tbl[, gan_suavizada :=\n",
    "    frollmean(\n",
    "      x = gan_acum, n = 2001, align = \"center\",\n",
    "      na.rm = TRUE, hasNA = TRUE\n",
    "    )]\n",
    "\n",
    "  gan <- tbl[, max(gan_suavizada, na.rm = TRUE)]\n",
    "\n",
    "\n",
    "  pos <- which.max(tbl[, gan_suavizada])\n",
    "  vcant_optima <<- c(vcant_optima, pos)\n",
    "\n",
    "  if (GLOBAL_arbol %% 10 == 0) {\n",
    "    if (gan > GLOBAL_gan_max) GLOBAL_gan_max <<- gan\n",
    "\n",
    "    cat(\"\\r\")\n",
    "    cat(\n",
    "      \"Validate \", GLOBAL_iteracion, \" \", \" \",\n",
    "      GLOBAL_arbol, \"  \", gan, \"   \", GLOBAL_gan_max, \"   \"\n",
    "    )\n",
    "  }\n",
    "\n",
    "\n",
    "  return(list(\n",
    "    \"name\" = \"ganancia\",\n",
    "    \"value\" = gan,\n",
    "    \"higher_better\" = TRUE\n",
    "  ))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------\n",
    "\n",
    "EstimarGanancia_lightgbm <- function(x) {\n",
    "  gc()\n",
    "  GLOBAL_iteracion <<- GLOBAL_iteracion + 1L\n",
    "\n",
    "  # hago la union de los parametros basicos y los moviles que vienen en x\n",
    "  param_completo <- c(PARAM$lgb_basicos, x)\n",
    "\n",
    "  param_completo$early_stopping_rounds <-\n",
    "    as.integer(400 + 4 / param_completo$learning_rate)\n",
    "\n",
    "  GLOBAL_arbol <<- 0L\n",
    "  GLOBAL_gan_max <<- -Inf\n",
    "  vcant_optima <<- c()\n",
    "  set.seed(PARAM$lgb_semilla, kind = \"L'Ecuyer-CMRG\")\n",
    "  modelo_train <- lgb.train(\n",
    "    data = dtrain,\n",
    "    valids = list(valid = dvalidate),\n",
    "    eval = fganancia_lgbm_meseta,\n",
    "    param = param_completo,\n",
    "    verbose = -100\n",
    "  )\n",
    "\n",
    "  cat(\"\\n\")\n",
    "\n",
    "  cant_corte <- vcant_optima[modelo_train$best_iter]\n",
    "\n",
    "  # aplico el modelo a testing y calculo la ganancia\n",
    "  prediccion <- predict(\n",
    "    modelo_train,\n",
    "    data.matrix(dataset_test[, campos_buenos, with = FALSE])\n",
    "  )\n",
    "\n",
    "  tbl <- copy(dataset_test[, list(\"gan\" = ifelse(clase_ternaria == \"BAJA+2\",\n",
    "    PARAM$hyperparametertuning$POS_ganancia, \n",
    "    PARAM$hyperparametertuning$NEG_ganancia))])\n",
    "\n",
    "  tbl[, prob := prediccion]\n",
    "  setorder(tbl, -prob)\n",
    "  tbl[, gan_acum := cumsum(gan)]\n",
    "  tbl[, gan_suavizada := frollmean(\n",
    "    x = gan_acum, n = 2001,\n",
    "    align = \"center\", na.rm = TRUE, hasNA = TRUE\n",
    "  )]\n",
    "\n",
    "\n",
    "  ganancia_test <- tbl[, max(gan_suavizada, na.rm = TRUE)]\n",
    "\n",
    "  cantidad_test_normalizada <- which.max(tbl[, gan_suavizada])\n",
    "\n",
    "  rm(tbl)\n",
    "  gc()\n",
    "\n",
    "  ganancia_test_normalizada <- ganancia_test\n",
    "\n",
    "\n",
    "  # voy grabando las mejores column importance\n",
    "  if (ganancia_test_normalizada > GLOBAL_gananciamax) {\n",
    "    GLOBAL_gananciamax <<- ganancia_test_normalizada\n",
    "    tb_importancia <- as.data.table(lgb.importance(modelo_train))\n",
    "\n",
    "    fwrite(tb_importancia,\n",
    "      file = paste0(\"impo_\", sprintf(\"%03d\", GLOBAL_iteracion), \".txt\"),\n",
    "      sep = \"\\t\"\n",
    "    )\n",
    "\n",
    "    rm(tb_importancia)\n",
    "  }\n",
    "\n",
    "\n",
    "  # logueo final\n",
    "  ds <- list(\"cols\" = ncol(dtrain), \"rows\" = nrow(dtrain))\n",
    "  xx <- c(ds, copy(param_completo))\n",
    "\n",
    "  xx$early_stopping_rounds <- NULL\n",
    "  xx$num_iterations <- modelo_train$best_iter\n",
    "  xx$estimulos <- cantidad_test_normalizada\n",
    "  xx$ganancia <- ganancia_test_normalizada\n",
    "  xx$iteracion_bayesiana <- GLOBAL_iteracion\n",
    "\n",
    "  loguear(xx, arch = \"BO_log.txt\")\n",
    "\n",
    "  set.seed(PARAM$lgb_semilla, kind = \"L'Ecuyer-CMRG\")\n",
    "  return(ganancia_test_normalizada)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aquí empieza el programa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Aqui empieza el programa\n",
    "\n",
    "# Aqui se debe poner la carpeta de la computadora local\n",
    "setwd(\"~/buckets/b1/\") # Establezco el Working Directory\n",
    "\n",
    "# cargo el dataset donde voy a entrenar el modelo\n",
    "dataset <- fread(PARAM$input$dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# creo la carpeta donde va el experimento\n",
    "dir.create(\"./exp/\", showWarnings = FALSE)\n",
    "dir.create(paste0(\"./exp/\", PARAM$experimento, \"/\"), showWarnings = FALSE)\n",
    "\n",
    "# Establezco el Working Directory DEL EXPERIMENTO\n",
    "setwd(paste0(\"./exp/\", PARAM$experimento, \"/\"))\n",
    "\n",
    "# en estos archivos quedan los resultados\n",
    "kbayesiana <- paste0(PARAM$experimento, \".RDATA\")\n",
    "klog <- paste0(PARAM$experimento, \".txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Catastrophe Analysis  -------------------------------------------------------\n",
    "# deben ir cosas de este estilo\n",
    "#   dataset[foto_mes == 202006, active_quarter := NA]\n",
    "\n",
    "# Data Drifting\n",
    "# por ahora, no hago nada\n",
    "\n",
    "\n",
    "# Feature Engineering Historico  ----------------------------------------------\n",
    "#   aqui deben calcularse los  lags y  lag_delta\n",
    "#   Sin lags no hay paraiso !  corta la bocha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# ahora SI comienza la optimizacion Bayesiana\n",
    "\n",
    "GLOBAL_iteracion <- 0 # inicializo la variable global\n",
    "GLOBAL_gananciamax <- -1 # inicializo la variable global\n",
    "\n",
    "# si ya existe el archivo log, traigo hasta donde llegue\n",
    "if (file.exists(klog)) {\n",
    "  tabla_log <- fread(klog)\n",
    "  GLOBAL_iteracion <- nrow(tabla_log)\n",
    "  GLOBAL_gananciamax <- tabla_log[, max(ganancia)]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# paso la clase a binaria que tome valores {0,1}  enteros\n",
    "dataset[, clase01 := ifelse(clase_ternaria == \"CONTINUA\", 0L, 1L)]\n",
    "\n",
    "\n",
    "# los campos que se van a utilizar\n",
    "campos_buenos <- setdiff(\n",
    "  colnames(dataset),\n",
    "  c(\"clase_ternaria\", \"clase01\", \"azar\", \"training\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# defino los datos que forma parte del training\n",
    "# aqui se hace el undersampling de los CONTINUA\n",
    "set.seed(PARAM$trainingstrategy$semilla_azar)\n",
    "dataset[, azar := runif(nrow(dataset))]\n",
    "dataset[, training := 0L]\n",
    "dataset[\n",
    "  foto_mes %in% PARAM$input$training &\n",
    "    (azar <= PARAM$trainingstrategy$undersampling | clase_ternaria %in% c(\"BAJA+1\", \"BAJA+2\")),\n",
    "  training := 1L\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# dejo los datos en el formato que necesita LightGBM\n",
    "dtrain <- lgb.Dataset(\n",
    "  data = data.matrix(dataset[training == 1L, campos_buenos, with = FALSE]),\n",
    "  label = dataset[training == 1L, clase01],\n",
    "  weight = dataset[training == 1L, \n",
    "    ifelse(clase_ternaria == \"BAJA+2\", 1.0000001, \n",
    "      ifelse(clase_ternaria == \"BAJA+1\", 1.0, 1.0))],\n",
    "  free_raw_data = FALSE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# defino los datos que forman parte de validation\n",
    "#  no hay undersampling\n",
    "dataset[, validation := 0L]\n",
    "dataset[ foto_mes %in% PARAM$input$validation,  validation := 1L]\n",
    "\n",
    "dvalidate <- lgb.Dataset(\n",
    "  data = data.matrix(dataset[validation == 1L, campos_buenos, with = FALSE]),\n",
    "  label = dataset[validation == 1L, clase01],\n",
    "  weight = dataset[validation == 1L, \n",
    "    ifelse(clase_ternaria == \"BAJA+2\", 1.0000001, \n",
    "      ifelse(clase_ternaria == \"BAJA+1\", 1.0, 1.0))],\n",
    "  free_raw_data = FALSE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# defino los datos de testing\n",
    "dataset[, testing := 0L]\n",
    "dataset[ foto_mes %in% PARAM$input$testing,  testing := 1L]\n",
    "\n",
    "\n",
    "dataset_test <- dataset[testing == 1, ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# libero espacio\n",
    "rm(dataset)\n",
    "gc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Aqui comienza la configuracion de la Bayesian Optimization\n",
    "funcion_optimizar <- EstimarGanancia_lightgbm # la funcion que voy a maximizar\n",
    "\n",
    "configureMlr(show.learner.output = FALSE)\n",
    "\n",
    "# configuro la busqueda bayesiana,  los hiperparametros que se van a optimizar\n",
    "# por favor, no desesperarse por lo complejo\n",
    "obj.fun <- makeSingleObjectiveFunction(\n",
    "  fn = funcion_optimizar, # la funcion que voy a maximizar\n",
    "  minimize = FALSE, # estoy Maximizando la ganancia\n",
    "  noisy = TRUE,\n",
    "  par.set = PARAM$bo_lgb, # definido al comienzo del programa\n",
    "  has.simple.signature = FALSE # paso los parametros en una lista\n",
    ")\n",
    "\n",
    "# cada 600 segundos guardo el resultado intermedio\n",
    "ctrl <- makeMBOControl(\n",
    "  save.on.disk.at.time = 600, # se graba cada 600 segundos\n",
    "  save.file.path = kbayesiana\n",
    ") # se graba cada 600 segundos\n",
    "\n",
    "# indico la cantidad de iteraciones que va a tener la Bayesian Optimization\n",
    "ctrl <- setMBOControlTermination(\n",
    "  ctrl,\n",
    "  iters = PARAM$bo_iteraciones\n",
    ") # cantidad de iteraciones\n",
    "\n",
    "# defino el método estandar para la creacion de los puntos iniciales,\n",
    "# los \"No Inteligentes\"\n",
    "ctrl <- setMBOControlInfill(ctrl, crit = makeMBOInfillCritEI())\n",
    "\n",
    "\n",
    "# establezco la funcion que busca el maximo\n",
    "surr.km <- makeLearner(\n",
    "  \"regr.km\",\n",
    "  predict.type = \"se\",\n",
    "  covtype = \"matern3_2\",\n",
    "  control = list(trace = TRUE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# inicio la optimizacion bayesiana\n",
    "if (!file.exists(kbayesiana)) {\n",
    "  run <- mbo(obj.fun, learner = surr.km, control = ctrl)\n",
    "} else {\n",
    "  run <- mboContinue(kbayesiana) # retomo en caso que ya exista\n",
    "}\n",
    "\n",
    "\n",
    "cat(\"\\n\\nLa optimizacion Bayesiana ha terminado\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semilla 5, ajustado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# limpio la memoria\n",
    "rm(list = ls()) # remove all objects\n",
    "gc() # garbage collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "require(\"data.table\")\n",
    "require(\"rlist\")\n",
    "require(\"lightgbm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# paquetes necesarios para la Bayesian Optimization\n",
    "require(\"DiceKriging\")\n",
    "require(\"mlrMBO\")\n",
    "\n",
    "# para que se detenga ante el primer error\n",
    "# y muestre el stack de funciones invocadas\n",
    "options(error = function() {\n",
    "  traceback(20)\n",
    "  options(error = NULL)\n",
    "  stop(\"exiting after script error\")\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definición de Parametros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# defino los parametros de la corrida, en una lista, la variable global  PARAM\n",
    "#  muy pronto esto se leera desde un archivo formato .yaml\n",
    "PARAM <- list()\n",
    "\n",
    "PARAM$experimento <- \"HT8230 (Clase 12, Ajustado Sem5)\"\n",
    "\n",
    "PARAM$input$dataset <- \"datasets/competencia_03_infla_ajustado_lags.csv.gz\"\n",
    "\n",
    "# los meses en los que vamos a entrenar\n",
    "#  mucha magia emerger de esta eleccion\n",
    "PARAM$input$testing <- c(202107) # Último mes, lo más cercano al 202109 de kaggle\n",
    "PARAM$input$validation <- c(202106)\n",
    "PARAM$input$training <- c(202105, 202104, 202103, 202102, 202101,202012) # 6 meses de entrenamiento\t\n",
    "\n",
    "# un undersampling de 0.1  toma solo el 10% de los CONTINUA\n",
    "PARAM$trainingstrategy$undersampling <- 1.0\n",
    "PARAM$trainingstrategy$semilla_azar <- c(279511, 279523, 279541, 279551, 279571)  # Aqui poner su  primer  semilla/ pongo todas mis semillas\n",
    "\n",
    "PARAM$hyperparametertuning$POS_ganancia <- 273000\n",
    "PARAM$hyperparametertuning$NEG_ganancia <- -7000\n",
    "\n",
    "# Aqui va semilla\n",
    "PARAM$lgb_semilla <- 279571"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Hiperparametros FIJOS de  lightgbm\n",
    "PARAM$lgb_basicos <- list(\n",
    "  boosting = \"gbdt\", # puede ir  dart  , ni pruebe random_forest\n",
    "  objective = \"binary\",\n",
    "  metric = \"custom\",\n",
    "  first_metric_only = TRUE,\n",
    "  boost_from_average = TRUE,\n",
    "  feature_pre_filter = FALSE,\n",
    "  force_row_wise = TRUE, # para reducir warnings\n",
    "  verbosity = -100,\n",
    "  max_depth = -1L, # -1 significa no limitar,  por ahora lo dejo fijo\n",
    "  min_gain_to_split = 0.0, # min_gain_to_split >= 0.0\n",
    "  min_sum_hessian_in_leaf = 0.001, #  min_sum_hessian_in_leaf >= 0.0\n",
    "  lambda_l1 = 0.0, # lambda_l1 >= 0.0\n",
    "  lambda_l2 = 0.0, # lambda_l2 >= 0.0\n",
    "  max_bin = 31L, # lo debo dejar fijo, no participa de la BO\n",
    "  num_iterations = 9999, # un numero muy grande, lo limita early_stopping_rounds\n",
    "\n",
    "  bagging_fraction = 1.0, # 0.0 < bagging_fraction <= 1.0\n",
    "  pos_bagging_fraction = 1.0, # 0.0 < pos_bagging_fraction <= 1.0\n",
    "  neg_bagging_fraction = 1.0, # 0.0 < neg_bagging_fraction <= 1.0\n",
    "  is_unbalance = FALSE, #\n",
    "  scale_pos_weight = 1.0, # scale_pos_weight > 0.0\n",
    "\n",
    "  drop_rate = 0.1, # 0.0 < neg_bagging_fraction <= 1.0\n",
    "  max_drop = 50, # <=0 means no limit\n",
    "  skip_drop = 0.5, # 0.0 <= skip_drop <= 1.0\n",
    "\n",
    "  extra_trees = TRUE, # Magic Sauce\n",
    "\n",
    "  seed = PARAM$lgb_semilla\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Aqui se cargan los hiperparametros que se optimizan\n",
    "#  en la Bayesian Optimization\n",
    "PARAM$bo_lgb <- makeParamSet(\n",
    "  makeNumericParam(\"learning_rate\", lower = 0.02, upper = 0.3),\n",
    "  makeNumericParam(\"feature_fraction\", lower = 0.01, upper = 1.0),\n",
    "  makeIntegerParam(\"num_leaves\", lower = 8L, upper = 1024L),\n",
    "  makeIntegerParam(\"min_data_in_leaf\", lower = 100L, upper = 50000L)\n",
    ")\n",
    "\n",
    "# si usted es ambicioso, y tiene paciencia, podria subir este valor a 100\n",
    "PARAM$bo_iteraciones <- 50 # iteraciones de la Optimizacion Bayesiana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# graba a un archivo los componentes de lista\n",
    "# para el primer registro, escribe antes los titulos\n",
    "\n",
    "loguear <- function(\n",
    "    reg, arch = NA, folder = \"./exp/\",\n",
    "    ext = \".txt\", verbose = TRUE) {\n",
    "  archivo <- arch\n",
    "  if (is.na(arch)) archivo <- paste0(folder, substitute(reg), ext)\n",
    "\n",
    "  if (!file.exists(archivo)) # Escribo los titulos\n",
    "    {\n",
    "      linea <- paste0(\n",
    "        \"fecha\\t\",\n",
    "        paste(list.names(reg), collapse = \"\\t\"), \"\\n\"\n",
    "      )\n",
    "\n",
    "      cat(linea, file = archivo)\n",
    "    }\n",
    "\n",
    "  linea <- paste0(\n",
    "    format(Sys.time(), \"%Y%m%d %H%M%S\"), \"\\t\", # la fecha y hora\n",
    "    gsub(\", \", \"\\t\", toString(reg)), \"\\n\"\n",
    "  )\n",
    "\n",
    "  cat(linea, file = archivo, append = TRUE) # grabo al archivo\n",
    "\n",
    "  if (verbose) cat(linea) # imprimo por pantalla\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------\n",
    "GLOBAL_arbol <- 0L\n",
    "GLOBAL_gan_max <- -Inf\n",
    "vcant_optima <- c()\n",
    "\n",
    "fganancia_lgbm_meseta <- function(probs, datos) {\n",
    "  vlabels <- get_field(datos, \"label\")\n",
    "  vpesos <- get_field(datos, \"weight\")\n",
    "\n",
    "\n",
    "  GLOBAL_arbol <<- GLOBAL_arbol + 1\n",
    "  tbl <- as.data.table(list(\n",
    "    \"prob\" = probs,\n",
    "    \"gan\" = ifelse(vlabels == 1 & vpesos > 1,\n",
    "      PARAM$hyperparametertuning$POS_ganancia,\n",
    "      PARAM$hyperparametertuning$NEG_ganancia  )\n",
    "  ))\n",
    "\n",
    "  setorder(tbl, -prob)\n",
    "  tbl[, posicion := .I]\n",
    "  tbl[, gan_acum := cumsum(gan)]\n",
    "\n",
    "  tbl[, gan_suavizada :=\n",
    "    frollmean(\n",
    "      x = gan_acum, n = 2001, align = \"center\",\n",
    "      na.rm = TRUE, hasNA = TRUE\n",
    "    )]\n",
    "\n",
    "  gan <- tbl[, max(gan_suavizada, na.rm = TRUE)]\n",
    "\n",
    "\n",
    "  pos <- which.max(tbl[, gan_suavizada])\n",
    "  vcant_optima <<- c(vcant_optima, pos)\n",
    "\n",
    "  if (GLOBAL_arbol %% 10 == 0) {\n",
    "    if (gan > GLOBAL_gan_max) GLOBAL_gan_max <<- gan\n",
    "\n",
    "    cat(\"\\r\")\n",
    "    cat(\n",
    "      \"Validate \", GLOBAL_iteracion, \" \", \" \",\n",
    "      GLOBAL_arbol, \"  \", gan, \"   \", GLOBAL_gan_max, \"   \"\n",
    "    )\n",
    "  }\n",
    "\n",
    "\n",
    "  return(list(\n",
    "    \"name\" = \"ganancia\",\n",
    "    \"value\" = gan,\n",
    "    \"higher_better\" = TRUE\n",
    "  ))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------\n",
    "\n",
    "EstimarGanancia_lightgbm <- function(x) {\n",
    "  gc()\n",
    "  GLOBAL_iteracion <<- GLOBAL_iteracion + 1L\n",
    "\n",
    "  # hago la union de los parametros basicos y los moviles que vienen en x\n",
    "  param_completo <- c(PARAM$lgb_basicos, x)\n",
    "\n",
    "  param_completo$early_stopping_rounds <-\n",
    "    as.integer(400 + 4 / param_completo$learning_rate)\n",
    "\n",
    "  GLOBAL_arbol <<- 0L\n",
    "  GLOBAL_gan_max <<- -Inf\n",
    "  vcant_optima <<- c()\n",
    "  set.seed(PARAM$lgb_semilla, kind = \"L'Ecuyer-CMRG\")\n",
    "  modelo_train <- lgb.train(\n",
    "    data = dtrain,\n",
    "    valids = list(valid = dvalidate),\n",
    "    eval = fganancia_lgbm_meseta,\n",
    "    param = param_completo,\n",
    "    verbose = -100\n",
    "  )\n",
    "\n",
    "  cat(\"\\n\")\n",
    "\n",
    "  cant_corte <- vcant_optima[modelo_train$best_iter]\n",
    "\n",
    "  # aplico el modelo a testing y calculo la ganancia\n",
    "  prediccion <- predict(\n",
    "    modelo_train,\n",
    "    data.matrix(dataset_test[, campos_buenos, with = FALSE])\n",
    "  )\n",
    "\n",
    "  tbl <- copy(dataset_test[, list(\"gan\" = ifelse(clase_ternaria == \"BAJA+2\",\n",
    "    PARAM$hyperparametertuning$POS_ganancia, \n",
    "    PARAM$hyperparametertuning$NEG_ganancia))])\n",
    "\n",
    "  tbl[, prob := prediccion]\n",
    "  setorder(tbl, -prob)\n",
    "  tbl[, gan_acum := cumsum(gan)]\n",
    "  tbl[, gan_suavizada := frollmean(\n",
    "    x = gan_acum, n = 2001,\n",
    "    align = \"center\", na.rm = TRUE, hasNA = TRUE\n",
    "  )]\n",
    "\n",
    "\n",
    "  ganancia_test <- tbl[, max(gan_suavizada, na.rm = TRUE)]\n",
    "\n",
    "  cantidad_test_normalizada <- which.max(tbl[, gan_suavizada])\n",
    "\n",
    "  rm(tbl)\n",
    "  gc()\n",
    "\n",
    "  ganancia_test_normalizada <- ganancia_test\n",
    "\n",
    "\n",
    "  # voy grabando las mejores column importance\n",
    "  if (ganancia_test_normalizada > GLOBAL_gananciamax) {\n",
    "    GLOBAL_gananciamax <<- ganancia_test_normalizada\n",
    "    tb_importancia <- as.data.table(lgb.importance(modelo_train))\n",
    "\n",
    "    fwrite(tb_importancia,\n",
    "      file = paste0(\"impo_\", sprintf(\"%03d\", GLOBAL_iteracion), \".txt\"),\n",
    "      sep = \"\\t\"\n",
    "    )\n",
    "\n",
    "    rm(tb_importancia)\n",
    "  }\n",
    "\n",
    "\n",
    "  # logueo final\n",
    "  ds <- list(\"cols\" = ncol(dtrain), \"rows\" = nrow(dtrain))\n",
    "  xx <- c(ds, copy(param_completo))\n",
    "\n",
    "  xx$early_stopping_rounds <- NULL\n",
    "  xx$num_iterations <- modelo_train$best_iter\n",
    "  xx$estimulos <- cantidad_test_normalizada\n",
    "  xx$ganancia <- ganancia_test_normalizada\n",
    "  xx$iteracion_bayesiana <- GLOBAL_iteracion\n",
    "\n",
    "  loguear(xx, arch = \"BO_log.txt\")\n",
    "\n",
    "  set.seed(PARAM$lgb_semilla, kind = \"L'Ecuyer-CMRG\")\n",
    "  return(ganancia_test_normalizada)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aquí empieza el programa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Aqui empieza el programa\n",
    "\n",
    "# Aqui se debe poner la carpeta de la computadora local\n",
    "setwd(\"~/buckets/b1/\") # Establezco el Working Directory\n",
    "\n",
    "# cargo el dataset donde voy a entrenar el modelo\n",
    "dataset <- fread(PARAM$input$dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# creo la carpeta donde va el experimento\n",
    "dir.create(\"./exp/\", showWarnings = FALSE)\n",
    "dir.create(paste0(\"./exp/\", PARAM$experimento, \"/\"), showWarnings = FALSE)\n",
    "\n",
    "# Establezco el Working Directory DEL EXPERIMENTO\n",
    "setwd(paste0(\"./exp/\", PARAM$experimento, \"/\"))\n",
    "\n",
    "# en estos archivos quedan los resultados\n",
    "kbayesiana <- paste0(PARAM$experimento, \".RDATA\")\n",
    "klog <- paste0(PARAM$experimento, \".txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Catastrophe Analysis  -------------------------------------------------------\n",
    "# deben ir cosas de este estilo\n",
    "#   dataset[foto_mes == 202006, active_quarter := NA]\n",
    "\n",
    "# Data Drifting\n",
    "# por ahora, no hago nada\n",
    "\n",
    "\n",
    "# Feature Engineering Historico  ----------------------------------------------\n",
    "#   aqui deben calcularse los  lags y  lag_delta\n",
    "#   Sin lags no hay paraiso !  corta la bocha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# ahora SI comienza la optimizacion Bayesiana\n",
    "\n",
    "GLOBAL_iteracion <- 0 # inicializo la variable global\n",
    "GLOBAL_gananciamax <- -1 # inicializo la variable global\n",
    "\n",
    "# si ya existe el archivo log, traigo hasta donde llegue\n",
    "if (file.exists(klog)) {\n",
    "  tabla_log <- fread(klog)\n",
    "  GLOBAL_iteracion <- nrow(tabla_log)\n",
    "  GLOBAL_gananciamax <- tabla_log[, max(ganancia)]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# paso la clase a binaria que tome valores {0,1}  enteros\n",
    "dataset[, clase01 := ifelse(clase_ternaria == \"CONTINUA\", 0L, 1L)]\n",
    "\n",
    "\n",
    "# los campos que se van a utilizar\n",
    "campos_buenos <- setdiff(\n",
    "  colnames(dataset),\n",
    "  c(\"clase_ternaria\", \"clase01\", \"azar\", \"training\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# defino los datos que forma parte del training\n",
    "# aqui se hace el undersampling de los CONTINUA\n",
    "set.seed(PARAM$trainingstrategy$semilla_azar)\n",
    "dataset[, azar := runif(nrow(dataset))]\n",
    "dataset[, training := 0L]\n",
    "dataset[\n",
    "  foto_mes %in% PARAM$input$training &\n",
    "    (azar <= PARAM$trainingstrategy$undersampling | clase_ternaria %in% c(\"BAJA+1\", \"BAJA+2\")),\n",
    "  training := 1L\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# dejo los datos en el formato que necesita LightGBM\n",
    "dtrain <- lgb.Dataset(\n",
    "  data = data.matrix(dataset[training == 1L, campos_buenos, with = FALSE]),\n",
    "  label = dataset[training == 1L, clase01],\n",
    "  weight = dataset[training == 1L, \n",
    "    ifelse(clase_ternaria == \"BAJA+2\", 1.0000001, \n",
    "      ifelse(clase_ternaria == \"BAJA+1\", 1.0, 1.0))],\n",
    "  free_raw_data = FALSE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# defino los datos que forman parte de validation\n",
    "#  no hay undersampling\n",
    "dataset[, validation := 0L]\n",
    "dataset[ foto_mes %in% PARAM$input$validation,  validation := 1L]\n",
    "\n",
    "dvalidate <- lgb.Dataset(\n",
    "  data = data.matrix(dataset[validation == 1L, campos_buenos, with = FALSE]),\n",
    "  label = dataset[validation == 1L, clase01],\n",
    "  weight = dataset[validation == 1L, \n",
    "    ifelse(clase_ternaria == \"BAJA+2\", 1.0000001, \n",
    "      ifelse(clase_ternaria == \"BAJA+1\", 1.0, 1.0))],\n",
    "  free_raw_data = FALSE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# defino los datos de testing\n",
    "dataset[, testing := 0L]\n",
    "dataset[ foto_mes %in% PARAM$input$testing,  testing := 1L]\n",
    "\n",
    "\n",
    "dataset_test <- dataset[testing == 1, ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# libero espacio\n",
    "rm(dataset)\n",
    "gc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Aqui comienza la configuracion de la Bayesian Optimization\n",
    "funcion_optimizar <- EstimarGanancia_lightgbm # la funcion que voy a maximizar\n",
    "\n",
    "configureMlr(show.learner.output = FALSE)\n",
    "\n",
    "# configuro la busqueda bayesiana,  los hiperparametros que se van a optimizar\n",
    "# por favor, no desesperarse por lo complejo\n",
    "obj.fun <- makeSingleObjectiveFunction(\n",
    "  fn = funcion_optimizar, # la funcion que voy a maximizar\n",
    "  minimize = FALSE, # estoy Maximizando la ganancia\n",
    "  noisy = TRUE,\n",
    "  par.set = PARAM$bo_lgb, # definido al comienzo del programa\n",
    "  has.simple.signature = FALSE # paso los parametros en una lista\n",
    ")\n",
    "\n",
    "# cada 600 segundos guardo el resultado intermedio\n",
    "ctrl <- makeMBOControl(\n",
    "  save.on.disk.at.time = 600, # se graba cada 600 segundos\n",
    "  save.file.path = kbayesiana\n",
    ") # se graba cada 600 segundos\n",
    "\n",
    "# indico la cantidad de iteraciones que va a tener la Bayesian Optimization\n",
    "ctrl <- setMBOControlTermination(\n",
    "  ctrl,\n",
    "  iters = PARAM$bo_iteraciones\n",
    ") # cantidad de iteraciones\n",
    "\n",
    "# defino el método estandar para la creacion de los puntos iniciales,\n",
    "# los \"No Inteligentes\"\n",
    "ctrl <- setMBOControlInfill(ctrl, crit = makeMBOInfillCritEI())\n",
    "\n",
    "\n",
    "# establezco la funcion que busca el maximo\n",
    "surr.km <- makeLearner(\n",
    "  \"regr.km\",\n",
    "  predict.type = \"se\",\n",
    "  covtype = \"matern3_2\",\n",
    "  control = list(trace = TRUE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# inicio la optimizacion bayesiana\n",
    "if (!file.exists(kbayesiana)) {\n",
    "  run <- mbo(obj.fun, learner = surr.km, control = ctrl)\n",
    "} else {\n",
    "  run <- mboContinue(kbayesiana) # retomo en caso que ya exista\n",
    "}\n",
    "\n",
    "\n",
    "cat(\"\\n\\nLa optimizacion Bayesiana ha terminado\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.3.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
