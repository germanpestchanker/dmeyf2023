{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importación de Librerías"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Este script esta pensado para correr en Google Cloud\n",
    "-   8 vCPU\n",
    "-  32 GB memoria RAM\n",
    "\n",
    "Se entrena con clase_binaria2  POS =  { BAJA+1, BAJA+2 }\n",
    "- Optimizacion Bayesiana de hiperparametros de  lightgbm,\n",
    "- con el metodo TRADICIONAL de los hiperparametros originales de lightgbm\n",
    "- 5-fold cross validation el cual es muuuy lento\n",
    "- la probabilidad de corte es un hiperparametro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# limpio la memoria\n",
    "rm(list = ls()) # remove all objects\n",
    "gc() # garbage collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "require(\"data.table\")\n",
    "require(\"rlist\")\n",
    "\n",
    "require(\"lightgbm\")\n",
    "\n",
    "# paquetes necesarios para la Bayesian Optimization\n",
    "require(\"DiceKriging\")\n",
    "require(\"mlrMBO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# para que se detenga ante el primer error\n",
    "# y muestre el stack de funciones invocadas\n",
    "options(error = function() {\n",
    "  traceback(20)\n",
    "  options(error = NULL)\n",
    "  stop(\"exiting after script error\")\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# defino los parametros de la corrida, en una lista, la variable global  PARAM\n",
    "#  muy pronto esto se leera desde un archivo formato .yaml\n",
    "PARAM <- list()\n",
    "\n",
    "PARAM$experimento <- \"HT5230\"\n",
    "\n",
    "PARAM$input$dataset <- \"competencia_02_clase7.csv.gz\"\n",
    "\n",
    " # los meses en los que vamos a entrenar\n",
    "PARAM$input$training <- c(201901, 201902, 201903, 201904, 201905,201906, 201907, 201908, 201909, 201910,201911, 201912,202001, 202101, 202102, 202103, 202104, 202105)\n",
    "\n",
    "# un undersampling de 0.1  toma solo el 10% de los CONTINUA\n",
    "PARAM$trainingstrategy$undersampling <- 1.0\n",
    "PARAM$trainingstrategy$semilla_azar <- 279511 # Aqui poner su  primer  semilla\n",
    "\n",
    "PARAM$hyperparametertuning$iteraciones <- 100\n",
    "PARAM$hyperparametertuning$xval_folds <- 5\n",
    "PARAM$hyperparametertuning$POS_ganancia <- 273000\n",
    "PARAM$hyperparametertuning$NEG_ganancia <- -7000\n",
    "\n",
    "# Aqui poner su segunda semilla\n",
    "PARAM$hyperparametertuning$semilla_azar <- 279523"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------\n",
    "\n",
    "# Aqui se cargan los bordes de los hiperparametros\n",
    "hs <- makeParamSet(\n",
    "  makeNumericParam(\"learning_rate\", lower = 0.01, upper = 0.3),\n",
    "  makeNumericParam(\"feature_fraction\", lower = 0.2, upper = 1.0),\n",
    "  makeIntegerParam(\"min_data_in_leaf\", lower = 1L, upper = 8000L),\n",
    "  makeIntegerParam(\"num_leaves\", lower = 16L, upper = 1024L),\n",
    "  makeIntegerParam(\"envios\", lower = 5000L, upper = 15000L)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------\n",
    "# graba a un archivo los componentes de lista\n",
    "# para el primer registro, escribe antes los titulos\n",
    "\n",
    "loguear <- function(\n",
    "    reg, arch = NA, folder = \"./exp/\",\n",
    "    ext = \".txt\", verbose = TRUE) {\n",
    "  archivo <- arch\n",
    "  if (is.na(arch)) archivo <- paste0(folder, substitute(reg), ext)\n",
    "\n",
    "  if (!file.exists(archivo)) # Escribo los titulos\n",
    "    {\n",
    "      linea <- paste0(\n",
    "        \"fecha\\t\",\n",
    "        paste(list.names(reg), collapse = \"\\t\"), \"\\n\"\n",
    "      )\n",
    "\n",
    "      cat(linea, file = archivo)\n",
    "    }\n",
    "\n",
    "  linea <- paste0(\n",
    "    format(Sys.time(), \"%Y%m%d %H%M%S\"), \"\\t\", # la fecha y hora\n",
    "    gsub(\", \", \"\\t\", toString(reg)), \"\\n\"\n",
    "  )\n",
    "\n",
    "  cat(linea, file = archivo, append = TRUE) # grabo al archivo\n",
    "\n",
    "  if (verbose) cat(linea) # imprimo por pantalla\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------\n",
    "# esta funcion calcula internamente la ganancia de la prediccion probs\n",
    "# es llamada por lightgbm luego de construir cada  arbolito\n",
    "\n",
    "fganancia_logistic_lightgbm <- function(probs, datos) {\n",
    "  vpesos <- get_field(datos, \"weight\")\n",
    "\n",
    "  # vector de ganancias\n",
    "  vgan <- ifelse(vpesos == 1.0000002, PARAM$hyperparametertuning$POS_ganancia,\n",
    "    ifelse(vpesos == 1.0000001, PARAM$hyperparametertuning$NEG_ganancia,\n",
    "      PARAM$hyperparametertuning$NEG_ganancia /\n",
    "        PARAM$trainingstrategy$undersampling\n",
    "    )\n",
    "  )\n",
    "\n",
    "  tbl <- as.data.table(list(\"vprobs\" = probs, \"vgan\" = vgan))\n",
    "  setorder(tbl, -vprobs)\n",
    "  ganancia <- tbl[1:GLOBAL_envios, sum(vgan)]\n",
    "\n",
    "  return(list(\n",
    "    \"name\" = \"ganancia\",\n",
    "    \"value\" = ganancia,\n",
    "    \"higher_better\" = TRUE\n",
    "  ))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------\n",
    "# esta funcion solo puede recibir los parametros que se estan optimizando\n",
    "# el resto de los parametros se pasan como variables globales,\n",
    "# la semilla del mal ...\n",
    "\n",
    "\n",
    "EstimarGanancia_lightgbm <- function(x) {\n",
    "  gc() # libero memoria\n",
    "\n",
    "  # llevo el registro de la iteracion por la que voy\n",
    "  GLOBAL_iteracion <<- GLOBAL_iteracion + 1\n",
    "\n",
    "  # para usar en fganancia_logistic_lightgbm\n",
    "  # asigno la variable global\n",
    "  GLOBAL_envios <<- as.integer(x$envios / PARAM$hyperparametertuning$xval_folds)\n",
    "\n",
    "  # cantidad de folds para cross validation\n",
    "  kfolds <- PARAM$hyperparametertuning$xval_folds\n",
    "\n",
    "  param_basicos <- list(\n",
    "    objective = \"binary\",\n",
    "    metric = \"custom\",\n",
    "    first_metric_only = TRUE,\n",
    "    boost_from_average = TRUE,\n",
    "    feature_pre_filter = FALSE,\n",
    "    verbosity = -100,\n",
    "    max_depth = -1, # -1 significa no limitar,  por ahora lo dejo fijo\n",
    "    min_gain_to_split = 0.0, # por ahora, lo dejo fijo\n",
    "    lambda_l1 = 0.0, # por ahora, lo dejo fijo\n",
    "    lambda_l2 = 0.0, # por ahora, lo dejo fijo\n",
    "    max_bin = 31, # por ahora, lo dejo fijo\n",
    "    num_iterations = 9999, # valor grande, lo limita early_stopping_rounds\n",
    "    force_row_wise = TRUE, # para evitar warning\n",
    "    seed = PARAM$hyperparametertuning$semilla_azar\n",
    "  )\n",
    "\n",
    "  # el parametro discolo, que depende de otro\n",
    "  param_variable <- list(\n",
    "    early_stopping_rounds =\n",
    "      as.integer(50 + 5 / x$learning_rate)\n",
    "  )\n",
    "\n",
    "  param_completo <- c(param_basicos, param_variable, x)\n",
    "\n",
    "  set.seed(PARAM$hyperparametertuning$semilla_azar)\n",
    "  modelocv <- lgb.cv(\n",
    "    data = dtrain,\n",
    "    eval = fganancia_logistic_lightgbm,\n",
    "    stratified = TRUE, # sobre el cross validation\n",
    "    nfold = kfolds, # folds del cross validation\n",
    "    param = param_completo,\n",
    "    verbose = -100\n",
    "  )\n",
    "\n",
    "  # obtengo la ganancia\n",
    "  ganancia <- unlist(modelocv$record_evals$valid$ganancia$eval)[modelocv$best_iter]\n",
    "\n",
    "  ganancia_normalizada <- ganancia * kfolds # normailizo la ganancia\n",
    "\n",
    "  # asigno el mejor num_iterations\n",
    "  param_completo$num_iterations <- modelocv$best_iter\n",
    "  # elimino de la lista el componente\n",
    "  param_completo[\"early_stopping_rounds\"] <- NULL\n",
    "\n",
    "  # Voy registrando la importancia de variables\n",
    "  if (ganancia_normalizada > GLOBAL_gananciamax) {\n",
    "    GLOBAL_gananciamax <<- ganancia_normalizada\n",
    "    modelo <- lgb.train(\n",
    "      data = dtrain,\n",
    "      param = param_completo,\n",
    "      verbose = -100\n",
    "    )\n",
    "\n",
    "    tb_importancia <- as.data.table(lgb.importance(modelo))\n",
    "    archivo_importancia <- paste0(\"impo_\", GLOBAL_iteracion, \".txt\")\n",
    "    fwrite(tb_importancia,\n",
    "      file = archivo_importancia,\n",
    "      sep = \"\\t\"\n",
    "    )\n",
    "  }\n",
    "\n",
    "\n",
    "  # el lenguaje R permite asignarle ATRIBUTOS a cualquier variable\n",
    "  # esta es la forma de devolver un parametro extra\n",
    "  attr(ganancia_normalizada, \"extras\") <-\n",
    "    list(\"num_iterations\" = modelocv$best_iter)\n",
    "\n",
    "  # logueo\n",
    "  xx <- param_completo\n",
    "  xx$ganancia <- ganancia_normalizada # le agrego la ganancia\n",
    "  xx$iteracion <- GLOBAL_iteracion\n",
    "  loguear(xx, arch = klog)\n",
    "\n",
    "  return(ganancia_normalizada)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aqui empieza el programa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Aqui se debe poner la carpeta de la computadora local\n",
    "setwd(\"~/buckets/b1/\")  # Establezco el Working Directory\n",
    "\n",
    "# cargo el dataset donde voy a entrenar el modelo\n",
    "dataset <- fread(PARAM$input$dataset)\n",
    "\n",
    "# creo la carpeta donde va el experimento\n",
    "dir.create(\"./exp/\", showWarnings = FALSE)\n",
    "dir.create(paste0(\"./exp/\", PARAM$experimento, \"/\"), showWarnings = FALSE)\n",
    "\n",
    "# Establezco el Working Directory DEL EXPERIMENTO\n",
    "setwd(paste0(\"./exp/\", PARAM$experimento, \"/\"))\n",
    "\n",
    "# en estos archivos quedan los resultados\n",
    "kbayesiana <- paste0(PARAM$experimento, \".RDATA\")\n",
    "klog <- paste0(PARAM$experimento, \".txt\")\n",
    "\n",
    "\n",
    "GLOBAL_iteracion <- 0 # inicializo la variable global\n",
    "GLOBAL_gananciamax <- -1 # inicializo la variable global\n",
    "\n",
    "# si ya existe el archivo log, traigo hasta donde llegue\n",
    "if (file.exists(klog)) {\n",
    "  tabla_log <- fread(klog)\n",
    "  GLOBAL_iteracion <- nrow(tabla_log)\n",
    "  GLOBAL_gananciamax <- tabla_log[, max(ganancia)]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# paso la clase a binaria que tome valores {0,1}  enteros\n",
    "dataset[\n",
    "  foto_mes %in% PARAM$input$training,\n",
    "  clase01 := ifelse(clase_ternaria == \"CONTINUA\", 0L, 1L)\n",
    "]\n",
    "\n",
    "\n",
    "# los campos que se van a utilizar\n",
    "campos_buenos <- setdiff(\n",
    "  colnames(dataset),\n",
    "  c(\"clase_ternaria\", \"clase01\", \"azar\", \"training\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# defino los datos que forma parte del training\n",
    "# aqui se hace el undersampling de los CONTINUA\n",
    "set.seed(PARAM$trainingstrategy$semilla_azar)\n",
    "dataset[, azar := runif(nrow(dataset))]\n",
    "dataset[, training := 0L]\n",
    "dataset[\n",
    "  foto_mes %in% PARAM$input$training &\n",
    "    (azar <= PARAM$trainingstrategy$undersampling | clase_ternaria %in% c(\"BAJA+1\", \"BAJA+2\")),\n",
    "  training := 1L\n",
    "]\n",
    "\n",
    "# dejo los datos en el formato que necesita LightGBM\n",
    "dtrain <- lgb.Dataset(\n",
    "  data = data.matrix(dataset[training == 1L, campos_buenos, with = FALSE]),\n",
    "  label = dataset[training == 1L, clase01],\n",
    "  weight = dataset[training == 1L, ifelse(clase_ternaria == \"BAJA+2\", 1.0000002, ifelse(clase_ternaria == \"BAJA+1\", 1.0000001, 1.0))],\n",
    "  free_raw_data = FALSE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Aqui comienza la configuracion de la Bayesian Optimization\n",
    "funcion_optimizar <- EstimarGanancia_lightgbm # la funcion que voy a maximizar\n",
    "\n",
    "configureMlr(show.learner.output = FALSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# configuro la busqueda bayesiana,  los hiperparametros que se van a optimizar\n",
    "# por favor, no desesperarse por lo complejo\n",
    "obj.fun <- makeSingleObjectiveFunction(\n",
    "  fn = funcion_optimizar, # la funcion que voy a maximizar\n",
    "  minimize = FALSE, # estoy Maximizando la ganancia\n",
    "  noisy = TRUE,\n",
    "  par.set = hs, # definido al comienzo del programa\n",
    "  has.simple.signature = FALSE # paso los parametros en una lista\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# cada 600 segundos guardo el resultado intermedio\n",
    "ctrl <- makeMBOControl(\n",
    "  save.on.disk.at.time = 600, # se graba cada 600 segundos\n",
    "  save.file.path = kbayesiana\n",
    ") # se graba cada 600 segundos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# indico la cantidad de iteraciones que va a tener la Bayesian Optimization\n",
    "ctrl <- setMBOControlTermination(\n",
    "  ctrl,\n",
    "  iters = PARAM$hyperparametertuning$iteraciones\n",
    ") # cantidad de iteraciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# defino el método estandar para la creacion de los puntos iniciales,\n",
    "# los \"No Inteligentes\"\n",
    "ctrl <- setMBOControlInfill(ctrl, crit = makeMBOInfillCritEI())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# establezco la funcion que busca el maximo\n",
    "surr.km <- makeLearner(\n",
    "  \"regr.km\",\n",
    "  predict.type = \"se\",\n",
    "  covtype = \"matern3_2\",\n",
    "  control = list(trace = TRUE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# inicio la optimizacion bayesiana\n",
    "if (!file.exists(kbayesiana)) {\n",
    "  run <- mbo(obj.fun, learner = surr.km, control = ctrl)\n",
    "} else {\n",
    "  run <- mboContinue(kbayesiana) # retomo en caso que ya exista\n",
    "# }\n",
    "\n",
    "\n",
    "cat(\"\\n\\nLa optimizacion Bayesiana ha terminado\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
