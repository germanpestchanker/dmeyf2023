{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8XpJxejaRZw"
      },
      "source": [
        "# Interpretando los modelos, conectando con humanos\n",
        "\n",
        "## SHAP Values\n",
        "\n",
        "Veremos brevemente un modelo de interpretabilidad para modelos complejos de machine learning como es un LGBM.\n",
        "\n",
        "El uso de **python** para esta exploración se debe a la madurez encontrada en las librerías en mi búsqueda.\n",
        "\n",
        "Empezamos con la instalación y carga de los módulos necesarios:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Collecting shap\n",
            "  Obtaining dependency information for shap from https://files.pythonhosted.org/packages/9e/3f/247e0017d52eeef37c170d71357eb3a12a2c06718d2e184c9929b6f3d9ed/shap-0.43.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
            "  Downloading shap-0.43.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (24 kB)\n",
            "Requirement already satisfied: numpy in /home/germanpestchanker/.local/lib/python3.10/site-packages (from shap) (1.24.3)\n",
            "Requirement already satisfied: scipy in /usr/lib/python3/dist-packages (from shap) (1.8.0)\n",
            "Requirement already satisfied: scikit-learn in /home/germanpestchanker/.local/lib/python3.10/site-packages (from shap) (1.3.0)\n",
            "Requirement already satisfied: pandas in /home/germanpestchanker/.local/lib/python3.10/site-packages (from shap) (2.1.0)\n",
            "Requirement already satisfied: tqdm>=4.27.0 in /home/germanpestchanker/.local/lib/python3.10/site-packages (from shap) (4.66.1)\n",
            "Requirement already satisfied: packaging>20.9 in /usr/lib/python3/dist-packages (from shap) (21.3)\n",
            "Collecting slicer==0.0.7 (from shap)\n",
            "  Downloading slicer-0.0.7-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: numba in /home/germanpestchanker/.local/lib/python3.10/site-packages (from shap) (0.57.1)\n",
            "Requirement already satisfied: cloudpickle in /home/germanpestchanker/.local/lib/python3.10/site-packages (from shap) (2.2.1)\n",
            "Requirement already satisfied: llvmlite<0.41,>=0.40.0dev0 in /home/germanpestchanker/.local/lib/python3.10/site-packages (from numba->shap) (0.40.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /home/germanpestchanker/.local/lib/python3.10/site-packages (from pandas->shap) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas->shap) (2022.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /home/germanpestchanker/.local/lib/python3.10/site-packages (from pandas->shap) (2023.3)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /home/germanpestchanker/.local/lib/python3.10/site-packages (from scikit-learn->shap) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/germanpestchanker/.local/lib/python3.10/site-packages (from scikit-learn->shap) (3.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->shap) (1.16.0)\n",
            "Downloading shap-0.43.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (532 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m532.9/532.9 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: slicer, shap\n",
            "Successfully installed shap-0.43.0 slicer-0.0.7\n"
          ]
        }
      ],
      "source": [
        "!pip install shap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: lightgbm in /home/germanpestchanker/.local/lib/python3.10/site-packages (4.1.0)\n",
            "Requirement already satisfied: numpy in /home/germanpestchanker/.local/lib/python3.10/site-packages (from lightgbm) (1.24.3)\n",
            "Requirement already satisfied: scipy in /usr/lib/python3/dist-packages (from lightgbm) (1.8.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install lightgbm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Collecting umap\n",
            "  Downloading umap-0.1.1.tar.gz (3.2 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hBuilding wheels for collected packages: umap\n",
            "  Building wheel for umap (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for umap: filename=umap-0.1.1-py3-none-any.whl size=3565 sha256=9c2966c6299b24f1fc7f53735cebb5ab94dabbaf700bb43efae3f11fe25b249c\n",
            "  Stored in directory: /home/germanpestchanker/.cache/pip/wheels/15/f1/28/53dcf7a309118ed35d810a5f9cb995217800f3f269ab5771cb\n",
            "Successfully built umap\n",
            "Installing collected packages: umap\n",
            "Successfully installed umap-0.1.1\n"
          ]
        }
      ],
      "source": [
        "!pip install umap "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Collecting umap-learn\n",
            "  Downloading umap-learn-0.5.4.tar.gz (90 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.8/90.8 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hRequirement already satisfied: numba>=0.51.2 in /home/germanpestchanker/.local/lib/python3.10/site-packages (from umap-learn) (0.57.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /home/germanpestchanker/.local/lib/python3.10/site-packages (from umap-learn) (1.24.3)\n",
            "Collecting pynndescent>=0.5 (from umap-learn)\n",
            "  Downloading pynndescent-0.5.10.tar.gz (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hRequirement already satisfied: scikit-learn>=0.22 in /home/germanpestchanker/.local/lib/python3.10/site-packages (from umap-learn) (1.3.0)\n",
            "Requirement already satisfied: scipy>=1.3.1 in /usr/lib/python3/dist-packages (from umap-learn) (1.8.0)\n",
            "Collecting tbb>=2019.0 (from umap-learn)\n",
            "  Obtaining dependency information for tbb>=2019.0 from https://files.pythonhosted.org/packages/79/a8/01ac205ff1f68f543aa73d69d6947218cd0973992a4b60cf0a4bfe507561/tbb-2021.10.0-py2.py3-none-manylinux1_x86_64.whl.metadata\n",
            "  Downloading tbb-2021.10.0-py2.py3-none-manylinux1_x86_64.whl.metadata (1.0 kB)\n",
            "Requirement already satisfied: tqdm in /home/germanpestchanker/.local/lib/python3.10/site-packages (from umap-learn) (4.66.1)\n",
            "Requirement already satisfied: llvmlite<0.41,>=0.40.0dev0 in /home/germanpestchanker/.local/lib/python3.10/site-packages (from numba>=0.51.2->umap-learn) (0.40.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /home/germanpestchanker/.local/lib/python3.10/site-packages (from pynndescent>=0.5->umap-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/germanpestchanker/.local/lib/python3.10/site-packages (from scikit-learn>=0.22->umap-learn) (3.2.0)\n",
            "Downloading tbb-2021.10.0-py2.py3-none-manylinux1_x86_64.whl (4.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: umap-learn, pynndescent\n",
            "  Building wheel for umap-learn (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for umap-learn: filename=umap_learn-0.5.4-py3-none-any.whl size=86793 sha256=c092a7289682bdc9b05b56fbb11d2517c6baa3ddd07394c3b7b4b347c695aaa3\n",
            "  Stored in directory: /home/germanpestchanker/.cache/pip/wheels/fb/66/29/199acf5784d0f7b8add6d466175ab45506c96e386ed5dd0633\n",
            "  Building wheel for pynndescent (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for pynndescent: filename=pynndescent-0.5.10-py3-none-any.whl size=55636 sha256=9cbf437169ec81dc9f143705fa27ba6262770161020e1bb1aa307f78a114048b\n",
            "  Stored in directory: /home/germanpestchanker/.cache/pip/wheels/4a/38/5d/f60a40a66a9512b7e5e83517ebc2d1b42d857be97d135f1096\n",
            "Successfully built umap-learn pynndescent\n",
            "Installing collected packages: tbb, pynndescent, umap-learn\n",
            "Successfully installed pynndescent-0.5.10 tbb-2021.10.0 umap-learn-0.5.4\n"
          ]
        }
      ],
      "source": [
        "!pip install umap-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I6d2e7cMaRZ0",
        "outputId": "c88e2be4-e7be-461d-b6f7-63da2adcbd4b"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style type='text/css'>\n",
              ".datatable table.frame { margin-bottom: 0; }\n",
              ".datatable table.frame thead { border-bottom: none; }\n",
              ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
              ".datatable .bool    { background: #DDDD99; }\n",
              ".datatable .object  { background: #565656; }\n",
              ".datatable .int     { background: #5D9E5D; }\n",
              ".datatable .float   { background: #4040CC; }\n",
              ".datatable .str     { background: #CC4040; }\n",
              ".datatable .time    { background: #40CC40; }\n",
              ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
              ".datatable .frame tbody td { text-align: left; }\n",
              ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
              ".datatable th:nth-child(2) { padding-left: 12px; }\n",
              ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
              ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
              ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
              ".datatable .sp {  opacity: 0.25;}\n",
              ".datatable .footer { font-size: 9px; }\n",
              ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
              "</style>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Matplotlib is building the font cache; this may take a moment.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import lightgbm as lgb\n",
        "import shap\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.ensemble import  RandomForestClassifier\n",
        "from sklearn.impute import SimpleImputer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-10-14 22:50:07.777804: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2023-10-14 22:50:10.341146: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2023-10-14 22:50:10.343999: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-10-14 22:50:14.879405: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        }
      ],
      "source": [
        "## Revisar https://github.com/MaartenGr/BERTopic/issues/723\n",
        "from umap import UMAP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztnX-i3-aRZ1"
      },
      "source": [
        "Cargamos el dataset. Vamos a trabajar con el primer conjunto de datos pero usted deberá utilizar un dataset ampliado y más refinado. Preparamos y separamos las clases y "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "5iEEz8KraRZ1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Columns (154) have mixed types. Specify dtype option on import or set low_memory=False.\n"
          ]
        }
      ],
      "source": [
        "#ds_train = pd.read_csv(\"D:/Equipo/Documentos/Posgrado/2C23/Competencia 2/competencia_02.csv.gz\")\n",
        "\n",
        "ds_train = pd.read_csv(\"~/buckets/b1/datasets/competencia_02.csv.gz\")\n",
        "\n",
        "clase_train = ds_train[\"clase_ternaria\"].map(lambda x: 0 if x == \"CONTINUA\" else 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A55xcb-taRZ2"
      },
      "source": [
        "Y armamos un **dataset** con solo los casos de **BAJAS+1** y **BAJAS+2**. Sacamos los de los **datasets** los targets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "rP9dL3g5aRZ2"
      },
      "outputs": [],
      "source": [
        "ds_bajas = ds_train.query(\"clase_ternaria != 'CONTINUA'\")\n",
        "ds_train = ds_train.drop([\"clase_ternaria\", \"numero_de_cliente\"], axis=1)\n",
        "ds_bajas = ds_bajas.drop([\"clase_ternaria\", \"numero_de_cliente\"], axis=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WU89THJaaRZ3"
      },
      "source": [
        "Y hacemos un modelo **LGBM**. En este punto, usted agregue los mejores parámetros que haya encontrado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gJFi741zaRZ3",
        "outputId": "6de91933-1761-4d79-a510-206ddc2b920b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Found `num_iterations` in params. Will use it instead of argument\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 1.070656 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3798\n",
            "[LightGBM] [Info] Number of data points in the train set: 4562810, number of used features: 153\n",
            "[LightGBM] [Info] Start training from score 0.080405\n"
          ]
        }
      ],
      "source": [
        "# create dataset for lightgbm\n",
        "lgb_train = lgb.Dataset(ds_train, clase_train)\n",
        "\n",
        "# random.seed(279523)\n",
        "# specify your configurations as a dict (pongo los dado en el script)\n",
        "params = {\n",
        "\n",
        "    'boosting_type': 'gbdt',  # Método de impulso, gradient boosting decision tree\n",
        "    'objective': 'regression',  # Tipo de objetivo, en este caso, regresión\n",
        "    'seed': 279523,  # Semilla aleatoria\n",
        "    'num_iterations': 1838,  # Número de iteraciones\n",
        "    'learning_rate': 0.0443304506619713,  # Tasa de aprendizaje\n",
        "    'feature_fraction': 0.699403921241248,  # Fracción de características a considerar por cada árbol\n",
        "    'min_data_in_leaf': 363,  # Número mínimo de datos en una hoja\n",
        "    'num_leaves': 636,  # Número máximo de hojas en un árbol\n",
        "    'max_bin': 31  # Número máximo de bins\n",
        "}\n",
        "\n",
        "gbm = lgb.train(params, lgb_train, num_boost_round=100)\n",
        "# calculamos las p para los clientes bajas (no me estoy preocupando del overfitting ni nada, algo quizás usted debiera)\n",
        "p_bajas = gbm.predict(ds_bajas)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PGLD_OL0aRZ4"
      },
      "source": [
        "Hasta ahora, las herramientas para saber a que feature le daba importancia un modelo eran básicas. No era mucho más que un **feature importance**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "igeVgb3MaRZ4",
        "outputId": "db370158-08bb-41d9-a47c-b0e72eb5474e"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Features</th>\n",
              "      <th>Importances</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>106</th>\n",
              "      <td>ctrx_quarter</td>\n",
              "      <td>48521</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>cliente_edad</td>\n",
              "      <td>44293</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>mcaja_ahorro</td>\n",
              "      <td>41539</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>foto_mes</td>\n",
              "      <td>41309</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>mcuentas_saldo</td>\n",
              "      <td>35540</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>cliente_vip</td>\n",
              "      <td>49</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>ccuenta_corriente</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>121</th>\n",
              "      <td>Master_madelantodolares</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53</th>\n",
              "      <td>cpayroll2_trx</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52</th>\n",
              "      <td>mpayroll2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>153 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                    Features  Importances\n",
              "106             ctrx_quarter        48521\n",
              "4               cliente_edad        44293\n",
              "17              mcaja_ahorro        41539\n",
              "0                   foto_mes        41309\n",
              "21            mcuentas_saldo        35540\n",
              "..                       ...          ...\n",
              "2                cliente_vip           49\n",
              "13         ccuenta_corriente           16\n",
              "121  Master_madelantodolares            4\n",
              "53             cpayroll2_trx            1\n",
              "52                 mpayroll2            0\n",
              "\n",
              "[153 rows x 2 columns]"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "lgbm_importancia = pd.DataFrame({'Features': gbm.feature_name(),\n",
        "                        'Importances': gbm.feature_importance()})\n",
        "lgbm_importancia.sort_values(by='Importances', inplace=True, ascending=False)\n",
        "lgbm_importancia\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sin embargo podemos intuir que si un modelo detecta múltiples patrones, no todos los casos van a ser afectados por el mismo. Tiene sentido que dos clientes se den de baja por motivos distintos y que un modelo pondere para cada caso, un peso distinto a los **features**. O sea, necesitamos una forma de explicar de forma local (caso a caso) en vez de una global (para todos los casos). Para esto utilizaremos los modelos [SHAP](https://christophm.github.io/interpretable-ml-book/shap.html)  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CzK9kANaRZ4"
      },
      "source": [
        "Ahora avanzamos aplicando un modelo de interpretabilidad sobre el modelo anterior."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dRhywfaAaRZ5",
        "outputId": "02b92dec-b648-443b-caea-f290c09ee247"
      },
      "outputs": [],
      "source": [
        "explainer = shap.TreeExplainer(gbm)\n",
        "shap_values = explainer.shap_values(ds_bajas)\n",
        "shap_bajas = pd.DataFrame(shap_values[0], columns = ds_bajas.columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5X7LfoXaRZ5"
      },
      "source": [
        "En la variable `shap_bajas` contaríamos para cada caso, el peso de la influencia de esa **feature** de acuerdo al modelo. Veamos un caso, tomemos un cliente:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461
        },
        "id": "GIxeFGeKaRZ5",
        "outputId": "c43fd3fd-6070-40c5-b9f1-7e17afadcd32"
      },
      "outputs": [],
      "source": [
        "# busco un caso en p_bajas, con un p alta, y tomo su indice\n",
        "cliente_idx = 7\n",
        "print(p_bajas[cliente_idx])\n",
        "df = pd.concat([ds_bajas.iloc[cliente_idx], shap_bajas.iloc[cliente_idx]], axis=1)\n",
        "df.sort_values(by=[cliente_idx], ascending=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "En la tabla anterior, los **features** con números positivos \"adicionan(*)\" a la probabilidad y los negativos. \n",
        "\n",
        "(*) Aquí estamos usando TreeSHAP, que no es tan directa como Kernel SHAP para la interpretación directa. \n",
        "\n",
        "También podemos combinar todos los valores de SHAP y recrear una nueva importancia de variables:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J16lpSHjaRZ5",
        "outputId": "c6693404-69dc-4929-97d7-1930e876d557"
      },
      "outputs": [],
      "source": [
        "shap_bajas.mean().abs().sort_values(ascending=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmKDT3NTaRZ5"
      },
      "source": [
        "La librería contiene útiles herramientas para vizualizar la info anterior"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 957
        },
        "id": "SEF-xuD8aRZ5",
        "outputId": "06daeae5-9f7a-4d3a-95e4-4bda7d31f010"
      },
      "outputs": [],
      "source": [
        "shap.summary_plot(shap_values, ds_bajas)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJFeofiaaRZ6"
      },
      "source": [
        "Para ver más opciones de visualización\n",
        "https://www.youtube.com/watch?v=L8_sVRhBDLU&t=3s"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZSoSHvyaRZ6"
      },
      "source": [
        "## ¿Se podrá hacer una clusterización sobre los valores de shap?\n",
        "\n",
        "Lo que estaríamos buscando es: \"Juntar a todos los clientes que son afectamos por los mismos patrones del modelo, entendiendo que si les afecto el mismo patrón, sus SHAP van a ser similares.\" Presunción fuerte.\n",
        " \n",
        "Como la cantidad de variables es muy alto, es conveniente hacer un embedding con **UMAP**. Por como es la estrutura de los **shap values** se pueden hacer sin mucho preprocesamiento. Usamos UMAP para estos embedding, https://www.youtube.com/watch?v=eN0wFzBA4Sc (triple BAM (?))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448
        },
        "id": "5qWkfZfBfO9u",
        "outputId": "d9ddebd6-04fc-4536-8d55-c849504b4fff"
      },
      "outputs": [],
      "source": [
        "embedding_2d = UMAP(\n",
        "  n_components=2, n_neighbors=40\n",
        ").fit_transform(shap_values[0])\n",
        "plt.scatter(embedding_2d[:,0], embedding_2d[:,1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vemos manchas separadas entre ellas, admiramos a UMAP. Hizo el trabajo sucio.\n",
        "\n",
        "Algo importante cuando trabaja con modelos, es recordar que no siempre detectan con la misma \"fuerza\" todos los casos. Incluso nosotros solo terminamos mandando a Kaggle un subconjunto de casos. Si vemos el histograma de las probabilidades de salida"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        },
        "id": "klnop5qF45JI",
        "outputId": "31b7bc02-fbe6-4188-fa11-44d5e3f1fdef"
      },
      "outputs": [],
      "source": [
        "plt.hist(p_bajas,bins=30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vemos que hay en los 2000 y algo de casos, 500 que no estaríamos mandando. Veamos si hay alguna relación si cruzamos el embedding y su probabilidad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448
        },
        "id": "bHEm1r7Wh0PN",
        "outputId": "298d6308-52c8-4442-f90b-84f5577df403"
      },
      "outputs": [],
      "source": [
        "sc = plt.scatter(embedding_2d[:, 0], embedding_2d[:, 1], c=p_bajas)\n",
        "plt.colorbar(sc)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "o0yNwaZVi7YI",
        "outputId": "23926904-94e8-4a5a-aa01-4c8206aefa08"
      },
      "outputs": [],
      "source": [
        "\n",
        "hdb = DBSCAN(eps=0.75) # juegue con los parámetros \n",
        "y = hdb.fit(embedding_2d)\n",
        "\n",
        "plt.scatter(embedding_2d[:, 0], embedding_2d[:, 1], c=y.labels_)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Comparamos 2 clusters para ver si cambian sus variables importantes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "evt3uVP4pWUF",
        "outputId": "28e4eadd-df65-43b6-b39f-61589682fa7f"
      },
      "outputs": [],
      "source": [
        "shap_bajas[y.labels_ == 1].mean().sort_values(ascending=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "shap_bajas[y.labels_ == 2].mean().sort_values(ascending=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "shap_bajas[y.labels_ == 3].mean().sort_values(ascending=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "shap_bajas[y.labels_ == 4].mean().sort_values(ascending=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ah4G51VJqP12",
        "outputId": "ba2459f4-ff64-4f47-f95b-dfb0d3a9e177"
      },
      "outputs": [],
      "source": [
        "shap_bajas[y.labels_ == 5].mean().sort_values(ascending=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Y comparamos las distribuciones de las variables importantes con la de la población general, para entender que distingue a nuestro cluster."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Caso con payroll"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.hist(ds_bajas.loc[:][\"cpayroll_trx\"], label='all', density=True, bins=25)\n",
        "plt.hist(ds_bajas.loc[y.labels_ == 1][\"cpayroll_trx\"], label='cluster 1', density=True, bins=25)\n",
        "\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Overlapping')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.hist(ds_bajas.loc[:][\"cpayroll_trx\"], label='all', density=True, bins=25)\n",
        "plt.hist(ds_bajas.loc[y.labels_ == 2][\"cpayroll_trx\"], label='cluster 2', density=True, bins=25)\n",
        "\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Overlapping')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.hist(ds_bajas.loc[:][\"cpayroll_trx\"], label='all', density=True, bins=25)\n",
        "plt.hist(ds_bajas.loc[y.labels_ == 3][\"cpayroll_trx\"], label='cluster 3', density=True, bins=25)\n",
        "\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Overlapping')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.hist(ds_bajas.loc[:][\"cpayroll_trx\"], label='all', density=True, bins=25)\n",
        "plt.hist(ds_bajas.loc[y.labels_ == 4][\"cpayroll_trx\"], label='cluster 4', density=True, bins=25)\n",
        "\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Overlapping')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "GAnsBUWC3z7u",
        "outputId": "9269053c-8db0-47e7-8422-7364d5d56b5a"
      },
      "outputs": [],
      "source": [
        "plt.hist(ds_bajas.loc[:][\"cpayroll_trx\"], label='all', density=True, bins=25)\n",
        "plt.hist(ds_bajas.loc[y.labels_ == 5][\"cpayroll_trx\"], label='cluster 5', density=True, bins=25)\n",
        "\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Overlapping')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.4 ('monday-2l6OfFZC-py3.10')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "d82221ca95a05516fde857407cc8e78d06cecb56814b70ff11970b46a17253e5"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
